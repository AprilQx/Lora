{
  "experiment_name": "continued_ctx768_lr1e-04_rank8_prec2",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-04-02 17:47:37",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:41",
      "total_flops_so_far": 9124036964352.0,
      "budget_used_percent": 0.009124036964352
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:42",
      "total_flops_so_far": 18248073928704.0,
      "budget_used_percent": 0.018248073928704
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:42",
      "total_flops_so_far": 27372110893056.0,
      "budget_used_percent": 0.027372110893056
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:42",
      "total_flops_so_far": 36496147857408.0,
      "budget_used_percent": 0.036496147857408
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:43",
      "total_flops_so_far": 45620184821760.0,
      "budget_used_percent": 0.04562018482176
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:43",
      "total_flops_so_far": 54744221786112.0,
      "budget_used_percent": 0.054744221786112
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:44",
      "total_flops_so_far": 63868258750464.0,
      "budget_used_percent": 0.063868258750464
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:44",
      "total_flops_so_far": 72992295714816.0,
      "budget_used_percent": 0.072992295714816
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:45",
      "total_flops_so_far": 82116332679168.0,
      "budget_used_percent": 0.082116332679168
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:45",
      "total_flops_so_far": 91240369643520.0,
      "budget_used_percent": 0.09124036964352
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:45",
      "total_flops_so_far": 100364406607872.0,
      "budget_used_percent": 0.10036440660787199
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:46",
      "total_flops_so_far": 109488443572224.0,
      "budget_used_percent": 0.109488443572224
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:46",
      "total_flops_so_far": 118612480536576.0,
      "budget_used_percent": 0.118612480536576
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:47",
      "total_flops_so_far": 127736517500928.0,
      "budget_used_percent": 0.127736517500928
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:47",
      "total_flops_so_far": 136860554465280.0,
      "budget_used_percent": 0.13686055446528
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:48",
      "total_flops_so_far": 145984591429632.0,
      "budget_used_percent": 0.145984591429632
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:48",
      "total_flops_so_far": 155108628393984.0,
      "budget_used_percent": 0.155108628393984
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:48",
      "total_flops_so_far": 164232665358336.0,
      "budget_used_percent": 0.164232665358336
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:49",
      "total_flops_so_far": 173356702322688.0,
      "budget_used_percent": 0.173356702322688
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:49",
      "total_flops_so_far": 182480739287040.0,
      "budget_used_percent": 0.18248073928704
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:50",
      "total_flops_so_far": 191604776251392.0,
      "budget_used_percent": 0.191604776251392
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:50",
      "total_flops_so_far": 200728813215744.0,
      "budget_used_percent": 0.20072881321574398
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:51",
      "total_flops_so_far": 209852850180096.0,
      "budget_used_percent": 0.20985285018009597
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:51",
      "total_flops_so_far": 218976887144448.0,
      "budget_used_percent": 0.218976887144448
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:51",
      "total_flops_so_far": 228100924108800.0,
      "budget_used_percent": 0.22810092410879998
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:52",
      "total_flops_so_far": 237224961073152.0,
      "budget_used_percent": 0.237224961073152
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:52",
      "total_flops_so_far": 246348998037504.0,
      "budget_used_percent": 0.24634899803750399
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:53",
      "total_flops_so_far": 255473035001856.0,
      "budget_used_percent": 0.255473035001856
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:53",
      "total_flops_so_far": 264597071966208.0,
      "budget_used_percent": 0.264597071966208
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:54",
      "total_flops_so_far": 273721108930560.0,
      "budget_used_percent": 0.27372110893056
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:54",
      "total_flops_so_far": 282845145894912.0,
      "budget_used_percent": 0.282845145894912
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:54",
      "total_flops_so_far": 291969182859264.0,
      "budget_used_percent": 0.291969182859264
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:55",
      "total_flops_so_far": 301093219823616.0,
      "budget_used_percent": 0.30109321982361603
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:55",
      "total_flops_so_far": 310217256787968.0,
      "budget_used_percent": 0.310217256787968
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:56",
      "total_flops_so_far": 319341293752320.0,
      "budget_used_percent": 0.31934129375232
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:56",
      "total_flops_so_far": 328465330716672.0,
      "budget_used_percent": 0.328465330716672
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:57",
      "total_flops_so_far": 337589367681024.0,
      "budget_used_percent": 0.337589367681024
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:57",
      "total_flops_so_far": 346713404645376.0,
      "budget_used_percent": 0.346713404645376
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:57",
      "total_flops_so_far": 355837441609728.0,
      "budget_used_percent": 0.35583744160972797
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:58",
      "total_flops_so_far": 364961478574080.0,
      "budget_used_percent": 0.36496147857408
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:58",
      "total_flops_so_far": 374085515538432.0,
      "budget_used_percent": 0.374085515538432
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:59",
      "total_flops_so_far": 383209552502784.0,
      "budget_used_percent": 0.383209552502784
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:47:59",
      "total_flops_so_far": 392333589467136.0,
      "budget_used_percent": 0.392333589467136
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:00",
      "total_flops_so_far": 401457626431488.0,
      "budget_used_percent": 0.40145762643148797
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:00",
      "total_flops_so_far": 410581663395840.0,
      "budget_used_percent": 0.41058166339583996
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:00",
      "total_flops_so_far": 419705700360192.0,
      "budget_used_percent": 0.41970570036019195
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:01",
      "total_flops_so_far": 428829737324544.0,
      "budget_used_percent": 0.428829737324544
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:01",
      "total_flops_so_far": 437953774288896.0,
      "budget_used_percent": 0.437953774288896
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:02",
      "total_flops_so_far": 447077811253248.0,
      "budget_used_percent": 0.44707781125324797
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:02",
      "total_flops_so_far": 456201848217600.0,
      "budget_used_percent": 0.45620184821759996
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:03",
      "total_flops_so_far": 465325885181952.0,
      "budget_used_percent": 0.465325885181952
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:03",
      "total_flops_so_far": 474449922146304.0,
      "budget_used_percent": 0.474449922146304
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:03",
      "total_flops_so_far": 483573959110656.0,
      "budget_used_percent": 0.483573959110656
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:04",
      "total_flops_so_far": 492697996075008.0,
      "budget_used_percent": 0.49269799607500797
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:04",
      "total_flops_so_far": 501822033039360.0,
      "budget_used_percent": 0.50182203303936
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:05",
      "total_flops_so_far": 510946070003712.0,
      "budget_used_percent": 0.510946070003712
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:05",
      "total_flops_so_far": 520070106968064.0,
      "budget_used_percent": 0.520070106968064
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:06",
      "total_flops_so_far": 529194143932416.0,
      "budget_used_percent": 0.529194143932416
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:06",
      "total_flops_so_far": 538318180896768.0,
      "budget_used_percent": 0.538318180896768
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:07",
      "total_flops_so_far": 547442217861120.0,
      "budget_used_percent": 0.54744221786112
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:07",
      "total_flops_so_far": 556566254825472.0,
      "budget_used_percent": 0.5565662548254721
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:07",
      "total_flops_so_far": 565690291789824.0,
      "budget_used_percent": 0.565690291789824
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:08",
      "total_flops_so_far": 574814328754176.0,
      "budget_used_percent": 0.574814328754176
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:08",
      "total_flops_so_far": 583938365718528.0,
      "budget_used_percent": 0.583938365718528
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:09",
      "total_flops_so_far": 593062402682880.0,
      "budget_used_percent": 0.59306240268288
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:09",
      "total_flops_so_far": 602186439647232.0,
      "budget_used_percent": 0.6021864396472321
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:10",
      "total_flops_so_far": 611310476611584.0,
      "budget_used_percent": 0.611310476611584
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:10",
      "total_flops_so_far": 620434513575936.0,
      "budget_used_percent": 0.620434513575936
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:10",
      "total_flops_so_far": 629558550540288.0,
      "budget_used_percent": 0.6295585505402881
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:11",
      "total_flops_so_far": 638682587504640.0,
      "budget_used_percent": 0.63868258750464
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:11",
      "total_flops_so_far": 647806624468992.0,
      "budget_used_percent": 0.647806624468992
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:12",
      "total_flops_so_far": 656930661433344.0,
      "budget_used_percent": 0.656930661433344
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:12",
      "total_flops_so_far": 666054698397696.0,
      "budget_used_percent": 0.6660546983976959
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:13",
      "total_flops_so_far": 675178735362048.0,
      "budget_used_percent": 0.675178735362048
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:13",
      "total_flops_so_far": 684302772326400.0,
      "budget_used_percent": 0.6843027723264
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:13",
      "total_flops_so_far": 693426809290752.0,
      "budget_used_percent": 0.693426809290752
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:14",
      "total_flops_so_far": 702550846255104.0,
      "budget_used_percent": 0.702550846255104
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:14",
      "total_flops_so_far": 711674883219456.0,
      "budget_used_percent": 0.7116748832194559
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:15",
      "total_flops_so_far": 720798920183808.0,
      "budget_used_percent": 0.720798920183808
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:15",
      "total_flops_so_far": 729922957148160.0,
      "budget_used_percent": 0.72992295714816
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:16",
      "total_flops_so_far": 739046994112512.0,
      "budget_used_percent": 0.739046994112512
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:16",
      "total_flops_so_far": 748171031076864.0,
      "budget_used_percent": 0.748171031076864
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:16",
      "total_flops_so_far": 757295068041216.0,
      "budget_used_percent": 0.757295068041216
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:17",
      "total_flops_so_far": 766419105005568.0,
      "budget_used_percent": 0.766419105005568
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:17",
      "total_flops_so_far": 775543141969920.0,
      "budget_used_percent": 0.77554314196992
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:18",
      "total_flops_so_far": 784667178934272.0,
      "budget_used_percent": 0.784667178934272
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:18",
      "total_flops_so_far": 793791215898624.0,
      "budget_used_percent": 0.7937912158986239
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:19",
      "total_flops_so_far": 802915252862976.0,
      "budget_used_percent": 0.8029152528629759
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:19",
      "total_flops_so_far": 812039289827328.0,
      "budget_used_percent": 0.8120392898273279
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:19",
      "total_flops_so_far": 821163326791680.0,
      "budget_used_percent": 0.8211633267916799
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:20",
      "total_flops_so_far": 830287363756032.0,
      "budget_used_percent": 0.830287363756032
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:20",
      "total_flops_so_far": 839411400720384.0,
      "budget_used_percent": 0.8394114007203839
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:21",
      "total_flops_so_far": 848535437684736.0,
      "budget_used_percent": 0.8485354376847359
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:21",
      "total_flops_so_far": 857659474649088.0,
      "budget_used_percent": 0.857659474649088
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:22",
      "total_flops_so_far": 866783511613440.0,
      "budget_used_percent": 0.8667835116134399
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:22",
      "total_flops_so_far": 875907548577792.0,
      "budget_used_percent": 0.875907548577792
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:23",
      "total_flops_so_far": 885031585542144.0,
      "budget_used_percent": 0.885031585542144
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:23",
      "total_flops_so_far": 894155622506496.0,
      "budget_used_percent": 0.8941556225064959
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:23",
      "total_flops_so_far": 903279659470848.0,
      "budget_used_percent": 0.903279659470848
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:24",
      "total_flops_so_far": 912403696435200.0,
      "budget_used_percent": 0.9124036964351999
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:24",
      "total_flops_so_far": 921527733399552.0,
      "budget_used_percent": 0.921527733399552
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:25",
      "total_flops_so_far": 930651770363904.0,
      "budget_used_percent": 0.930651770363904
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:25",
      "total_flops_so_far": 939775807328256.0,
      "budget_used_percent": 0.9397758073282559
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:26",
      "total_flops_so_far": 948899844292608.0,
      "budget_used_percent": 0.948899844292608
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:26",
      "total_flops_so_far": 958023881256960.0,
      "budget_used_percent": 0.9580238812569599
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:26",
      "total_flops_so_far": 967147918221312.0,
      "budget_used_percent": 0.967147918221312
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:27",
      "total_flops_so_far": 976271955185664.0,
      "budget_used_percent": 0.976271955185664
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:27",
      "total_flops_so_far": 985395992150016.0,
      "budget_used_percent": 0.9853959921500159
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:28",
      "total_flops_so_far": 994520029114368.0,
      "budget_used_percent": 0.994520029114368
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:28",
      "total_flops_so_far": 1003644066078720.0,
      "budget_used_percent": 1.00364406607872
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:29",
      "total_flops_so_far": 1012768103043072.0,
      "budget_used_percent": 1.012768103043072
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:29",
      "total_flops_so_far": 1021892140007424.0,
      "budget_used_percent": 1.021892140007424
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:29",
      "total_flops_so_far": 1031016176971776.0,
      "budget_used_percent": 1.031016176971776
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:30",
      "total_flops_so_far": 1040140213936128.0,
      "budget_used_percent": 1.040140213936128
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:30",
      "total_flops_so_far": 1049264250900480.0,
      "budget_used_percent": 1.04926425090048
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:31",
      "total_flops_so_far": 1058388287864832.0,
      "budget_used_percent": 1.058388287864832
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:31",
      "total_flops_so_far": 1067512324829184.0,
      "budget_used_percent": 1.067512324829184
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:32",
      "total_flops_so_far": 1076636361793536.0,
      "budget_used_percent": 1.076636361793536
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:32",
      "total_flops_so_far": 1085760398757888.0,
      "budget_used_percent": 1.085760398757888
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:32",
      "total_flops_so_far": 1094884435722240.0,
      "budget_used_percent": 1.09488443572224
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:33",
      "total_flops_so_far": 1104008472686592.0,
      "budget_used_percent": 1.104008472686592
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:33",
      "total_flops_so_far": 1113132509650944.0,
      "budget_used_percent": 1.1131325096509441
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:34",
      "total_flops_so_far": 1122256546615296.0,
      "budget_used_percent": 1.122256546615296
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:34",
      "total_flops_so_far": 1131380583579648.0,
      "budget_used_percent": 1.131380583579648
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:35",
      "total_flops_so_far": 1140504620544000.0,
      "budget_used_percent": 1.1405046205440001
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:35",
      "total_flops_so_far": 1149628657508352.0,
      "budget_used_percent": 1.149628657508352
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:36",
      "total_flops_so_far": 1158752694472704.0,
      "budget_used_percent": 1.158752694472704
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:36",
      "total_flops_so_far": 1167876731437056.0,
      "budget_used_percent": 1.167876731437056
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:36",
      "total_flops_so_far": 1177000768401408.0,
      "budget_used_percent": 1.177000768401408
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:37",
      "total_flops_so_far": 1186124805365760.0,
      "budget_used_percent": 1.18612480536576
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:37",
      "total_flops_so_far": 1195248842330112.0,
      "budget_used_percent": 1.195248842330112
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:38",
      "total_flops_so_far": 1204372879294464.0,
      "budget_used_percent": 1.2043728792944641
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:38",
      "total_flops_so_far": 1213496916258816.0,
      "budget_used_percent": 1.213496916258816
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:39",
      "total_flops_so_far": 1222620953223168.0,
      "budget_used_percent": 1.222620953223168
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:39",
      "total_flops_so_far": 1231744990187520.0,
      "budget_used_percent": 1.2317449901875202
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:39",
      "total_flops_so_far": 1240869027151872.0,
      "budget_used_percent": 1.240869027151872
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:40",
      "total_flops_so_far": 1249993064116224.0,
      "budget_used_percent": 1.249993064116224
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:40",
      "total_flops_so_far": 1259117101080576.0,
      "budget_used_percent": 1.2591171010805762
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:41",
      "total_flops_so_far": 1268241138044928.0,
      "budget_used_percent": 1.268241138044928
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:41",
      "total_flops_so_far": 1277365175009280.0,
      "budget_used_percent": 1.27736517500928
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:42",
      "total_flops_so_far": 1286489211973632.0,
      "budget_used_percent": 1.286489211973632
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:42",
      "total_flops_so_far": 1295613248937984.0,
      "budget_used_percent": 1.295613248937984
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:42",
      "total_flops_so_far": 1304737285902336.0,
      "budget_used_percent": 1.3047372859023358
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:43",
      "total_flops_so_far": 1313861322866688.0,
      "budget_used_percent": 1.313861322866688
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:43",
      "total_flops_so_far": 1322985359831040.0,
      "budget_used_percent": 1.32298535983104
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:44",
      "total_flops_so_far": 1332109396795392.0,
      "budget_used_percent": 1.3321093967953919
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:44",
      "total_flops_so_far": 1341233433759744.0,
      "budget_used_percent": 1.341233433759744
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:45",
      "total_flops_so_far": 1350357470724096.0,
      "budget_used_percent": 1.350357470724096
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:45",
      "total_flops_so_far": 1359481507688448.0,
      "budget_used_percent": 1.3594815076884479
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:45",
      "total_flops_so_far": 1368605544652800.0,
      "budget_used_percent": 1.3686055446528
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:46",
      "total_flops_so_far": 1377729581617152.0,
      "budget_used_percent": 1.377729581617152
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:46",
      "total_flops_so_far": 1386853618581504.0,
      "budget_used_percent": 1.386853618581504
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:47",
      "total_flops_so_far": 1395977655545856.0,
      "budget_used_percent": 1.3959776555458558
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:47",
      "total_flops_so_far": 1405101692510208.0,
      "budget_used_percent": 1.405101692510208
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:48",
      "total_flops_so_far": 1414225729474560.0,
      "budget_used_percent": 1.41422572947456
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:48",
      "total_flops_so_far": 1423349766438912.0,
      "budget_used_percent": 1.4233497664389119
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:49",
      "total_flops_so_far": 1432473803403264.0,
      "budget_used_percent": 1.432473803403264
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:49",
      "total_flops_so_far": 1441597840367616.0,
      "budget_used_percent": 1.441597840367616
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:49",
      "total_flops_so_far": 1450721877331968.0,
      "budget_used_percent": 1.450721877331968
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:50",
      "total_flops_so_far": 1459845914296320.0,
      "budget_used_percent": 1.45984591429632
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:50",
      "total_flops_so_far": 1468969951260672.0,
      "budget_used_percent": 1.468969951260672
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:51",
      "total_flops_so_far": 1478093988225024.0,
      "budget_used_percent": 1.478093988225024
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:51",
      "total_flops_so_far": 1487218025189376.0,
      "budget_used_percent": 1.487218025189376
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:52",
      "total_flops_so_far": 1496342062153728.0,
      "budget_used_percent": 1.496342062153728
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:52",
      "total_flops_so_far": 1505466099118080.0,
      "budget_used_percent": 1.50546609911808
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:53",
      "total_flops_so_far": 1514590136082432.0,
      "budget_used_percent": 1.514590136082432
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:53",
      "total_flops_so_far": 1523714173046784.0,
      "budget_used_percent": 1.523714173046784
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:53",
      "total_flops_so_far": 1532838210011136.0,
      "budget_used_percent": 1.532838210011136
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:54",
      "total_flops_so_far": 1541962246975488.0,
      "budget_used_percent": 1.541962246975488
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:54",
      "total_flops_so_far": 1551086283939840.0,
      "budget_used_percent": 1.55108628393984
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:55",
      "total_flops_so_far": 1560210320904192.0,
      "budget_used_percent": 1.560210320904192
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:55",
      "total_flops_so_far": 1569334357868544.0,
      "budget_used_percent": 1.569334357868544
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:56",
      "total_flops_so_far": 1578458394832896.0,
      "budget_used_percent": 1.5784583948328959
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:56",
      "total_flops_so_far": 1587582431797248.0,
      "budget_used_percent": 1.5875824317972478
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:56",
      "total_flops_so_far": 1596706468761600.0,
      "budget_used_percent": 1.5967064687616
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:57",
      "total_flops_so_far": 1605830505725952.0,
      "budget_used_percent": 1.6058305057259519
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:57",
      "total_flops_so_far": 1614954542690304.0,
      "budget_used_percent": 1.6149545426903038
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:58",
      "total_flops_so_far": 1624078579654656.0,
      "budget_used_percent": 1.6240785796546557
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:58",
      "total_flops_so_far": 1633202616619008.0,
      "budget_used_percent": 1.633202616619008
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:59",
      "total_flops_so_far": 1642326653583360.0,
      "budget_used_percent": 1.6423266535833598
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:59",
      "total_flops_so_far": 1651450690547712.0,
      "budget_used_percent": 1.6514506905477118
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:48:59",
      "total_flops_so_far": 1660574727512064.0,
      "budget_used_percent": 1.660574727512064
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:00",
      "total_flops_so_far": 1669698764476416.0,
      "budget_used_percent": 1.6696987644764159
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:00",
      "total_flops_so_far": 1678822801440768.0,
      "budget_used_percent": 1.6788228014407678
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:01",
      "total_flops_so_far": 1687946838405120.0,
      "budget_used_percent": 1.68794683840512
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:01",
      "total_flops_so_far": 1697070875369472.0,
      "budget_used_percent": 1.6970708753694719
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:02",
      "total_flops_so_far": 1706194912333824.0,
      "budget_used_percent": 1.7061949123338238
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:02",
      "total_flops_so_far": 1715318949298176.0,
      "budget_used_percent": 1.715318949298176
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:03",
      "total_flops_so_far": 1724442986262528.0,
      "budget_used_percent": 1.724442986262528
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:03",
      "total_flops_so_far": 1733567023226880.0,
      "budget_used_percent": 1.7335670232268798
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:03",
      "total_flops_so_far": 1742691060191232.0,
      "budget_used_percent": 1.742691060191232
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:04",
      "total_flops_so_far": 1751815097155584.0,
      "budget_used_percent": 1.751815097155584
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:04",
      "total_flops_so_far": 1760939134119936.0,
      "budget_used_percent": 1.7609391341199359
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:05",
      "total_flops_so_far": 1770063171084288.0,
      "budget_used_percent": 1.770063171084288
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:05",
      "total_flops_so_far": 1779187208048640.0,
      "budget_used_percent": 1.77918720804864
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:06",
      "total_flops_so_far": 1788311245012992.0,
      "budget_used_percent": 1.7883112450129919
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:06",
      "total_flops_so_far": 1797435281977344.0,
      "budget_used_percent": 1.7974352819773438
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:06",
      "total_flops_so_far": 1806559318941696.0,
      "budget_used_percent": 1.806559318941696
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:07",
      "total_flops_so_far": 1815683355906048.0,
      "budget_used_percent": 1.815683355906048
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:07",
      "total_flops_so_far": 1824807392870400.0,
      "budget_used_percent": 1.8248073928703998
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:08",
      "total_flops_so_far": 1833931429834752.0,
      "budget_used_percent": 1.833931429834752
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:08",
      "total_flops_so_far": 1843055466799104.0,
      "budget_used_percent": 1.843055466799104
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:09",
      "total_flops_so_far": 1852179503763456.0,
      "budget_used_percent": 1.8521795037634559
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:09",
      "total_flops_so_far": 1861303540727808.0,
      "budget_used_percent": 1.861303540727808
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:09",
      "total_flops_so_far": 1870427577692160.0,
      "budget_used_percent": 1.87042757769216
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:10",
      "total_flops_so_far": 1879551614656512.0,
      "budget_used_percent": 1.8795516146565119
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:10",
      "total_flops_so_far": 1888675651620864.0,
      "budget_used_percent": 1.888675651620864
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:11",
      "total_flops_so_far": 1897799688585216.0,
      "budget_used_percent": 1.897799688585216
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:11",
      "total_flops_so_far": 1906923725549568.0,
      "budget_used_percent": 1.906923725549568
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:12",
      "total_flops_so_far": 1916047762513920.0,
      "budget_used_percent": 1.9160477625139198
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:12",
      "total_flops_so_far": 1925171799478272.0,
      "budget_used_percent": 1.925171799478272
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:13",
      "total_flops_so_far": 1934295836442624.0,
      "budget_used_percent": 1.934295836442624
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:13",
      "total_flops_so_far": 1943419873406976.0,
      "budget_used_percent": 1.9434198734069759
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:13",
      "total_flops_so_far": 1952543910371328.0,
      "budget_used_percent": 1.952543910371328
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:14",
      "total_flops_so_far": 1961667947335680.0,
      "budget_used_percent": 1.96166794733568
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:14",
      "total_flops_so_far": 1970791984300032.0,
      "budget_used_percent": 1.9707919843000319
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:15",
      "total_flops_so_far": 1979916021264384.0,
      "budget_used_percent": 1.979916021264384
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:15",
      "total_flops_so_far": 1989040058228736.0,
      "budget_used_percent": 1.989040058228736
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:16",
      "total_flops_so_far": 1998164095193088.0,
      "budget_used_percent": 1.998164095193088
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:16",
      "total_flops_so_far": 2007288132157440.0,
      "budget_used_percent": 2.00728813215744
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:16",
      "total_flops_so_far": 2016412169121792.0,
      "budget_used_percent": 2.016412169121792
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:17",
      "total_flops_so_far": 2025536206086144.0,
      "budget_used_percent": 2.025536206086144
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:17",
      "total_flops_so_far": 2034660243050496.0,
      "budget_used_percent": 2.034660243050496
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:18",
      "total_flops_so_far": 2043784280014848.0,
      "budget_used_percent": 2.043784280014848
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:18",
      "total_flops_so_far": 2052908316979200.0,
      "budget_used_percent": 2.0529083169792
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:19",
      "total_flops_so_far": 2062032353943552.0,
      "budget_used_percent": 2.062032353943552
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:19",
      "total_flops_so_far": 2071156390907904.0,
      "budget_used_percent": 2.071156390907904
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:19",
      "total_flops_so_far": 2080280427872256.0,
      "budget_used_percent": 2.080280427872256
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:20",
      "total_flops_so_far": 2089404464836608.0,
      "budget_used_percent": 2.089404464836608
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:20",
      "total_flops_so_far": 2098528501800960.0,
      "budget_used_percent": 2.09852850180096
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:21",
      "total_flops_so_far": 2107652538765312.0,
      "budget_used_percent": 2.1076525387653122
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:21",
      "total_flops_so_far": 2116776575729664.0,
      "budget_used_percent": 2.116776575729664
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:22",
      "total_flops_so_far": 2125900612694016.0,
      "budget_used_percent": 2.125900612694016
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:22",
      "total_flops_so_far": 2135024649658368.0,
      "budget_used_percent": 2.135024649658368
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:23",
      "total_flops_so_far": 2144148686622720.0,
      "budget_used_percent": 2.14414868662272
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:23",
      "total_flops_so_far": 2153272723587072.0,
      "budget_used_percent": 2.153272723587072
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:23",
      "total_flops_so_far": 2162396760551424.0,
      "budget_used_percent": 2.162396760551424
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:24",
      "total_flops_so_far": 2171520797515776.0,
      "budget_used_percent": 2.171520797515776
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:24",
      "total_flops_so_far": 2180644834480128.0,
      "budget_used_percent": 2.180644834480128
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:25",
      "total_flops_so_far": 2189768871444480.0,
      "budget_used_percent": 2.18976887144448
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:25",
      "total_flops_so_far": 2198892908408832.0,
      "budget_used_percent": 2.198892908408832
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:26",
      "total_flops_so_far": 2208016945373184.0,
      "budget_used_percent": 2.208016945373184
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:26",
      "total_flops_so_far": 2217140982337536.0,
      "budget_used_percent": 2.217140982337536
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:26",
      "total_flops_so_far": 2226265019301888.0,
      "budget_used_percent": 2.2262650193018882
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:27",
      "total_flops_so_far": 2235389056266240.0,
      "budget_used_percent": 2.23538905626624
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:27",
      "total_flops_so_far": 2244513093230592.0,
      "budget_used_percent": 2.244513093230592
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:28",
      "total_flops_so_far": 2253637130194944.0,
      "budget_used_percent": 2.253637130194944
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:28",
      "total_flops_so_far": 2262761167159296.0,
      "budget_used_percent": 2.262761167159296
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:29",
      "total_flops_so_far": 2271885204123648.0,
      "budget_used_percent": 2.271885204123648
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:29",
      "total_flops_so_far": 2281009241088000.0,
      "budget_used_percent": 2.2810092410880003
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:30",
      "total_flops_so_far": 2290133278052352.0,
      "budget_used_percent": 2.2901332780523522
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:30",
      "total_flops_so_far": 2299257315016704.0,
      "budget_used_percent": 2.299257315016704
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:30",
      "total_flops_so_far": 2308381351981056.0,
      "budget_used_percent": 2.308381351981056
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:31",
      "total_flops_so_far": 2317505388945408.0,
      "budget_used_percent": 2.317505388945408
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:31",
      "total_flops_so_far": 2326629425909760.0,
      "budget_used_percent": 2.32662942590976
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:32",
      "total_flops_so_far": 2335753462874112.0,
      "budget_used_percent": 2.335753462874112
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:32",
      "total_flops_so_far": 2344877499838464.0,
      "budget_used_percent": 2.3448774998384643
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:33",
      "total_flops_so_far": 2354001536802816.0,
      "budget_used_percent": 2.354001536802816
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:33",
      "total_flops_so_far": 2363125573767168.0,
      "budget_used_percent": 2.363125573767168
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:33",
      "total_flops_so_far": 2372249610731520.0,
      "budget_used_percent": 2.37224961073152
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:34",
      "total_flops_so_far": 2381373647695872.0,
      "budget_used_percent": 2.381373647695872
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:34",
      "total_flops_so_far": 2390497684660224.0,
      "budget_used_percent": 2.390497684660224
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:35",
      "total_flops_so_far": 2399621721624576.0,
      "budget_used_percent": 2.3996217216245763
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:35",
      "total_flops_so_far": 2408745758588928.0,
      "budget_used_percent": 2.4087457585889283
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:36",
      "total_flops_so_far": 2417869795553280.0,
      "budget_used_percent": 2.41786979555328
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:36",
      "total_flops_so_far": 2426993832517632.0,
      "budget_used_percent": 2.426993832517632
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:37",
      "total_flops_so_far": 2436117869481984.0,
      "budget_used_percent": 2.436117869481984
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:37",
      "total_flops_so_far": 2445241906446336.0,
      "budget_used_percent": 2.445241906446336
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:37",
      "total_flops_so_far": 2454365943410688.0,
      "budget_used_percent": 2.454365943410688
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:38",
      "total_flops_so_far": 2463489980375040.0,
      "budget_used_percent": 2.4634899803750403
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:38",
      "total_flops_so_far": 2472614017339392.0,
      "budget_used_percent": 2.4726140173393922
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:39",
      "total_flops_so_far": 2481738054303744.0,
      "budget_used_percent": 2.481738054303744
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:39",
      "total_flops_so_far": 2490862091268096.0,
      "budget_used_percent": 2.490862091268096
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:40",
      "total_flops_so_far": 2499986128232448.0,
      "budget_used_percent": 2.499986128232448
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:40",
      "total_flops_so_far": 2509110165196800.0,
      "budget_used_percent": 2.5091101651968
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:40",
      "total_flops_so_far": 2518234202161152.0,
      "budget_used_percent": 2.5182342021611523
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:41",
      "total_flops_so_far": 2527358239125504.0,
      "budget_used_percent": 2.5273582391255043
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:41",
      "total_flops_so_far": 2536482276089856.0,
      "budget_used_percent": 2.536482276089856
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:42",
      "total_flops_so_far": 2545606313054208.0,
      "budget_used_percent": 2.545606313054208
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:42",
      "total_flops_so_far": 2554730350018560.0,
      "budget_used_percent": 2.55473035001856
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:43",
      "total_flops_so_far": 2563854386982912.0,
      "budget_used_percent": 2.563854386982912
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:43",
      "total_flops_so_far": 2572978423947264.0,
      "budget_used_percent": 2.572978423947264
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:44",
      "total_flops_so_far": 2582102460911616.0,
      "budget_used_percent": 2.582102460911616
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:44",
      "total_flops_so_far": 2591226497875968.0,
      "budget_used_percent": 2.591226497875968
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:44",
      "total_flops_so_far": 2600350534840320.0,
      "budget_used_percent": 2.6003505348403197
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:45",
      "total_flops_so_far": 2609474571804672.0,
      "budget_used_percent": 2.6094745718046717
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:45",
      "total_flops_so_far": 2618598608769024.0,
      "budget_used_percent": 2.618598608769024
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:46",
      "total_flops_so_far": 2627722645733376.0,
      "budget_used_percent": 2.627722645733376
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:46",
      "total_flops_so_far": 2636846682697728.0,
      "budget_used_percent": 2.636846682697728
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:47",
      "total_flops_so_far": 2645970719662080.0,
      "budget_used_percent": 2.64597071966208
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:47",
      "total_flops_so_far": 2655094756626432.0,
      "budget_used_percent": 2.655094756626432
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:47",
      "total_flops_so_far": 2664218793590784.0,
      "budget_used_percent": 2.6642187935907837
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:48",
      "total_flops_so_far": 2673342830555136.0,
      "budget_used_percent": 2.6733428305551357
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:48",
      "total_flops_so_far": 2682466867519488.0,
      "budget_used_percent": 2.682466867519488
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:49",
      "total_flops_so_far": 2691590904483840.0,
      "budget_used_percent": 2.69159090448384
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:49",
      "total_flops_so_far": 2700714941448192.0,
      "budget_used_percent": 2.700714941448192
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:50",
      "total_flops_so_far": 2709838978412544.0,
      "budget_used_percent": 2.709838978412544
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:50",
      "total_flops_so_far": 2718963015376896.0,
      "budget_used_percent": 2.7189630153768958
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:51",
      "total_flops_so_far": 2728087052341248.0,
      "budget_used_percent": 2.7280870523412477
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:51",
      "total_flops_so_far": 2737211089305600.0,
      "budget_used_percent": 2.7372110893056
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:51",
      "total_flops_so_far": 2746335126269952.0,
      "budget_used_percent": 2.746335126269952
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:52",
      "total_flops_so_far": 2755459163234304.0,
      "budget_used_percent": 2.755459163234304
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:52",
      "total_flops_so_far": 2764583200198656.0,
      "budget_used_percent": 2.764583200198656
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:53",
      "total_flops_so_far": 2773707237163008.0,
      "budget_used_percent": 2.773707237163008
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:53",
      "total_flops_so_far": 2782831274127360.0,
      "budget_used_percent": 2.7828312741273598
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:54",
      "total_flops_so_far": 2791955311091712.0,
      "budget_used_percent": 2.7919553110917117
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:54",
      "total_flops_so_far": 2801079348056064.0,
      "budget_used_percent": 2.801079348056064
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:54",
      "total_flops_so_far": 2810203385020416.0,
      "budget_used_percent": 2.810203385020416
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:55",
      "total_flops_so_far": 2819327421984768.0,
      "budget_used_percent": 2.819327421984768
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:55",
      "total_flops_so_far": 2828451458949120.0,
      "budget_used_percent": 2.82845145894912
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:56",
      "total_flops_so_far": 2837575495913472.0,
      "budget_used_percent": 2.837575495913472
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:56",
      "total_flops_so_far": 2846699532877824.0,
      "budget_used_percent": 2.8466995328778237
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:57",
      "total_flops_so_far": 2855823569842176.0,
      "budget_used_percent": 2.855823569842176
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:57",
      "total_flops_so_far": 2864947606806528.0,
      "budget_used_percent": 2.864947606806528
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:58",
      "total_flops_so_far": 2874071643770880.0,
      "budget_used_percent": 2.87407164377088
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:58",
      "total_flops_so_far": 2883195680735232.0,
      "budget_used_percent": 2.883195680735232
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:58",
      "total_flops_so_far": 2892319717699584.0,
      "budget_used_percent": 2.892319717699584
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:59",
      "total_flops_so_far": 2901443754663936.0,
      "budget_used_percent": 2.901443754663936
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:49:59",
      "total_flops_so_far": 2910567791628288.0,
      "budget_used_percent": 2.9105677916282877
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:00",
      "total_flops_so_far": 2919691828592640.0,
      "budget_used_percent": 2.91969182859264
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:00",
      "total_flops_so_far": 2928815865556992.0,
      "budget_used_percent": 2.928815865556992
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:01",
      "total_flops_so_far": 2937939902521344.0,
      "budget_used_percent": 2.937939902521344
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:01",
      "total_flops_so_far": 2947063939485696.0,
      "budget_used_percent": 2.947063939485696
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:01",
      "total_flops_so_far": 2956187976450048.0,
      "budget_used_percent": 2.956187976450048
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:02",
      "total_flops_so_far": 2965312013414400.0,
      "budget_used_percent": 2.9653120134143998
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:02",
      "total_flops_so_far": 2974436050378752.0,
      "budget_used_percent": 2.974436050378752
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:03",
      "total_flops_so_far": 2983560087343104.0,
      "budget_used_percent": 2.983560087343104
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:03",
      "total_flops_so_far": 2992684124307456.0,
      "budget_used_percent": 2.992684124307456
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:04",
      "total_flops_so_far": 3001808161271808.0,
      "budget_used_percent": 3.001808161271808
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:04",
      "total_flops_so_far": 3010932198236160.0,
      "budget_used_percent": 3.01093219823616
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:05",
      "total_flops_so_far": 3020056235200512.0,
      "budget_used_percent": 3.020056235200512
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:05",
      "total_flops_so_far": 3029180272164864.0,
      "budget_used_percent": 3.029180272164864
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:05",
      "total_flops_so_far": 3038304309129216.0,
      "budget_used_percent": 3.038304309129216
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:06",
      "total_flops_so_far": 3047428346093568.0,
      "budget_used_percent": 3.047428346093568
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:06",
      "total_flops_so_far": 3056552383057920.0,
      "budget_used_percent": 3.05655238305792
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:07",
      "total_flops_so_far": 3065676420022272.0,
      "budget_used_percent": 3.065676420022272
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:07",
      "total_flops_so_far": 3074800456986624.0,
      "budget_used_percent": 3.074800456986624
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:08",
      "total_flops_so_far": 3083924493950976.0,
      "budget_used_percent": 3.083924493950976
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:08",
      "total_flops_so_far": 3093048530915328.0,
      "budget_used_percent": 3.093048530915328
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:08",
      "total_flops_so_far": 3102172567879680.0,
      "budget_used_percent": 3.10217256787968
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:09",
      "total_flops_so_far": 3111296604844032.0,
      "budget_used_percent": 3.111296604844032
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:09",
      "total_flops_so_far": 3120420641808384.0,
      "budget_used_percent": 3.120420641808384
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:10",
      "total_flops_so_far": 3129544678772736.0,
      "budget_used_percent": 3.129544678772736
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:10",
      "total_flops_so_far": 3138668715737088.0,
      "budget_used_percent": 3.138668715737088
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:11",
      "total_flops_so_far": 3147792752701440.0,
      "budget_used_percent": 3.14779275270144
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:11",
      "total_flops_so_far": 3156916789665792.0,
      "budget_used_percent": 3.1569167896657917
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:12",
      "total_flops_so_far": 3166040826630144.0,
      "budget_used_percent": 3.166040826630144
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:12",
      "total_flops_so_far": 3175164863594496.0,
      "budget_used_percent": 3.1751648635944956
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:12",
      "total_flops_so_far": 3184288900558848.0,
      "budget_used_percent": 3.184288900558848
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:13",
      "total_flops_so_far": 3193412937523200.0,
      "budget_used_percent": 3.1934129375232
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:13",
      "total_flops_so_far": 3202536974487552.0,
      "budget_used_percent": 3.2025369744875523
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:14",
      "total_flops_so_far": 3211661011451904.0,
      "budget_used_percent": 3.2116610114519037
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:14",
      "total_flops_so_far": 3220785048416256.0,
      "budget_used_percent": 3.220785048416256
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:15",
      "total_flops_so_far": 3229909085380608.0,
      "budget_used_percent": 3.2299090853806076
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:15",
      "total_flops_so_far": 3239033122344960.0,
      "budget_used_percent": 3.23903312234496
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:15",
      "total_flops_so_far": 3248157159309312.0,
      "budget_used_percent": 3.2481571593093115
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:16",
      "total_flops_so_far": 3257281196273664.0,
      "budget_used_percent": 3.257281196273664
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:16",
      "total_flops_so_far": 3266405233238016.0,
      "budget_used_percent": 3.266405233238016
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:17",
      "total_flops_so_far": 3275529270202368.0,
      "budget_used_percent": 3.275529270202368
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:17",
      "total_flops_so_far": 3284653307166720.0,
      "budget_used_percent": 3.2846533071667197
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:18",
      "total_flops_so_far": 3293777344131072.0,
      "budget_used_percent": 3.293777344131072
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:18",
      "total_flops_so_far": 3302901381095424.0,
      "budget_used_percent": 3.3029013810954235
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:19",
      "total_flops_so_far": 3312025418059776.0,
      "budget_used_percent": 3.312025418059776
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:19",
      "total_flops_so_far": 3321149455024128.0,
      "budget_used_percent": 3.321149455024128
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:19",
      "total_flops_so_far": 3330273491988480.0,
      "budget_used_percent": 3.33027349198848
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:20",
      "total_flops_so_far": 3339397528952832.0,
      "budget_used_percent": 3.3393975289528317
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:20",
      "total_flops_so_far": 3348521565917184.0,
      "budget_used_percent": 3.348521565917184
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:21",
      "total_flops_so_far": 3357645602881536.0,
      "budget_used_percent": 3.3576456028815356
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:21",
      "total_flops_so_far": 3366769639845888.0,
      "budget_used_percent": 3.366769639845888
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:22",
      "total_flops_so_far": 3375893676810240.0,
      "budget_used_percent": 3.37589367681024
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:22",
      "total_flops_so_far": 3385017713774592.0,
      "budget_used_percent": 3.3850177137745923
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:23",
      "total_flops_so_far": 3394141750738944.0,
      "budget_used_percent": 3.3941417507389438
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:23",
      "total_flops_so_far": 3403265787703296.0,
      "budget_used_percent": 3.403265787703296
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:23",
      "total_flops_so_far": 3412389824667648.0,
      "budget_used_percent": 3.4123898246676476
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:24",
      "total_flops_so_far": 3421513861632000.0,
      "budget_used_percent": 3.421513861632
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:24",
      "total_flops_so_far": 3430637898596352.0,
      "budget_used_percent": 3.430637898596352
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:25",
      "total_flops_so_far": 3439761935560704.0,
      "budget_used_percent": 3.4397619355607043
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:25",
      "total_flops_so_far": 3448885972525056.0,
      "budget_used_percent": 3.448885972525056
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:26",
      "total_flops_so_far": 3458010009489408.0,
      "budget_used_percent": 3.458010009489408
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:26",
      "total_flops_so_far": 3467134046453760.0,
      "budget_used_percent": 3.4671340464537597
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:26",
      "total_flops_so_far": 3476258083418112.0,
      "budget_used_percent": 3.476258083418112
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:27",
      "total_flops_so_far": 3485382120382464.0,
      "budget_used_percent": 3.485382120382464
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:27",
      "total_flops_so_far": 3494506157346816.0,
      "budget_used_percent": 3.494506157346816
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:28",
      "total_flops_so_far": 3503630194311168.0,
      "budget_used_percent": 3.503630194311168
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:28",
      "total_flops_so_far": 3512754231275520.0,
      "budget_used_percent": 3.5127542312755202
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:29",
      "total_flops_so_far": 3521878268239872.0,
      "budget_used_percent": 3.5218782682398717
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:29",
      "total_flops_so_far": 3531002305204224.0,
      "budget_used_percent": 3.531002305204224
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:30",
      "total_flops_so_far": 3540126342168576.0,
      "budget_used_percent": 3.540126342168576
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:30",
      "total_flops_so_far": 3549250379132928.0,
      "budget_used_percent": 3.549250379132928
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:30",
      "total_flops_so_far": 3558374416097280.0,
      "budget_used_percent": 3.55837441609728
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:31",
      "total_flops_so_far": 3567498453061632.0,
      "budget_used_percent": 3.5674984530616323
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:31",
      "total_flops_so_far": 3576622490025984.0,
      "budget_used_percent": 3.5766224900259838
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:32",
      "total_flops_so_far": 3585746526990336.0,
      "budget_used_percent": 3.585746526990336
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:32",
      "total_flops_so_far": 3594870563954688.0,
      "budget_used_percent": 3.5948705639546876
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:33",
      "total_flops_so_far": 3603994600919040.0,
      "budget_used_percent": 3.60399460091904
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:33",
      "total_flops_so_far": 3613118637883392.0,
      "budget_used_percent": 3.613118637883392
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:34",
      "total_flops_so_far": 3622242674847744.0,
      "budget_used_percent": 3.6222426748477443
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:34",
      "total_flops_so_far": 3631366711812096.0,
      "budget_used_percent": 3.631366711812096
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:34",
      "total_flops_so_far": 3640490748776448.0,
      "budget_used_percent": 3.640490748776448
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:35",
      "total_flops_so_far": 3649614785740800.0,
      "budget_used_percent": 3.6496147857407997
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:35",
      "total_flops_so_far": 3658738822705152.0,
      "budget_used_percent": 3.658738822705152
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:36",
      "total_flops_so_far": 3667862859669504.0,
      "budget_used_percent": 3.667862859669504
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:36",
      "total_flops_so_far": 3676986896633856.0,
      "budget_used_percent": 3.6769868966338564
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:37",
      "total_flops_so_far": 3686110933598208.0,
      "budget_used_percent": 3.686110933598208
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:37",
      "total_flops_so_far": 3695234970562560.0,
      "budget_used_percent": 3.6952349705625602
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:37",
      "total_flops_so_far": 3704359007526912.0,
      "budget_used_percent": 3.7043590075269117
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:38",
      "total_flops_so_far": 3713483044491264.0,
      "budget_used_percent": 3.713483044491264
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:38",
      "total_flops_so_far": 3722607081455616.0,
      "budget_used_percent": 3.722607081455616
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:39",
      "total_flops_so_far": 3731731118419968.0,
      "budget_used_percent": 3.7317311184199684
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:39",
      "total_flops_so_far": 3740855155384320.0,
      "budget_used_percent": 3.74085515538432
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:40",
      "total_flops_so_far": 3749979192348672.0,
      "budget_used_percent": 3.7499791923486723
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:40",
      "total_flops_so_far": 3759103229313024.0,
      "budget_used_percent": 3.7591032293130238
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:41",
      "total_flops_so_far": 3768227266277376.0,
      "budget_used_percent": 3.768227266277376
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:41",
      "total_flops_so_far": 3777351303241728.0,
      "budget_used_percent": 3.777351303241728
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:41",
      "total_flops_so_far": 3786475340206080.0,
      "budget_used_percent": 3.7864753402060805
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:42",
      "total_flops_so_far": 3795599377170432.0,
      "budget_used_percent": 3.795599377170432
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:42",
      "total_flops_so_far": 3804723414134784.0,
      "budget_used_percent": 3.8047234141347843
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:43",
      "total_flops_so_far": 3813847451099136.0,
      "budget_used_percent": 3.813847451099136
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:43",
      "total_flops_so_far": 3822971488063488.0,
      "budget_used_percent": 3.822971488063488
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:44",
      "total_flops_so_far": 3832095525027840.0,
      "budget_used_percent": 3.8320955250278397
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:44",
      "total_flops_so_far": 3841219561992192.0,
      "budget_used_percent": 3.841219561992192
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:45",
      "total_flops_so_far": 3850343598956544.0,
      "budget_used_percent": 3.850343598956544
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:45",
      "total_flops_so_far": 3859467635920896.0,
      "budget_used_percent": 3.8594676359208964
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:45",
      "total_flops_so_far": 3868591672885248.0,
      "budget_used_percent": 3.868591672885248
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:46",
      "total_flops_so_far": 3877715709849600.0,
      "budget_used_percent": 3.8777157098496002
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:46",
      "total_flops_so_far": 3886839746813952.0,
      "budget_used_percent": 3.8868397468139517
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:47",
      "total_flops_so_far": 3895963783778304.0,
      "budget_used_percent": 3.895963783778304
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:47",
      "total_flops_so_far": 3905087820742656.0,
      "budget_used_percent": 3.905087820742656
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:48",
      "total_flops_so_far": 3914211857707008.0,
      "budget_used_percent": 3.9142118577070084
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:48",
      "total_flops_so_far": 3923335894671360.0,
      "budget_used_percent": 3.92333589467136
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:48",
      "total_flops_so_far": 3932459931635712.0,
      "budget_used_percent": 3.9324599316357123
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:49",
      "total_flops_so_far": 3941583968600064.0,
      "budget_used_percent": 3.9415839686000638
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:49",
      "total_flops_so_far": 3950708005564416.0,
      "budget_used_percent": 3.950708005564416
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:50",
      "total_flops_so_far": 3959832042528768.0,
      "budget_used_percent": 3.959832042528768
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:50",
      "total_flops_so_far": 3968956079493120.0,
      "budget_used_percent": 3.9689560794931205
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:51",
      "total_flops_so_far": 3978080116457472.0,
      "budget_used_percent": 3.978080116457472
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:51",
      "total_flops_so_far": 3987204153421824.0,
      "budget_used_percent": 3.9872041534218243
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:52",
      "total_flops_so_far": 3996328190386176.0,
      "budget_used_percent": 3.996328190386176
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:52",
      "total_flops_so_far": 4005452227350528.0,
      "budget_used_percent": 4.005452227350529
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:52",
      "total_flops_so_far": 4014576264314880.0,
      "budget_used_percent": 4.01457626431488
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:53",
      "total_flops_so_far": 4023700301279232.0,
      "budget_used_percent": 4.0237003012792325
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:53",
      "total_flops_so_far": 4032824338243584.0,
      "budget_used_percent": 4.032824338243584
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:54",
      "total_flops_so_far": 4041948375207936.0,
      "budget_used_percent": 4.041948375207936
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:54",
      "total_flops_so_far": 4051072412172288.0,
      "budget_used_percent": 4.051072412172288
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:55",
      "total_flops_so_far": 4060196449136640.0,
      "budget_used_percent": 4.06019644913664
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:55",
      "total_flops_so_far": 4069320486100992.0,
      "budget_used_percent": 4.069320486100992
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:56",
      "total_flops_so_far": 4078444523065344.0,
      "budget_used_percent": 4.078444523065344
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:56",
      "total_flops_so_far": 4087568560029696.0,
      "budget_used_percent": 4.087568560029696
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:56",
      "total_flops_so_far": 4096692596994048.0,
      "budget_used_percent": 4.096692596994048
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:57",
      "total_flops_so_far": 4105816633958400.0,
      "budget_used_percent": 4.1058166339584
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:57",
      "total_flops_so_far": 4114940670922752.0,
      "budget_used_percent": 4.114940670922753
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:58",
      "total_flops_so_far": 4124064707887104.0,
      "budget_used_percent": 4.124064707887104
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:58",
      "total_flops_so_far": 4133188744851456.0,
      "budget_used_percent": 4.133188744851456
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:59",
      "total_flops_so_far": 4142312781815808.0,
      "budget_used_percent": 4.142312781815808
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:50:59",
      "total_flops_so_far": 4151436818780160.0,
      "budget_used_percent": 4.15143681878016
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:00",
      "total_flops_so_far": 4160560855744512.0,
      "budget_used_percent": 4.160560855744512
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:00",
      "total_flops_so_far": 4169684892708864.0,
      "budget_used_percent": 4.169684892708863
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:00",
      "total_flops_so_far": 4178808929673216.0,
      "budget_used_percent": 4.178808929673216
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:01",
      "total_flops_so_far": 4187932966637568.0,
      "budget_used_percent": 4.187932966637567
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:01",
      "total_flops_so_far": 4197057003601920.0,
      "budget_used_percent": 4.19705700360192
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:02",
      "total_flops_so_far": 4206181040566272.0,
      "budget_used_percent": 4.206181040566272
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:02",
      "total_flops_so_far": 4215305077530624.0,
      "budget_used_percent": 4.2153050775306244
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:03",
      "total_flops_so_far": 4224429114494976.0,
      "budget_used_percent": 4.224429114494976
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:03",
      "total_flops_so_far": 4233553151459328.0,
      "budget_used_percent": 4.233553151459328
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:03",
      "total_flops_so_far": 4242677188423680.0,
      "budget_used_percent": 4.24267718842368
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:04",
      "total_flops_so_far": 4251801225388032.0,
      "budget_used_percent": 4.251801225388032
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:04",
      "total_flops_so_far": 4260925262352384.0,
      "budget_used_percent": 4.260925262352384
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:05",
      "total_flops_so_far": 4270049299316736.0,
      "budget_used_percent": 4.270049299316736
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:05",
      "total_flops_so_far": 4279173336281088.0,
      "budget_used_percent": 4.2791733362810875
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:06",
      "total_flops_so_far": 4288297373245440.0,
      "budget_used_percent": 4.28829737324544
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:06",
      "total_flops_so_far": 4297421410209792.0,
      "budget_used_percent": 4.297421410209791
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:07",
      "total_flops_so_far": 4306545447174144.0,
      "budget_used_percent": 4.306545447174144
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:07",
      "total_flops_so_far": 4315669484138496.0,
      "budget_used_percent": 4.315669484138496
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:07",
      "total_flops_so_far": 4324793521102848.0,
      "budget_used_percent": 4.324793521102848
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:08",
      "total_flops_so_far": 4333917558067200.0,
      "budget_used_percent": 4.3339175580672
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:08",
      "total_flops_so_far": 4343041595031552.0,
      "budget_used_percent": 4.343041595031552
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:09",
      "total_flops_so_far": 4352165631995904.0,
      "budget_used_percent": 4.352165631995904
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:09",
      "total_flops_so_far": 4361289668960256.0,
      "budget_used_percent": 4.361289668960256
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:10",
      "total_flops_so_far": 4370413705924608.0,
      "budget_used_percent": 4.370413705924608
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:10",
      "total_flops_so_far": 4379537742888960.0,
      "budget_used_percent": 4.37953774288896
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:11",
      "total_flops_so_far": 4388661779853312.0,
      "budget_used_percent": 4.388661779853312
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:11",
      "total_flops_so_far": 4397785816817664.0,
      "budget_used_percent": 4.397785816817664
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:11",
      "total_flops_so_far": 4406909853782016.0,
      "budget_used_percent": 4.4069098537820155
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:12",
      "total_flops_so_far": 4416033890746368.0,
      "budget_used_percent": 4.416033890746368
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:12",
      "total_flops_so_far": 4425157927710720.0,
      "budget_used_percent": 4.425157927710719
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:13",
      "total_flops_so_far": 4434281964675072.0,
      "budget_used_percent": 4.434281964675072
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:13",
      "total_flops_so_far": 4443406001639424.0,
      "budget_used_percent": 4.443406001639424
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:14",
      "total_flops_so_far": 4452530038603776.0,
      "budget_used_percent": 4.4525300386037765
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:14",
      "total_flops_so_far": 4461654075568128.0,
      "budget_used_percent": 4.461654075568128
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:15",
      "total_flops_so_far": 4470778112532480.0,
      "budget_used_percent": 4.47077811253248
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:15",
      "total_flops_so_far": 4479902149496832.0,
      "budget_used_percent": 4.479902149496832
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:15",
      "total_flops_so_far": 4489026186461184.0,
      "budget_used_percent": 4.489026186461184
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:16",
      "total_flops_so_far": 4498150223425536.0,
      "budget_used_percent": 4.498150223425536
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:16",
      "total_flops_so_far": 4507274260389888.0,
      "budget_used_percent": 4.507274260389888
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:17",
      "total_flops_so_far": 4516398297354240.0,
      "budget_used_percent": 4.51639829735424
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:17",
      "total_flops_so_far": 4525522334318592.0,
      "budget_used_percent": 4.525522334318592
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:18",
      "total_flops_so_far": 4534646371282944.0,
      "budget_used_percent": 4.5346463712829435
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:18",
      "total_flops_so_far": 4543770408247296.0,
      "budget_used_percent": 4.543770408247296
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:19",
      "total_flops_so_far": 4552894445211648.0,
      "budget_used_percent": 4.552894445211648
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:51:19",
      "total_flops_so_far": 4562018482176000.0,
      "budget_used_percent": 4.562018482176001
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:44",
      "total_flops_so_far": 4571142519140352.0,
      "budget_used_percent": 4.571142519140352
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:44",
      "total_flops_so_far": 4580266556104704.0,
      "budget_used_percent": 4.5802665561047045
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:45",
      "total_flops_so_far": 4589390593069056.0,
      "budget_used_percent": 4.589390593069056
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:45",
      "total_flops_so_far": 4598514630033408.0,
      "budget_used_percent": 4.598514630033408
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:45",
      "total_flops_so_far": 4607638666997760.0,
      "budget_used_percent": 4.60763866699776
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:46",
      "total_flops_so_far": 4616762703962112.0,
      "budget_used_percent": 4.616762703962112
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:46",
      "total_flops_so_far": 4625886740926464.0,
      "budget_used_percent": 4.625886740926464
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:47",
      "total_flops_so_far": 4635010777890816.0,
      "budget_used_percent": 4.635010777890816
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:47",
      "total_flops_so_far": 4644134814855168.0,
      "budget_used_percent": 4.6441348148551675
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:48",
      "total_flops_so_far": 4653258851819520.0,
      "budget_used_percent": 4.65325885181952
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:48",
      "total_flops_so_far": 4662382888783872.0,
      "budget_used_percent": 4.662382888783871
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:48",
      "total_flops_so_far": 4671506925748224.0,
      "budget_used_percent": 4.671506925748224
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:49",
      "total_flops_so_far": 4680630962712576.0,
      "budget_used_percent": 4.680630962712576
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:49",
      "total_flops_so_far": 4689754999676928.0,
      "budget_used_percent": 4.6897549996769285
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:50",
      "total_flops_so_far": 4698879036641280.0,
      "budget_used_percent": 4.69887903664128
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:50",
      "total_flops_so_far": 4708003073605632.0,
      "budget_used_percent": 4.708003073605632
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:51",
      "total_flops_so_far": 4717127110569984.0,
      "budget_used_percent": 4.717127110569984
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:51",
      "total_flops_so_far": 4726251147534336.0,
      "budget_used_percent": 4.726251147534336
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:52",
      "total_flops_so_far": 4735375184498688.0,
      "budget_used_percent": 4.735375184498688
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:52",
      "total_flops_so_far": 4744499221463040.0,
      "budget_used_percent": 4.74449922146304
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:52",
      "total_flops_so_far": 4753623258427392.0,
      "budget_used_percent": 4.753623258427392
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:53",
      "total_flops_so_far": 4762747295391744.0,
      "budget_used_percent": 4.762747295391744
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:53",
      "total_flops_so_far": 4771871332356096.0,
      "budget_used_percent": 4.7718713323560955
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:54",
      "total_flops_so_far": 4780995369320448.0,
      "budget_used_percent": 4.780995369320448
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:54",
      "total_flops_so_far": 4790119406284800.0,
      "budget_used_percent": 4.7901194062848
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:55",
      "total_flops_so_far": 4799243443249152.0,
      "budget_used_percent": 4.799243443249153
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:55",
      "total_flops_so_far": 4808367480213504.0,
      "budget_used_percent": 4.808367480213504
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:56",
      "total_flops_so_far": 4817491517177856.0,
      "budget_used_percent": 4.8174915171778565
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:56",
      "total_flops_so_far": 4826615554142208.0,
      "budget_used_percent": 4.826615554142208
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:56",
      "total_flops_so_far": 4835739591106560.0,
      "budget_used_percent": 4.83573959110656
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:57",
      "total_flops_so_far": 4844863628070912.0,
      "budget_used_percent": 4.844863628070912
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:57",
      "total_flops_so_far": 4853987665035264.0,
      "budget_used_percent": 4.853987665035264
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:58",
      "total_flops_so_far": 4863111701999616.0,
      "budget_used_percent": 4.863111701999616
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:58",
      "total_flops_so_far": 4872235738963968.0,
      "budget_used_percent": 4.872235738963968
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:59",
      "total_flops_so_far": 4881359775928320.0,
      "budget_used_percent": 4.88135977592832
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:53:59",
      "total_flops_so_far": 4890483812892672.0,
      "budget_used_percent": 4.890483812892672
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:00",
      "total_flops_so_far": 4899607849857024.0,
      "budget_used_percent": 4.8996078498570235
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:00",
      "total_flops_so_far": 4908731886821376.0,
      "budget_used_percent": 4.908731886821376
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:00",
      "total_flops_so_far": 4917855923785728.0,
      "budget_used_percent": 4.917855923785728
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:01",
      "total_flops_so_far": 4926979960750080.0,
      "budget_used_percent": 4.926979960750081
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:01",
      "total_flops_so_far": 4936103997714432.0,
      "budget_used_percent": 4.936103997714432
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:02",
      "total_flops_so_far": 4945228034678784.0,
      "budget_used_percent": 4.9452280346787845
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:02",
      "total_flops_so_far": 4954352071643136.0,
      "budget_used_percent": 4.954352071643136
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:03",
      "total_flops_so_far": 4963476108607488.0,
      "budget_used_percent": 4.963476108607488
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:03",
      "total_flops_so_far": 4972600145571840.0,
      "budget_used_percent": 4.97260014557184
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:04",
      "total_flops_so_far": 4981724182536192.0,
      "budget_used_percent": 4.981724182536192
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:04",
      "total_flops_so_far": 4990848219500544.0,
      "budget_used_percent": 4.990848219500544
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:04",
      "total_flops_so_far": 4999972256464896.0,
      "budget_used_percent": 4.999972256464896
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:05",
      "total_flops_so_far": 5009096293429248.0,
      "budget_used_percent": 5.009096293429248
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:05",
      "total_flops_so_far": 5018220330393600.0,
      "budget_used_percent": 5.0182203303936
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:06",
      "total_flops_so_far": 5027344367357952.0,
      "budget_used_percent": 5.027344367357952
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:06",
      "total_flops_so_far": 5036468404322304.0,
      "budget_used_percent": 5.036468404322305
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:07",
      "total_flops_so_far": 5045592441286656.0,
      "budget_used_percent": 5.045592441286656
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:07",
      "total_flops_so_far": 5054716478251008.0,
      "budget_used_percent": 5.054716478251009
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:08",
      "total_flops_so_far": 5063840515215360.0,
      "budget_used_percent": 5.06384051521536
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:08",
      "total_flops_so_far": 5072964552179712.0,
      "budget_used_percent": 5.072964552179712
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:08",
      "total_flops_so_far": 5082088589144064.0,
      "budget_used_percent": 5.082088589144064
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:09",
      "total_flops_so_far": 5091212626108416.0,
      "budget_used_percent": 5.091212626108416
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:09",
      "total_flops_so_far": 5100336663072768.0,
      "budget_used_percent": 5.100336663072768
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:10",
      "total_flops_so_far": 5109460700037120.0,
      "budget_used_percent": 5.10946070003712
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:10",
      "total_flops_so_far": 5118584737001472.0,
      "budget_used_percent": 5.118584737001472
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:11",
      "total_flops_so_far": 5127708773965824.0,
      "budget_used_percent": 5.127708773965824
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:11",
      "total_flops_so_far": 5136832810930176.0,
      "budget_used_percent": 5.136832810930176
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:12",
      "total_flops_so_far": 5145956847894528.0,
      "budget_used_percent": 5.145956847894528
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:12",
      "total_flops_so_far": 5155080884858880.0,
      "budget_used_percent": 5.15508088485888
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:12",
      "total_flops_so_far": 5164204921823232.0,
      "budget_used_percent": 5.164204921823232
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:13",
      "total_flops_so_far": 5173328958787584.0,
      "budget_used_percent": 5.173328958787584
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:13",
      "total_flops_so_far": 5182452995751936.0,
      "budget_used_percent": 5.182452995751936
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:14",
      "total_flops_so_far": 5191577032716288.0,
      "budget_used_percent": 5.191577032716288
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:14",
      "total_flops_so_far": 5200701069680640.0,
      "budget_used_percent": 5.2007010696806395
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:15",
      "total_flops_so_far": 5209825106644992.0,
      "budget_used_percent": 5.209825106644992
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:15",
      "total_flops_so_far": 5218949143609344.0,
      "budget_used_percent": 5.218949143609343
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:16",
      "total_flops_so_far": 5228073180573696.0,
      "budget_used_percent": 5.228073180573696
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:16",
      "total_flops_so_far": 5237197217538048.0,
      "budget_used_percent": 5.237197217538048
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:16",
      "total_flops_so_far": 5246321254502400.0,
      "budget_used_percent": 5.2463212545024
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:17",
      "total_flops_so_far": 5255445291466752.0,
      "budget_used_percent": 5.255445291466752
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:17",
      "total_flops_so_far": 5264569328431104.0,
      "budget_used_percent": 5.264569328431104
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:18",
      "total_flops_so_far": 5273693365395456.0,
      "budget_used_percent": 5.273693365395456
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:18",
      "total_flops_so_far": 5282817402359808.0,
      "budget_used_percent": 5.282817402359808
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:19",
      "total_flops_so_far": 5291941439324160.0,
      "budget_used_percent": 5.29194143932416
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:19",
      "total_flops_so_far": 5301065476288512.0,
      "budget_used_percent": 5.301065476288512
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:20",
      "total_flops_so_far": 5310189513252864.0,
      "budget_used_percent": 5.310189513252864
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:20",
      "total_flops_so_far": 5319313550217216.0,
      "budget_used_percent": 5.319313550217216
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:20",
      "total_flops_so_far": 5328437587181568.0,
      "budget_used_percent": 5.3284375871815675
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:21",
      "total_flops_so_far": 5337561624145920.0,
      "budget_used_percent": 5.33756162414592
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:21",
      "total_flops_so_far": 5346685661110272.0,
      "budget_used_percent": 5.346685661110271
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:22",
      "total_flops_so_far": 5355809698074624.0,
      "budget_used_percent": 5.355809698074624
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:22",
      "total_flops_so_far": 5364933735038976.0,
      "budget_used_percent": 5.364933735038976
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:23",
      "total_flops_so_far": 5374057772003328.0,
      "budget_used_percent": 5.3740577720033285
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:23",
      "total_flops_so_far": 5383181808967680.0,
      "budget_used_percent": 5.38318180896768
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:24",
      "total_flops_so_far": 5392305845932032.0,
      "budget_used_percent": 5.392305845932032
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:24",
      "total_flops_so_far": 5401429882896384.0,
      "budget_used_percent": 5.401429882896384
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:24",
      "total_flops_so_far": 5410553919860736.0,
      "budget_used_percent": 5.410553919860736
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:25",
      "total_flops_so_far": 5419677956825088.0,
      "budget_used_percent": 5.419677956825088
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:25",
      "total_flops_so_far": 5428801993789440.0,
      "budget_used_percent": 5.42880199378944
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:26",
      "total_flops_so_far": 5437926030753792.0,
      "budget_used_percent": 5.4379260307537916
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:26",
      "total_flops_so_far": 5447050067718144.0,
      "budget_used_percent": 5.447050067718144
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:27",
      "total_flops_so_far": 5456174104682496.0,
      "budget_used_percent": 5.456174104682495
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:27",
      "total_flops_so_far": 5465298141646848.0,
      "budget_used_percent": 5.465298141646848
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:28",
      "total_flops_so_far": 5474422178611200.0,
      "budget_used_percent": 5.4744221786112
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:28",
      "total_flops_so_far": 5483546215575552.0,
      "budget_used_percent": 5.4835462155755526
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:28",
      "total_flops_so_far": 5492670252539904.0,
      "budget_used_percent": 5.492670252539904
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:29",
      "total_flops_so_far": 5501794289504256.0,
      "budget_used_percent": 5.501794289504256
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:29",
      "total_flops_so_far": 5510918326468608.0,
      "budget_used_percent": 5.510918326468608
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:30",
      "total_flops_so_far": 5520042363432960.0,
      "budget_used_percent": 5.52004236343296
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:30",
      "total_flops_so_far": 5529166400397312.0,
      "budget_used_percent": 5.529166400397312
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:31",
      "total_flops_so_far": 5538290437361664.0,
      "budget_used_percent": 5.538290437361664
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:31",
      "total_flops_so_far": 5547414474326016.0,
      "budget_used_percent": 5.547414474326016
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:32",
      "total_flops_so_far": 5556538511290368.0,
      "budget_used_percent": 5.556538511290368
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:32",
      "total_flops_so_far": 5565662548254720.0,
      "budget_used_percent": 5.5656625482547195
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:32",
      "total_flops_so_far": 5574786585219072.0,
      "budget_used_percent": 5.574786585219072
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:33",
      "total_flops_so_far": 5583910622183424.0,
      "budget_used_percent": 5.583910622183423
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:33",
      "total_flops_so_far": 5593034659147776.0,
      "budget_used_percent": 5.593034659147776
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:34",
      "total_flops_so_far": 5602158696112128.0,
      "budget_used_percent": 5.602158696112128
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:34",
      "total_flops_so_far": 5611282733076480.0,
      "budget_used_percent": 5.6112827330764805
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:35",
      "total_flops_so_far": 5620406770040832.0,
      "budget_used_percent": 5.620406770040832
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:35",
      "total_flops_so_far": 5629530807005184.0,
      "budget_used_percent": 5.629530807005184
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:36",
      "total_flops_so_far": 5638654843969536.0,
      "budget_used_percent": 5.638654843969536
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:36",
      "total_flops_so_far": 5647778880933888.0,
      "budget_used_percent": 5.647778880933888
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:36",
      "total_flops_so_far": 5656902917898240.0,
      "budget_used_percent": 5.65690291789824
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:37",
      "total_flops_so_far": 5666026954862592.0,
      "budget_used_percent": 5.666026954862592
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:37",
      "total_flops_so_far": 5675150991826944.0,
      "budget_used_percent": 5.675150991826944
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:38",
      "total_flops_so_far": 5684275028791296.0,
      "budget_used_percent": 5.684275028791296
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:38",
      "total_flops_so_far": 5693399065755648.0,
      "budget_used_percent": 5.6933990657556475
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:39",
      "total_flops_so_far": 5702523102720000.0,
      "budget_used_percent": 5.70252310272
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:39",
      "total_flops_so_far": 5711647139684352.0,
      "budget_used_percent": 5.711647139684352
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:40",
      "total_flops_so_far": 5720771176648704.0,
      "budget_used_percent": 5.720771176648705
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:40",
      "total_flops_so_far": 5729895213613056.0,
      "budget_used_percent": 5.729895213613056
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:41",
      "total_flops_so_far": 5739019250577408.0,
      "budget_used_percent": 5.7390192505774085
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:41",
      "total_flops_so_far": 5748143287541760.0,
      "budget_used_percent": 5.74814328754176
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:41",
      "total_flops_so_far": 5757267324506112.0,
      "budget_used_percent": 5.757267324506112
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:42",
      "total_flops_so_far": 5766391361470464.0,
      "budget_used_percent": 5.766391361470464
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:42",
      "total_flops_so_far": 5775515398434816.0,
      "budget_used_percent": 5.775515398434816
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:43",
      "total_flops_so_far": 5784639435399168.0,
      "budget_used_percent": 5.784639435399168
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:43",
      "total_flops_so_far": 5793763472363520.0,
      "budget_used_percent": 5.79376347236352
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:44",
      "total_flops_so_far": 5802887509327872.0,
      "budget_used_percent": 5.802887509327872
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:44",
      "total_flops_so_far": 5812011546292224.0,
      "budget_used_percent": 5.812011546292224
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:45",
      "total_flops_so_far": 5821135583256576.0,
      "budget_used_percent": 5.821135583256575
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:45",
      "total_flops_so_far": 5830259620220928.0,
      "budget_used_percent": 5.830259620220928
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:45",
      "total_flops_so_far": 5839383657185280.0,
      "budget_used_percent": 5.83938365718528
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:46",
      "total_flops_so_far": 5848507694149632.0,
      "budget_used_percent": 5.848507694149633
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:46",
      "total_flops_so_far": 5857631731113984.0,
      "budget_used_percent": 5.857631731113984
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:47",
      "total_flops_so_far": 5866755768078336.0,
      "budget_used_percent": 5.866755768078336
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:47",
      "total_flops_so_far": 5875879805042688.0,
      "budget_used_percent": 5.875879805042688
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:48",
      "total_flops_so_far": 5885003842007040.0,
      "budget_used_percent": 5.88500384200704
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:48",
      "total_flops_so_far": 5894127878971392.0,
      "budget_used_percent": 5.894127878971392
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:49",
      "total_flops_so_far": 5903251915935744.0,
      "budget_used_percent": 5.903251915935744
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:49",
      "total_flops_so_far": 5912375952900096.0,
      "budget_used_percent": 5.912375952900096
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:49",
      "total_flops_so_far": 5921499989864448.0,
      "budget_used_percent": 5.921499989864448
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:50",
      "total_flops_so_far": 5930624026828800.0,
      "budget_used_percent": 5.9306240268287995
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:50",
      "total_flops_so_far": 5939748063793152.0,
      "budget_used_percent": 5.939748063793152
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:51",
      "total_flops_so_far": 5948872100757504.0,
      "budget_used_percent": 5.948872100757504
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:51",
      "total_flops_so_far": 5957996137721856.0,
      "budget_used_percent": 5.957996137721857
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:52",
      "total_flops_so_far": 5967120174686208.0,
      "budget_used_percent": 5.967120174686208
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:52",
      "total_flops_so_far": 5976244211650560.0,
      "budget_used_percent": 5.9762442116505605
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:53",
      "total_flops_so_far": 5985368248614912.0,
      "budget_used_percent": 5.985368248614912
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:53",
      "total_flops_so_far": 5994492285579264.0,
      "budget_used_percent": 5.994492285579264
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:53",
      "total_flops_so_far": 6003616322543616.0,
      "budget_used_percent": 6.003616322543616
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:54",
      "total_flops_so_far": 6012740359507968.0,
      "budget_used_percent": 6.012740359507968
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:54",
      "total_flops_so_far": 6021864396472320.0,
      "budget_used_percent": 6.02186439647232
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:55",
      "total_flops_so_far": 6030988433436672.0,
      "budget_used_percent": 6.030988433436672
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:55",
      "total_flops_so_far": 6040112470401024.0,
      "budget_used_percent": 6.040112470401024
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:56",
      "total_flops_so_far": 6049236507365376.0,
      "budget_used_percent": 6.049236507365376
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:56",
      "total_flops_so_far": 6058360544329728.0,
      "budget_used_percent": 6.058360544329728
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:57",
      "total_flops_so_far": 6067484581294080.0,
      "budget_used_percent": 6.06748458129408
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:57",
      "total_flops_so_far": 6076608618258432.0,
      "budget_used_percent": 6.076608618258432
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:57",
      "total_flops_so_far": 6085732655222784.0,
      "budget_used_percent": 6.085732655222785
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:58",
      "total_flops_so_far": 6094856692187136.0,
      "budget_used_percent": 6.094856692187136
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:58",
      "total_flops_so_far": 6103980729151488.0,
      "budget_used_percent": 6.1039807291514885
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:59",
      "total_flops_so_far": 6113104766115840.0,
      "budget_used_percent": 6.11310476611584
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:54:59",
      "total_flops_so_far": 6122228803080192.0,
      "budget_used_percent": 6.122228803080192
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:00",
      "total_flops_so_far": 6131352840044544.0,
      "budget_used_percent": 6.131352840044544
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:00",
      "total_flops_so_far": 6140476877008896.0,
      "budget_used_percent": 6.140476877008896
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:01",
      "total_flops_so_far": 6149600913973248.0,
      "budget_used_percent": 6.149600913973248
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:01",
      "total_flops_so_far": 6158724950937600.0,
      "budget_used_percent": 6.1587249509376
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:01",
      "total_flops_so_far": 6167848987901952.0,
      "budget_used_percent": 6.167848987901952
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:02",
      "total_flops_so_far": 6176973024866304.0,
      "budget_used_percent": 6.176973024866304
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:02",
      "total_flops_so_far": 6186097061830656.0,
      "budget_used_percent": 6.186097061830656
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:03",
      "total_flops_so_far": 6195221098795008.0,
      "budget_used_percent": 6.195221098795009
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:03",
      "total_flops_so_far": 6204345135759360.0,
      "budget_used_percent": 6.20434513575936
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:04",
      "total_flops_so_far": 6213469172723712.0,
      "budget_used_percent": 6.213469172723712
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:04",
      "total_flops_so_far": 6222593209688064.0,
      "budget_used_percent": 6.222593209688064
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:05",
      "total_flops_so_far": 6231717246652416.0,
      "budget_used_percent": 6.2317172466524156
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:05",
      "total_flops_so_far": 6240841283616768.0,
      "budget_used_percent": 6.240841283616768
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:06",
      "total_flops_so_far": 6249965320581120.0,
      "budget_used_percent": 6.249965320581119
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:06",
      "total_flops_so_far": 6259089357545472.0,
      "budget_used_percent": 6.259089357545472
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:06",
      "total_flops_so_far": 6268213394509824.0,
      "budget_used_percent": 6.268213394509823
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:07",
      "total_flops_so_far": 6277337431474176.0,
      "budget_used_percent": 6.277337431474176
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:07",
      "total_flops_so_far": 6286461468438528.0,
      "budget_used_percent": 6.286461468438528
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:08",
      "total_flops_so_far": 6295585505402880.0,
      "budget_used_percent": 6.29558550540288
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:08",
      "total_flops_so_far": 6304709542367232.0,
      "budget_used_percent": 6.304709542367232
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:09",
      "total_flops_so_far": 6313833579331584.0,
      "budget_used_percent": 6.313833579331583
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:09",
      "total_flops_so_far": 6322957616295936.0,
      "budget_used_percent": 6.322957616295937
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:10",
      "total_flops_so_far": 6332081653260288.0,
      "budget_used_percent": 6.332081653260288
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:10",
      "total_flops_so_far": 6341205690224640.0,
      "budget_used_percent": 6.34120569022464
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:10",
      "total_flops_so_far": 6350329727188992.0,
      "budget_used_percent": 6.350329727188991
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:11",
      "total_flops_so_far": 6359453764153344.0,
      "budget_used_percent": 6.359453764153344
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:11",
      "total_flops_so_far": 6368577801117696.0,
      "budget_used_percent": 6.368577801117696
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:12",
      "total_flops_so_far": 6377701838082048.0,
      "budget_used_percent": 6.377701838082047
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:12",
      "total_flops_so_far": 6386825875046400.0,
      "budget_used_percent": 6.3868258750464
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:13",
      "total_flops_so_far": 6395949912010752.0,
      "budget_used_percent": 6.395949912010752
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:13",
      "total_flops_so_far": 6405073948975104.0,
      "budget_used_percent": 6.4050739489751045
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:14",
      "total_flops_so_far": 6414197985939456.0,
      "budget_used_percent": 6.414197985939456
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:14",
      "total_flops_so_far": 6423322022903808.0,
      "budget_used_percent": 6.4233220229038075
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:14",
      "total_flops_so_far": 6432446059868160.0,
      "budget_used_percent": 6.432446059868161
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:15",
      "total_flops_so_far": 6441570096832512.0,
      "budget_used_percent": 6.441570096832512
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:15",
      "total_flops_so_far": 6450694133796864.0,
      "budget_used_percent": 6.450694133796864
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:16",
      "total_flops_so_far": 6459818170761216.0,
      "budget_used_percent": 6.459818170761215
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:16",
      "total_flops_so_far": 6468942207725568.0,
      "budget_used_percent": 6.4689422077255685
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:17",
      "total_flops_so_far": 6478066244689920.0,
      "budget_used_percent": 6.47806624468992
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:17",
      "total_flops_so_far": 6487190281654272.0,
      "budget_used_percent": 6.4871902816542715
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:18",
      "total_flops_so_far": 6496314318618624.0,
      "budget_used_percent": 6.496314318618623
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:18",
      "total_flops_so_far": 6505438355582976.0,
      "budget_used_percent": 6.505438355582976
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:19",
      "total_flops_so_far": 6514562392547328.0,
      "budget_used_percent": 6.514562392547328
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:19",
      "total_flops_so_far": 6523686429511680.0,
      "budget_used_percent": 6.52368642951168
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:19",
      "total_flops_so_far": 6532810466476032.0,
      "budget_used_percent": 6.532810466476032
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:20",
      "total_flops_so_far": 6541934503440384.0,
      "budget_used_percent": 6.541934503440385
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:20",
      "total_flops_so_far": 6551058540404736.0,
      "budget_used_percent": 6.551058540404736
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:21",
      "total_flops_so_far": 6560182577369088.0,
      "budget_used_percent": 6.560182577369088
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:21",
      "total_flops_so_far": 6569306614333440.0,
      "budget_used_percent": 6.569306614333439
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:22",
      "total_flops_so_far": 6578430651297792.0,
      "budget_used_percent": 6.578430651297793
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:22",
      "total_flops_so_far": 6587554688262144.0,
      "budget_used_percent": 6.587554688262144
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:23",
      "total_flops_so_far": 6596678725226496.0,
      "budget_used_percent": 6.596678725226496
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:23",
      "total_flops_so_far": 6605802762190848.0,
      "budget_used_percent": 6.605802762190847
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:23",
      "total_flops_so_far": 6614926799155200.0,
      "budget_used_percent": 6.6149267991552
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:24",
      "total_flops_so_far": 6624050836119552.0,
      "budget_used_percent": 6.624050836119552
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:24",
      "total_flops_so_far": 6633174873083904.0,
      "budget_used_percent": 6.633174873083904
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:25",
      "total_flops_so_far": 6642298910048256.0,
      "budget_used_percent": 6.642298910048256
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:25",
      "total_flops_so_far": 6651422947012608.0,
      "budget_used_percent": 6.651422947012609
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:26",
      "total_flops_so_far": 6660546983976960.0,
      "budget_used_percent": 6.66054698397696
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:26",
      "total_flops_so_far": 6669671020941312.0,
      "budget_used_percent": 6.669671020941312
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:27",
      "total_flops_so_far": 6678795057905664.0,
      "budget_used_percent": 6.678795057905663
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:27",
      "total_flops_so_far": 6687919094870016.0,
      "budget_used_percent": 6.687919094870017
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:27",
      "total_flops_so_far": 6697043131834368.0,
      "budget_used_percent": 6.697043131834368
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:28",
      "total_flops_so_far": 6706167168798720.0,
      "budget_used_percent": 6.70616716879872
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:28",
      "total_flops_so_far": 6715291205763072.0,
      "budget_used_percent": 6.715291205763071
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:29",
      "total_flops_so_far": 6724415242727424.0,
      "budget_used_percent": 6.724415242727424
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:29",
      "total_flops_so_far": 6733539279691776.0,
      "budget_used_percent": 6.733539279691776
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:30",
      "total_flops_so_far": 6742663316656128.0,
      "budget_used_percent": 6.742663316656127
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:30",
      "total_flops_so_far": 6751787353620480.0,
      "budget_used_percent": 6.75178735362048
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:31",
      "total_flops_so_far": 6760911390584832.0,
      "budget_used_percent": 6.760911390584832
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:31",
      "total_flops_so_far": 6770035427549184.0,
      "budget_used_percent": 6.7700354275491845
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:31",
      "total_flops_so_far": 6779159464513536.0,
      "budget_used_percent": 6.779159464513536
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:32",
      "total_flops_so_far": 6788283501477888.0,
      "budget_used_percent": 6.7882835014778875
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:32",
      "total_flops_so_far": 6797407538442240.0,
      "budget_used_percent": 6.797407538442241
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:33",
      "total_flops_so_far": 6806531575406592.0,
      "budget_used_percent": 6.806531575406592
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:33",
      "total_flops_so_far": 6815655612370944.0,
      "budget_used_percent": 6.815655612370944
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:34",
      "total_flops_so_far": 6824779649335296.0,
      "budget_used_percent": 6.824779649335295
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:34",
      "total_flops_so_far": 6833903686299648.0,
      "budget_used_percent": 6.8339036862996485
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:35",
      "total_flops_so_far": 6843027723264000.0,
      "budget_used_percent": 6.843027723264
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:35",
      "total_flops_so_far": 6852151760228352.0,
      "budget_used_percent": 6.8521517602283515
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:36",
      "total_flops_so_far": 6861275797192704.0,
      "budget_used_percent": 6.861275797192704
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:36",
      "total_flops_so_far": 6870399834157056.0,
      "budget_used_percent": 6.870399834157056
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:36",
      "total_flops_so_far": 6879523871121408.0,
      "budget_used_percent": 6.879523871121409
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:37",
      "total_flops_so_far": 6888647908085760.0,
      "budget_used_percent": 6.88864790808576
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:37",
      "total_flops_so_far": 6897771945050112.0,
      "budget_used_percent": 6.897771945050112
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:38",
      "total_flops_so_far": 6906895982014464.0,
      "budget_used_percent": 6.906895982014465
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:38",
      "total_flops_so_far": 6916020018978816.0,
      "budget_used_percent": 6.916020018978816
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:39",
      "total_flops_so_far": 6925144055943168.0,
      "budget_used_percent": 6.925144055943168
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:39",
      "total_flops_so_far": 6934268092907520.0,
      "budget_used_percent": 6.934268092907519
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:40",
      "total_flops_so_far": 6943392129871872.0,
      "budget_used_percent": 6.943392129871873
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:40",
      "total_flops_so_far": 6952516166836224.0,
      "budget_used_percent": 6.952516166836224
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:40",
      "total_flops_so_far": 6961640203800576.0,
      "budget_used_percent": 6.961640203800576
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:41",
      "total_flops_so_far": 6970764240764928.0,
      "budget_used_percent": 6.970764240764928
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:41",
      "total_flops_so_far": 6979888277729280.0,
      "budget_used_percent": 6.97988827772928
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:42",
      "total_flops_so_far": 6989012314693632.0,
      "budget_used_percent": 6.989012314693632
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:42",
      "total_flops_so_far": 6998136351657984.0,
      "budget_used_percent": 6.998136351657984
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:43",
      "total_flops_so_far": 7007260388622336.0,
      "budget_used_percent": 7.007260388622336
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:43",
      "total_flops_so_far": 7016384425586688.0,
      "budget_used_percent": 7.016384425586689
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:44",
      "total_flops_so_far": 7025508462551040.0,
      "budget_used_percent": 7.0255084625510404
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:44",
      "total_flops_so_far": 7034632499515392.0,
      "budget_used_percent": 7.034632499515392
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:44",
      "total_flops_so_far": 7043756536479744.0,
      "budget_used_percent": 7.043756536479743
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:45",
      "total_flops_so_far": 7052880573444096.0,
      "budget_used_percent": 7.052880573444097
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:45",
      "total_flops_so_far": 7062004610408448.0,
      "budget_used_percent": 7.062004610408448
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:46",
      "total_flops_so_far": 7071128647372800.0,
      "budget_used_percent": 7.0711286473728
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:46",
      "total_flops_so_far": 7080252684337152.0,
      "budget_used_percent": 7.080252684337152
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:47",
      "total_flops_so_far": 7089376721301504.0,
      "budget_used_percent": 7.089376721301504
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:47",
      "total_flops_so_far": 7098500758265856.0,
      "budget_used_percent": 7.098500758265856
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:48",
      "total_flops_so_far": 7107624795230208.0,
      "budget_used_percent": 7.107624795230208
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:48",
      "total_flops_so_far": 7116748832194560.0,
      "budget_used_percent": 7.11674883219456
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:49",
      "total_flops_so_far": 7125872869158912.0,
      "budget_used_percent": 7.125872869158913
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:49",
      "total_flops_so_far": 7134996906123264.0,
      "budget_used_percent": 7.1349969061232645
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:49",
      "total_flops_so_far": 7144120943087616.0,
      "budget_used_percent": 7.144120943087616
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:50",
      "total_flops_so_far": 7153244980051968.0,
      "budget_used_percent": 7.1532449800519675
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:50",
      "total_flops_so_far": 7162369017016320.0,
      "budget_used_percent": 7.162369017016321
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:51",
      "total_flops_so_far": 7171493053980672.0,
      "budget_used_percent": 7.171493053980672
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:51",
      "total_flops_so_far": 7180617090945024.0,
      "budget_used_percent": 7.180617090945024
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:52",
      "total_flops_so_far": 7189741127909376.0,
      "budget_used_percent": 7.189741127909375
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:52",
      "total_flops_so_far": 7198865164873728.0,
      "budget_used_percent": 7.1988651648737285
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:53",
      "total_flops_so_far": 7207989201838080.0,
      "budget_used_percent": 7.20798920183808
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:53",
      "total_flops_so_far": 7217113238802432.0,
      "budget_used_percent": 7.217113238802432
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:53",
      "total_flops_so_far": 7226237275766784.0,
      "budget_used_percent": 7.226237275766784
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:54",
      "total_flops_so_far": 7235361312731136.0,
      "budget_used_percent": 7.235361312731135
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:54",
      "total_flops_so_far": 7244485349695488.0,
      "budget_used_percent": 7.244485349695489
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:55",
      "total_flops_so_far": 7253609386659840.0,
      "budget_used_percent": 7.25360938665984
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:55",
      "total_flops_so_far": 7262733423624192.0,
      "budget_used_percent": 7.262733423624192
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:56",
      "total_flops_so_far": 7271857460588544.0,
      "budget_used_percent": 7.271857460588543
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:56",
      "total_flops_so_far": 7280981497552896.0,
      "budget_used_percent": 7.280981497552896
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:57",
      "total_flops_so_far": 7290105534517248.0,
      "budget_used_percent": 7.290105534517248
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:57",
      "total_flops_so_far": 7299229571481600.0,
      "budget_used_percent": 7.299229571481599
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:58",
      "total_flops_so_far": 7308353608445952.0,
      "budget_used_percent": 7.308353608445952
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:58",
      "total_flops_so_far": 7317477645410304.0,
      "budget_used_percent": 7.317477645410304
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:58",
      "total_flops_so_far": 7326601682374656.0,
      "budget_used_percent": 7.3266016823746565
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:59",
      "total_flops_so_far": 7335725719339008.0,
      "budget_used_percent": 7.335725719339008
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:55:59",
      "total_flops_so_far": 7344849756303360.0,
      "budget_used_percent": 7.3448497563033595
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:00",
      "total_flops_so_far": 7353973793267712.0,
      "budget_used_percent": 7.353973793267713
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:00",
      "total_flops_so_far": 7363097830232064.0,
      "budget_used_percent": 7.363097830232064
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:01",
      "total_flops_so_far": 7372221867196416.0,
      "budget_used_percent": 7.372221867196416
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:01",
      "total_flops_so_far": 7381345904160768.0,
      "budget_used_percent": 7.381345904160767
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:02",
      "total_flops_so_far": 7390469941125120.0,
      "budget_used_percent": 7.3904699411251205
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:02",
      "total_flops_so_far": 7399593978089472.0,
      "budget_used_percent": 7.399593978089472
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:02",
      "total_flops_so_far": 7408718015053824.0,
      "budget_used_percent": 7.408718015053823
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:03",
      "total_flops_so_far": 7417842052018176.0,
      "budget_used_percent": 7.417842052018175
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:03",
      "total_flops_so_far": 7426966088982528.0,
      "budget_used_percent": 7.426966088982528
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:04",
      "total_flops_so_far": 7436090125946880.0,
      "budget_used_percent": 7.43609012594688
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:04",
      "total_flops_so_far": 7445214162911232.0,
      "budget_used_percent": 7.445214162911232
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:05",
      "total_flops_so_far": 7454338199875584.0,
      "budget_used_percent": 7.4543381998755835
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:05",
      "total_flops_so_far": 7463462236839936.0,
      "budget_used_percent": 7.463462236839937
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:06",
      "total_flops_so_far": 7472586273804288.0,
      "budget_used_percent": 7.472586273804288
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:06",
      "total_flops_so_far": 7481710310768640.0,
      "budget_used_percent": 7.48171031076864
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:07",
      "total_flops_so_far": 7490834347732992.0,
      "budget_used_percent": 7.490834347732991
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:07",
      "total_flops_so_far": 7499958384697344.0,
      "budget_used_percent": 7.4999583846973445
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:07",
      "total_flops_so_far": 7509082421661696.0,
      "budget_used_percent": 7.509082421661696
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:08",
      "total_flops_so_far": 7518206458626048.0,
      "budget_used_percent": 7.5182064586260475
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:08",
      "total_flops_so_far": 7527330495590400.0,
      "budget_used_percent": 7.527330495590399
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:09",
      "total_flops_so_far": 7536454532554752.0,
      "budget_used_percent": 7.536454532554752
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:09",
      "total_flops_so_far": 7545578569519104.0,
      "budget_used_percent": 7.545578569519104
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:10",
      "total_flops_so_far": 7554702606483456.0,
      "budget_used_percent": 7.554702606483456
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:10",
      "total_flops_so_far": 7563826643447808.0,
      "budget_used_percent": 7.563826643447808
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:11",
      "total_flops_so_far": 7572950680412160.0,
      "budget_used_percent": 7.572950680412161
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:11",
      "total_flops_so_far": 7582074717376512.0,
      "budget_used_percent": 7.582074717376512
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:11",
      "total_flops_so_far": 7591198754340864.0,
      "budget_used_percent": 7.591198754340864
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:12",
      "total_flops_so_far": 7600322791305216.0,
      "budget_used_percent": 7.600322791305215
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:12",
      "total_flops_so_far": 7609446828269568.0,
      "budget_used_percent": 7.609446828269569
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:13",
      "total_flops_so_far": 7618570865233920.0,
      "budget_used_percent": 7.61857086523392
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:13",
      "total_flops_so_far": 7627694902198272.0,
      "budget_used_percent": 7.627694902198272
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:14",
      "total_flops_so_far": 7636818939162624.0,
      "budget_used_percent": 7.636818939162623
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:14",
      "total_flops_so_far": 7645942976126976.0,
      "budget_used_percent": 7.645942976126976
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:15",
      "total_flops_so_far": 7655067013091328.0,
      "budget_used_percent": 7.655067013091328
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:15",
      "total_flops_so_far": 7664191050055680.0,
      "budget_used_percent": 7.664191050055679
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:16",
      "total_flops_so_far": 7673315087020032.0,
      "budget_used_percent": 7.673315087020032
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:16",
      "total_flops_so_far": 7682439123984384.0,
      "budget_used_percent": 7.682439123984384
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:16",
      "total_flops_so_far": 7691563160948736.0,
      "budget_used_percent": 7.6915631609487365
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:17",
      "total_flops_so_far": 7700687197913088.0,
      "budget_used_percent": 7.700687197913088
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:17",
      "total_flops_so_far": 7709811234877440.0,
      "budget_used_percent": 7.7098112348774395
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:18",
      "total_flops_so_far": 7718935271841792.0,
      "budget_used_percent": 7.718935271841793
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:18",
      "total_flops_so_far": 7728059308806144.0,
      "budget_used_percent": 7.728059308806144
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:19",
      "total_flops_so_far": 7737183345770496.0,
      "budget_used_percent": 7.737183345770496
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:19",
      "total_flops_so_far": 7746307382734848.0,
      "budget_used_percent": 7.746307382734847
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:20",
      "total_flops_so_far": 7755431419699200.0,
      "budget_used_percent": 7.7554314196992005
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:20",
      "total_flops_so_far": 7764555456663552.0,
      "budget_used_percent": 7.764555456663552
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:20",
      "total_flops_so_far": 7773679493627904.0,
      "budget_used_percent": 7.7736794936279034
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:21",
      "total_flops_so_far": 7782803530592256.0,
      "budget_used_percent": 7.782803530592256
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:21",
      "total_flops_so_far": 7791927567556608.0,
      "budget_used_percent": 7.791927567556608
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:22",
      "total_flops_so_far": 7801051604520960.0,
      "budget_used_percent": 7.801051604520961
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:22",
      "total_flops_so_far": 7810175641485312.0,
      "budget_used_percent": 7.810175641485312
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:23",
      "total_flops_so_far": 7819299678449664.0,
      "budget_used_percent": 7.819299678449664
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:23",
      "total_flops_so_far": 7828423715414016.0,
      "budget_used_percent": 7.828423715414017
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:24",
      "total_flops_so_far": 7837547752378368.0,
      "budget_used_percent": 7.837547752378368
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:24",
      "total_flops_so_far": 7846671789342720.0,
      "budget_used_percent": 7.84667178934272
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:25",
      "total_flops_so_far": 7855795826307072.0,
      "budget_used_percent": 7.855795826307071
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:25",
      "total_flops_so_far": 7864919863271424.0,
      "budget_used_percent": 7.864919863271425
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:25",
      "total_flops_so_far": 7874043900235776.0,
      "budget_used_percent": 7.874043900235776
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:26",
      "total_flops_so_far": 7883167937200128.0,
      "budget_used_percent": 7.8831679372001275
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:26",
      "total_flops_so_far": 7892291974164480.0,
      "budget_used_percent": 7.89229197416448
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:27",
      "total_flops_so_far": 7901416011128832.0,
      "budget_used_percent": 7.901416011128832
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:27",
      "total_flops_so_far": 7910540048093184.0,
      "budget_used_percent": 7.910540048093184
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:28",
      "total_flops_so_far": 7919664085057536.0,
      "budget_used_percent": 7.919664085057536
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:28",
      "total_flops_so_far": 7928788122021888.0,
      "budget_used_percent": 7.928788122021888
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:29",
      "total_flops_so_far": 7937912158986240.0,
      "budget_used_percent": 7.937912158986241
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:29",
      "total_flops_so_far": 7947036195950592.0,
      "budget_used_percent": 7.947036195950592
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:30",
      "total_flops_so_far": 7956160232914944.0,
      "budget_used_percent": 7.956160232914944
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:30",
      "total_flops_so_far": 7965284269879296.0,
      "budget_used_percent": 7.965284269879295
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:30",
      "total_flops_so_far": 7974408306843648.0,
      "budget_used_percent": 7.974408306843649
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:31",
      "total_flops_so_far": 7983532343808000.0,
      "budget_used_percent": 7.983532343808
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:31",
      "total_flops_so_far": 7992656380772352.0,
      "budget_used_percent": 7.992656380772352
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:32",
      "total_flops_so_far": 8001780417736704.0,
      "budget_used_percent": 8.001780417736704
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:32",
      "total_flops_so_far": 8010904454701056.0,
      "budget_used_percent": 8.010904454701057
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:33",
      "total_flops_so_far": 8020028491665408.0,
      "budget_used_percent": 8.020028491665409
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:33",
      "total_flops_so_far": 8029152528629760.0,
      "budget_used_percent": 8.02915252862976
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:34",
      "total_flops_so_far": 8038276565594112.0,
      "budget_used_percent": 8.038276565594112
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:34",
      "total_flops_so_far": 8047400602558464.0,
      "budget_used_percent": 8.047400602558465
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:34",
      "total_flops_so_far": 8056524639522816.0,
      "budget_used_percent": 8.056524639522816
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:35",
      "total_flops_so_far": 8065648676487168.0,
      "budget_used_percent": 8.065648676487168
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:35",
      "total_flops_so_far": 8074772713451520.0,
      "budget_used_percent": 8.07477271345152
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:36",
      "total_flops_so_far": 8083896750415872.0,
      "budget_used_percent": 8.083896750415873
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:36",
      "total_flops_so_far": 8093020787380224.0,
      "budget_used_percent": 8.093020787380224
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:37",
      "total_flops_so_far": 8102144824344576.0,
      "budget_used_percent": 8.102144824344576
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:37",
      "total_flops_so_far": 8111268861308928.0,
      "budget_used_percent": 8.111268861308927
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:38",
      "total_flops_so_far": 8120392898273280.0,
      "budget_used_percent": 8.12039289827328
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:38",
      "total_flops_so_far": 8129516935237632.0,
      "budget_used_percent": 8.129516935237632
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:39",
      "total_flops_so_far": 8138640972201984.0,
      "budget_used_percent": 8.138640972201983
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:39",
      "total_flops_so_far": 8147765009166336.0,
      "budget_used_percent": 8.147765009166335
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:39",
      "total_flops_so_far": 8156889046130688.0,
      "budget_used_percent": 8.156889046130688
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:40",
      "total_flops_so_far": 8166013083095040.0,
      "budget_used_percent": 8.16601308309504
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:40",
      "total_flops_so_far": 8175137120059392.0,
      "budget_used_percent": 8.175137120059391
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:41",
      "total_flops_so_far": 8184261157023744.0,
      "budget_used_percent": 8.184261157023744
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:41",
      "total_flops_so_far": 8193385193988096.0,
      "budget_used_percent": 8.193385193988096
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:42",
      "total_flops_so_far": 8202509230952448.0,
      "budget_used_percent": 8.202509230952447
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:42",
      "total_flops_so_far": 8211633267916800.0,
      "budget_used_percent": 8.2116332679168
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:43",
      "total_flops_so_far": 8220757304881152.0,
      "budget_used_percent": 8.220757304881152
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:43",
      "total_flops_so_far": 8229881341845504.0,
      "budget_used_percent": 8.229881341845505
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:44",
      "total_flops_so_far": 8239005378809856.0,
      "budget_used_percent": 8.239005378809857
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:44",
      "total_flops_so_far": 8248129415774208.0,
      "budget_used_percent": 8.248129415774208
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:44",
      "total_flops_so_far": 8257253452738560.0,
      "budget_used_percent": 8.25725345273856
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:45",
      "total_flops_so_far": 8266377489702912.0,
      "budget_used_percent": 8.266377489702911
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:45",
      "total_flops_so_far": 8275501526667264.0,
      "budget_used_percent": 8.275501526667265
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:46",
      "total_flops_so_far": 8284625563631616.0,
      "budget_used_percent": 8.284625563631616
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:46",
      "total_flops_so_far": 8293749600595968.0,
      "budget_used_percent": 8.293749600595968
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:47",
      "total_flops_so_far": 8302873637560320.0,
      "budget_used_percent": 8.30287363756032
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:47",
      "total_flops_so_far": 8311997674524672.0,
      "budget_used_percent": 8.311997674524672
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:48",
      "total_flops_so_far": 8321121711489024.0,
      "budget_used_percent": 8.321121711489024
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:48",
      "total_flops_so_far": 8330245748453376.0,
      "budget_used_percent": 8.330245748453375
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:48",
      "total_flops_so_far": 8339369785417728.0,
      "budget_used_percent": 8.339369785417727
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:49",
      "total_flops_so_far": 8348493822382080.0,
      "budget_used_percent": 8.34849382238208
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:49",
      "total_flops_so_far": 8357617859346432.0,
      "budget_used_percent": 8.357617859346432
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:50",
      "total_flops_so_far": 8366741896310784.0,
      "budget_used_percent": 8.366741896310783
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:50",
      "total_flops_so_far": 8375865933275136.0,
      "budget_used_percent": 8.375865933275135
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:51",
      "total_flops_so_far": 8384989970239488.0,
      "budget_used_percent": 8.384989970239488
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:51",
      "total_flops_so_far": 8394114007203840.0,
      "budget_used_percent": 8.39411400720384
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:52",
      "total_flops_so_far": 8403238044168192.0,
      "budget_used_percent": 8.40323804416819
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:52",
      "total_flops_so_far": 8412362081132544.0,
      "budget_used_percent": 8.412362081132544
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:53",
      "total_flops_so_far": 8421486118096896.0,
      "budget_used_percent": 8.421486118096896
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:53",
      "total_flops_so_far": 8430610155061248.0,
      "budget_used_percent": 8.430610155061249
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:53",
      "total_flops_so_far": 8439734192025600.0,
      "budget_used_percent": 8.4397341920256
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:54",
      "total_flops_so_far": 8448858228989952.0,
      "budget_used_percent": 8.448858228989952
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:54",
      "total_flops_so_far": 8457982265954304.0,
      "budget_used_percent": 8.457982265954305
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:55",
      "total_flops_so_far": 8467106302918656.0,
      "budget_used_percent": 8.467106302918657
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:55",
      "total_flops_so_far": 8476230339883008.0,
      "budget_used_percent": 8.476230339883008
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:56",
      "total_flops_so_far": 8485354376847360.0,
      "budget_used_percent": 8.48535437684736
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:56",
      "total_flops_so_far": 8494478413811712.0,
      "budget_used_percent": 8.494478413811713
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:57",
      "total_flops_so_far": 8503602450776064.0,
      "budget_used_percent": 8.503602450776064
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:57",
      "total_flops_so_far": 8512726487740416.0,
      "budget_used_percent": 8.512726487740416
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:58",
      "total_flops_so_far": 8521850524704768.0,
      "budget_used_percent": 8.521850524704767
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:58",
      "total_flops_so_far": 8530974561669120.0,
      "budget_used_percent": 8.53097456166912
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:58",
      "total_flops_so_far": 8540098598633472.0,
      "budget_used_percent": 8.540098598633472
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:59",
      "total_flops_so_far": 8549222635597824.0,
      "budget_used_percent": 8.549222635597824
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:56:59",
      "total_flops_so_far": 8558346672562176.0,
      "budget_used_percent": 8.558346672562175
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:00",
      "total_flops_so_far": 8567470709526528.0,
      "budget_used_percent": 8.567470709526528
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:00",
      "total_flops_so_far": 8576594746490880.0,
      "budget_used_percent": 8.57659474649088
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:01",
      "total_flops_so_far": 8585718783455232.0,
      "budget_used_percent": 8.585718783455231
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:01",
      "total_flops_so_far": 8594842820419584.0,
      "budget_used_percent": 8.594842820419583
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:02",
      "total_flops_so_far": 8603966857383936.0,
      "budget_used_percent": 8.603966857383936
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:02",
      "total_flops_so_far": 8613090894348288.0,
      "budget_used_percent": 8.613090894348288
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:03",
      "total_flops_so_far": 8622214931312640.0,
      "budget_used_percent": 8.622214931312639
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:03",
      "total_flops_so_far": 8631338968276992.0,
      "budget_used_percent": 8.631338968276992
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:03",
      "total_flops_so_far": 8640463005241344.0,
      "budget_used_percent": 8.640463005241344
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:04",
      "total_flops_so_far": 8649587042205696.0,
      "budget_used_percent": 8.649587042205695
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:04",
      "total_flops_so_far": 8658711079170048.0,
      "budget_used_percent": 8.658711079170049
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:05",
      "total_flops_so_far": 8667835116134400.0,
      "budget_used_percent": 8.6678351161344
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:05",
      "total_flops_so_far": 8676959153098752.0,
      "budget_used_percent": 8.676959153098753
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:06",
      "total_flops_so_far": 8686083190063104.0,
      "budget_used_percent": 8.686083190063105
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:06",
      "total_flops_so_far": 8695207227027456.0,
      "budget_used_percent": 8.695207227027456
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:07",
      "total_flops_so_far": 8704331263991808.0,
      "budget_used_percent": 8.704331263991808
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:07",
      "total_flops_so_far": 8713455300956160.0,
      "budget_used_percent": 8.713455300956161
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:08",
      "total_flops_so_far": 8722579337920512.0,
      "budget_used_percent": 8.722579337920513
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:08",
      "total_flops_so_far": 8731703374884864.0,
      "budget_used_percent": 8.731703374884864
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:08",
      "total_flops_so_far": 8740827411849216.0,
      "budget_used_percent": 8.740827411849216
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:09",
      "total_flops_so_far": 8749951448813568.0,
      "budget_used_percent": 8.749951448813569
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:09",
      "total_flops_so_far": 8759075485777920.0,
      "budget_used_percent": 8.75907548577792
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:10",
      "total_flops_so_far": 8768199522742272.0,
      "budget_used_percent": 8.768199522742272
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:10",
      "total_flops_so_far": 8777323559706624.0,
      "budget_used_percent": 8.777323559706623
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:11",
      "total_flops_so_far": 8786447596670976.0,
      "budget_used_percent": 8.786447596670977
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:11",
      "total_flops_so_far": 8795571633635328.0,
      "budget_used_percent": 8.795571633635328
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:12",
      "total_flops_so_far": 8804695670599680.0,
      "budget_used_percent": 8.80469567059968
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:12",
      "total_flops_so_far": 8813819707564032.0,
      "budget_used_percent": 8.813819707564031
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:13",
      "total_flops_so_far": 8822943744528384.0,
      "budget_used_percent": 8.822943744528384
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:13",
      "total_flops_so_far": 8832067781492736.0,
      "budget_used_percent": 8.832067781492736
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:13",
      "total_flops_so_far": 8841191818457088.0,
      "budget_used_percent": 8.841191818457087
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:14",
      "total_flops_so_far": 8850315855421440.0,
      "budget_used_percent": 8.850315855421439
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:14",
      "total_flops_so_far": 8859439892385792.0,
      "budget_used_percent": 8.859439892385792
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:15",
      "total_flops_so_far": 8868563929350144.0,
      "budget_used_percent": 8.868563929350143
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:15",
      "total_flops_so_far": 8877687966314496.0,
      "budget_used_percent": 8.877687966314497
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:16",
      "total_flops_so_far": 8886812003278848.0,
      "budget_used_percent": 8.886812003278848
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:16",
      "total_flops_so_far": 8895936040243200.0,
      "budget_used_percent": 8.8959360402432
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:17",
      "total_flops_so_far": 8905060077207552.0,
      "budget_used_percent": 8.905060077207553
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:17",
      "total_flops_so_far": 8914184114171904.0,
      "budget_used_percent": 8.914184114171904
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:18",
      "total_flops_so_far": 8923308151136256.0,
      "budget_used_percent": 8.923308151136256
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:18",
      "total_flops_so_far": 8932432188100608.0,
      "budget_used_percent": 8.93243218810061
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:18",
      "total_flops_so_far": 8941556225064960.0,
      "budget_used_percent": 8.94155622506496
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:19",
      "total_flops_so_far": 8950680262029312.0,
      "budget_used_percent": 8.950680262029312
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:19",
      "total_flops_so_far": 8959804298993664.0,
      "budget_used_percent": 8.959804298993664
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:20",
      "total_flops_so_far": 8968928335958016.0,
      "budget_used_percent": 8.968928335958017
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:20",
      "total_flops_so_far": 8978052372922368.0,
      "budget_used_percent": 8.978052372922368
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:21",
      "total_flops_so_far": 8987176409886720.0,
      "budget_used_percent": 8.98717640988672
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:21",
      "total_flops_so_far": 8996300446851072.0,
      "budget_used_percent": 8.996300446851071
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:22",
      "total_flops_so_far": 9005424483815424.0,
      "budget_used_percent": 9.005424483815425
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:22",
      "total_flops_so_far": 9014548520779776.0,
      "budget_used_percent": 9.014548520779776
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:23",
      "total_flops_so_far": 9023672557744128.0,
      "budget_used_percent": 9.023672557744128
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:23",
      "total_flops_so_far": 9032796594708480.0,
      "budget_used_percent": 9.03279659470848
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:23",
      "total_flops_so_far": 9041920631672832.0,
      "budget_used_percent": 9.041920631672832
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:24",
      "total_flops_so_far": 9051044668637184.0,
      "budget_used_percent": 9.051044668637184
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:24",
      "total_flops_so_far": 9060168705601536.0,
      "budget_used_percent": 9.060168705601535
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:25",
      "total_flops_so_far": 9069292742565888.0,
      "budget_used_percent": 9.069292742565887
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:25",
      "total_flops_so_far": 9078416779530240.0,
      "budget_used_percent": 9.07841677953024
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:26",
      "total_flops_so_far": 9087540816494592.0,
      "budget_used_percent": 9.087540816494592
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:26",
      "total_flops_so_far": 9096664853458944.0,
      "budget_used_percent": 9.096664853458943
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:27",
      "total_flops_so_far": 9105788890423296.0,
      "budget_used_percent": 9.105788890423296
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:27",
      "total_flops_so_far": 9114912927387648.0,
      "budget_used_percent": 9.114912927387648
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 768,
      "batch_size": 4,
      "forward_flops": 3041345654784.0,
      "backward_flops": 6082691309568.0,
      "flops": 9124036964352.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 17:57:28",
      "total_flops_so_far": 9124036964352000.0,
      "budget_used_percent": 9.124036964352001
    }
  ],
  "total_flops": 9124036964352000.0,
  "budget_used_percent": 9.124036964352001
}