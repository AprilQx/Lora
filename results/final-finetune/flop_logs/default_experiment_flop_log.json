{
  "experiment_name": "default_experiment",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-04-02 18:17:38",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:56",
      "total_flops_so_far": 5941058052096.0,
      "budget_used_percent": 0.005941058052096
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:57",
      "total_flops_so_far": 11882116104192.0,
      "budget_used_percent": 0.011882116104192
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:58",
      "total_flops_so_far": 17823174156288.0,
      "budget_used_percent": 0.017823174156288
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:58",
      "total_flops_so_far": 23764232208384.0,
      "budget_used_percent": 0.023764232208384
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:58",
      "total_flops_so_far": 29705290260480.0,
      "budget_used_percent": 0.02970529026048
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:59",
      "total_flops_so_far": 35646348312576.0,
      "budget_used_percent": 0.035646348312576
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:59",
      "total_flops_so_far": 41587406364672.0,
      "budget_used_percent": 0.041587406364672
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:17:59",
      "total_flops_so_far": 47528464416768.0,
      "budget_used_percent": 0.047528464416768
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:00",
      "total_flops_so_far": 53469522468864.0,
      "budget_used_percent": 0.05346952246886401
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:00",
      "total_flops_so_far": 59410580520960.0,
      "budget_used_percent": 0.05941058052096
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:00",
      "total_flops_so_far": 65351638573056.0,
      "budget_used_percent": 0.06535163857305601
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:00",
      "total_flops_so_far": 71292696625152.0,
      "budget_used_percent": 0.071292696625152
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:01",
      "total_flops_so_far": 77233754677248.0,
      "budget_used_percent": 0.07723375467724801
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:01",
      "total_flops_so_far": 83174812729344.0,
      "budget_used_percent": 0.083174812729344
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:01",
      "total_flops_so_far": 89115870781440.0,
      "budget_used_percent": 0.08911587078144001
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:02",
      "total_flops_so_far": 95056928833536.0,
      "budget_used_percent": 0.095056928833536
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:02",
      "total_flops_so_far": 100997986885632.0,
      "budget_used_percent": 0.10099798688563201
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:02",
      "total_flops_so_far": 106939044937728.0,
      "budget_used_percent": 0.10693904493772802
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:03",
      "total_flops_so_far": 112880102989824.0,
      "budget_used_percent": 0.112880102989824
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:03",
      "total_flops_so_far": 118821161041920.0,
      "budget_used_percent": 0.11882116104192
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:03",
      "total_flops_so_far": 124762219094016.0,
      "budget_used_percent": 0.12476221909401601
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:03",
      "total_flops_so_far": 130703277146112.0,
      "budget_used_percent": 0.13070327714611202
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:04",
      "total_flops_so_far": 136644335198208.0,
      "budget_used_percent": 0.13664433519820798
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:04",
      "total_flops_so_far": 142585393250304.0,
      "budget_used_percent": 0.142585393250304
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:04",
      "total_flops_so_far": 148526451302400.0,
      "budget_used_percent": 0.1485264513024
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:05",
      "total_flops_so_far": 154467509354496.0,
      "budget_used_percent": 0.15446750935449602
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:05",
      "total_flops_so_far": 160408567406592.0,
      "budget_used_percent": 0.16040856740659198
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:05",
      "total_flops_so_far": 166349625458688.0,
      "budget_used_percent": 0.166349625458688
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:06",
      "total_flops_so_far": 172290683510784.0,
      "budget_used_percent": 0.172290683510784
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:06",
      "total_flops_so_far": 178231741562880.0,
      "budget_used_percent": 0.17823174156288002
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:06",
      "total_flops_so_far": 184172799614976.0,
      "budget_used_percent": 0.18417279961497598
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:07",
      "total_flops_so_far": 190113857667072.0,
      "budget_used_percent": 0.190113857667072
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:07",
      "total_flops_so_far": 196054915719168.0,
      "budget_used_percent": 0.196054915719168
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:07",
      "total_flops_so_far": 201995973771264.0,
      "budget_used_percent": 0.20199597377126402
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:07",
      "total_flops_so_far": 207937031823360.0,
      "budget_used_percent": 0.20793703182336
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:08",
      "total_flops_so_far": 213878089875456.0,
      "budget_used_percent": 0.21387808987545603
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:08",
      "total_flops_so_far": 219819147927552.0,
      "budget_used_percent": 0.21981914792755197
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:08",
      "total_flops_so_far": 225760205979648.0,
      "budget_used_percent": 0.225760205979648
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:09",
      "total_flops_so_far": 231701264031744.0,
      "budget_used_percent": 0.231701264031744
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:09",
      "total_flops_so_far": 237642322083840.0,
      "budget_used_percent": 0.23764232208384
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:09",
      "total_flops_so_far": 243583380135936.0,
      "budget_used_percent": 0.243583380135936
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:10",
      "total_flops_so_far": 249524438188032.0,
      "budget_used_percent": 0.24952443818803202
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:10",
      "total_flops_so_far": 255465496240128.0,
      "budget_used_percent": 0.25546549624012804
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:10",
      "total_flops_so_far": 261406554292224.0,
      "budget_used_percent": 0.26140655429222404
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:10",
      "total_flops_so_far": 267347612344320.0,
      "budget_used_percent": 0.26734761234432
    },
    {
      "type": "training",
      "description": "Training step 45",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:11",
      "total_flops_so_far": 273288670396416.0,
      "budget_used_percent": 0.27328867039641597
    },
    {
      "type": "training",
      "description": "Training step 46",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:11",
      "total_flops_so_far": 279229728448512.0,
      "budget_used_percent": 0.279229728448512
    },
    {
      "type": "training",
      "description": "Training step 47",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:11",
      "total_flops_so_far": 285170786500608.0,
      "budget_used_percent": 0.285170786500608
    },
    {
      "type": "training",
      "description": "Training step 48",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:12",
      "total_flops_so_far": 291111844552704.0,
      "budget_used_percent": 0.291111844552704
    },
    {
      "type": "training",
      "description": "Training step 49",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:12",
      "total_flops_so_far": 297052902604800.0,
      "budget_used_percent": 0.2970529026048
    },
    {
      "type": "training",
      "description": "Training step 50",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:12",
      "total_flops_so_far": 302993960656896.0,
      "budget_used_percent": 0.302993960656896
    },
    {
      "type": "training",
      "description": "Training step 51",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:13",
      "total_flops_so_far": 308935018708992.0,
      "budget_used_percent": 0.30893501870899204
    },
    {
      "type": "training",
      "description": "Training step 52",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:13",
      "total_flops_so_far": 314876076761088.0,
      "budget_used_percent": 0.314876076761088
    },
    {
      "type": "training",
      "description": "Training step 53",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:13",
      "total_flops_so_far": 320817134813184.0,
      "budget_used_percent": 0.32081713481318397
    },
    {
      "type": "training",
      "description": "Training step 54",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:13",
      "total_flops_so_far": 326758192865280.0,
      "budget_used_percent": 0.32675819286528
    },
    {
      "type": "training",
      "description": "Training step 55",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:14",
      "total_flops_so_far": 332699250917376.0,
      "budget_used_percent": 0.332699250917376
    },
    {
      "type": "training",
      "description": "Training step 56",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:14",
      "total_flops_so_far": 338640308969472.0,
      "budget_used_percent": 0.338640308969472
    },
    {
      "type": "training",
      "description": "Training step 57",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:14",
      "total_flops_so_far": 344581367021568.0,
      "budget_used_percent": 0.344581367021568
    },
    {
      "type": "training",
      "description": "Training step 58",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:15",
      "total_flops_so_far": 350522425073664.0,
      "budget_used_percent": 0.350522425073664
    },
    {
      "type": "training",
      "description": "Training step 59",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:15",
      "total_flops_so_far": 356463483125760.0,
      "budget_used_percent": 0.35646348312576004
    },
    {
      "type": "training",
      "description": "Training step 60",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:15",
      "total_flops_so_far": 362404541177856.0,
      "budget_used_percent": 0.362404541177856
    },
    {
      "type": "training",
      "description": "Training step 61",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:16",
      "total_flops_so_far": 368345599229952.0,
      "budget_used_percent": 0.36834559922995197
    },
    {
      "type": "training",
      "description": "Training step 62",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:16",
      "total_flops_so_far": 374286657282048.0,
      "budget_used_percent": 0.374286657282048
    },
    {
      "type": "training",
      "description": "Training step 63",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:16",
      "total_flops_so_far": 380227715334144.0,
      "budget_used_percent": 0.380227715334144
    },
    {
      "type": "training",
      "description": "Training step 64",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:17",
      "total_flops_so_far": 386168773386240.0,
      "budget_used_percent": 0.38616877338624
    },
    {
      "type": "training",
      "description": "Training step 65",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:17",
      "total_flops_so_far": 392109831438336.0,
      "budget_used_percent": 0.392109831438336
    },
    {
      "type": "training",
      "description": "Training step 66",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:17",
      "total_flops_so_far": 398050889490432.0,
      "budget_used_percent": 0.398050889490432
    },
    {
      "type": "training",
      "description": "Training step 67",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:17",
      "total_flops_so_far": 403991947542528.0,
      "budget_used_percent": 0.40399194754252804
    },
    {
      "type": "training",
      "description": "Training step 68",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:18",
      "total_flops_so_far": 409933005594624.0,
      "budget_used_percent": 0.409933005594624
    },
    {
      "type": "training",
      "description": "Training step 69",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:18",
      "total_flops_so_far": 415874063646720.0,
      "budget_used_percent": 0.41587406364672
    },
    {
      "type": "training",
      "description": "Training step 70",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:18",
      "total_flops_so_far": 421815121698816.0,
      "budget_used_percent": 0.42181512169881596
    },
    {
      "type": "training",
      "description": "Training step 71",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:19",
      "total_flops_so_far": 427756179750912.0,
      "budget_used_percent": 0.42775617975091207
    },
    {
      "type": "training",
      "description": "Training step 72",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:19",
      "total_flops_so_far": 433697237803008.0,
      "budget_used_percent": 0.433697237803008
    },
    {
      "type": "training",
      "description": "Training step 73",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:19",
      "total_flops_so_far": 439638295855104.0,
      "budget_used_percent": 0.43963829585510394
    },
    {
      "type": "training",
      "description": "Training step 74",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:20",
      "total_flops_so_far": 445579353907200.0,
      "budget_used_percent": 0.4455793539072
    },
    {
      "type": "training",
      "description": "Training step 75",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:20",
      "total_flops_so_far": 451520411959296.0,
      "budget_used_percent": 0.451520411959296
    },
    {
      "type": "training",
      "description": "Training step 76",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:20",
      "total_flops_so_far": 457461470011392.0,
      "budget_used_percent": 0.45746147001139204
    },
    {
      "type": "training",
      "description": "Training step 77",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:20",
      "total_flops_so_far": 463402528063488.0,
      "budget_used_percent": 0.463402528063488
    },
    {
      "type": "training",
      "description": "Training step 78",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:21",
      "total_flops_so_far": 469343586115584.0,
      "budget_used_percent": 0.469343586115584
    },
    {
      "type": "training",
      "description": "Training step 79",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:21",
      "total_flops_so_far": 475284644167680.0,
      "budget_used_percent": 0.47528464416768
    },
    {
      "type": "training",
      "description": "Training step 80",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:21",
      "total_flops_so_far": 481225702219776.0,
      "budget_used_percent": 0.48122570221977595
    },
    {
      "type": "training",
      "description": "Training step 81",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:22",
      "total_flops_so_far": 487166760271872.0,
      "budget_used_percent": 0.487166760271872
    },
    {
      "type": "training",
      "description": "Training step 82",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:22",
      "total_flops_so_far": 493107818323968.0,
      "budget_used_percent": 0.493107818323968
    },
    {
      "type": "training",
      "description": "Training step 83",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:22",
      "total_flops_so_far": 499048876376064.0,
      "budget_used_percent": 0.49904887637606404
    },
    {
      "type": "training",
      "description": "Training step 84",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:23",
      "total_flops_so_far": 504989934428160.0,
      "budget_used_percent": 0.50498993442816
    },
    {
      "type": "training",
      "description": "Training step 85",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:23",
      "total_flops_so_far": 510930992480256.0,
      "budget_used_percent": 0.5109309924802561
    },
    {
      "type": "training",
      "description": "Training step 86",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:23",
      "total_flops_so_far": 516872050532352.0,
      "budget_used_percent": 0.516872050532352
    },
    {
      "type": "training",
      "description": "Training step 87",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:24",
      "total_flops_so_far": 522813108584448.0,
      "budget_used_percent": 0.5228131085844481
    },
    {
      "type": "training",
      "description": "Training step 88",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:24",
      "total_flops_so_far": 528754166636544.0,
      "budget_used_percent": 0.528754166636544
    },
    {
      "type": "training",
      "description": "Training step 89",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:24",
      "total_flops_so_far": 534695224688640.0,
      "budget_used_percent": 0.53469522468864
    },
    {
      "type": "training",
      "description": "Training step 90",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:24",
      "total_flops_so_far": 540636282740736.0,
      "budget_used_percent": 0.540636282740736
    },
    {
      "type": "training",
      "description": "Training step 91",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:25",
      "total_flops_so_far": 546577340792832.0,
      "budget_used_percent": 0.5465773407928319
    },
    {
      "type": "training",
      "description": "Training step 92",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:25",
      "total_flops_so_far": 552518398844928.0,
      "budget_used_percent": 0.552518398844928
    },
    {
      "type": "training",
      "description": "Training step 93",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:25",
      "total_flops_so_far": 558459456897024.0,
      "budget_used_percent": 0.558459456897024
    },
    {
      "type": "training",
      "description": "Training step 94",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:26",
      "total_flops_so_far": 564400514949120.0,
      "budget_used_percent": 0.56440051494912
    },
    {
      "type": "training",
      "description": "Training step 95",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:26",
      "total_flops_so_far": 570341573001216.0,
      "budget_used_percent": 0.570341573001216
    },
    {
      "type": "training",
      "description": "Training step 96",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:26",
      "total_flops_so_far": 576282631053312.0,
      "budget_used_percent": 0.576282631053312
    },
    {
      "type": "training",
      "description": "Training step 97",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:27",
      "total_flops_so_far": 582223689105408.0,
      "budget_used_percent": 0.582223689105408
    },
    {
      "type": "training",
      "description": "Training step 98",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:27",
      "total_flops_so_far": 588164747157504.0,
      "budget_used_percent": 0.588164747157504
    },
    {
      "type": "training",
      "description": "Training step 99",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:27",
      "total_flops_so_far": 594105805209600.0,
      "budget_used_percent": 0.5941058052096
    },
    {
      "type": "training",
      "description": "Training step 100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:27",
      "total_flops_so_far": 600046863261696.0,
      "budget_used_percent": 0.600046863261696
    },
    {
      "type": "training",
      "description": "Training step 101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:28",
      "total_flops_so_far": 605987921313792.0,
      "budget_used_percent": 0.605987921313792
    },
    {
      "type": "training",
      "description": "Training step 102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:28",
      "total_flops_so_far": 611928979365888.0,
      "budget_used_percent": 0.611928979365888
    },
    {
      "type": "training",
      "description": "Training step 103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:28",
      "total_flops_so_far": 617870037417984.0,
      "budget_used_percent": 0.6178700374179841
    },
    {
      "type": "training",
      "description": "Training step 104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:29",
      "total_flops_so_far": 623811095470080.0,
      "budget_used_percent": 0.62381109547008
    },
    {
      "type": "training",
      "description": "Training step 105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:29",
      "total_flops_so_far": 629752153522176.0,
      "budget_used_percent": 0.629752153522176
    },
    {
      "type": "training",
      "description": "Training step 106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:29",
      "total_flops_so_far": 635693211574272.0,
      "budget_used_percent": 0.635693211574272
    },
    {
      "type": "training",
      "description": "Training step 107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:30",
      "total_flops_so_far": 641634269626368.0,
      "budget_used_percent": 0.6416342696263679
    },
    {
      "type": "training",
      "description": "Training step 108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:30",
      "total_flops_so_far": 647575327678464.0,
      "budget_used_percent": 0.647575327678464
    },
    {
      "type": "training",
      "description": "Training step 109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:30",
      "total_flops_so_far": 653516385730560.0,
      "budget_used_percent": 0.65351638573056
    },
    {
      "type": "training",
      "description": "Training step 110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:31",
      "total_flops_so_far": 659457443782656.0,
      "budget_used_percent": 0.659457443782656
    },
    {
      "type": "training",
      "description": "Training step 111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:31",
      "total_flops_so_far": 665398501834752.0,
      "budget_used_percent": 0.665398501834752
    },
    {
      "type": "training",
      "description": "Training step 112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:31",
      "total_flops_so_far": 671339559886848.0,
      "budget_used_percent": 0.6713395598868479
    },
    {
      "type": "training",
      "description": "Training step 113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:31",
      "total_flops_so_far": 677280617938944.0,
      "budget_used_percent": 0.677280617938944
    },
    {
      "type": "training",
      "description": "Training step 114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:32",
      "total_flops_so_far": 683221675991040.0,
      "budget_used_percent": 0.68322167599104
    },
    {
      "type": "training",
      "description": "Training step 115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:32",
      "total_flops_so_far": 689162734043136.0,
      "budget_used_percent": 0.689162734043136
    },
    {
      "type": "training",
      "description": "Training step 116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:32",
      "total_flops_so_far": 695103792095232.0,
      "budget_used_percent": 0.695103792095232
    },
    {
      "type": "training",
      "description": "Training step 117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:33",
      "total_flops_so_far": 701044850147328.0,
      "budget_used_percent": 0.701044850147328
    },
    {
      "type": "training",
      "description": "Training step 118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:33",
      "total_flops_so_far": 706985908199424.0,
      "budget_used_percent": 0.706985908199424
    },
    {
      "type": "training",
      "description": "Training step 119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:33",
      "total_flops_so_far": 712926966251520.0,
      "budget_used_percent": 0.7129269662515201
    },
    {
      "type": "training",
      "description": "Training step 120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:34",
      "total_flops_so_far": 718868024303616.0,
      "budget_used_percent": 0.718868024303616
    },
    {
      "type": "training",
      "description": "Training step 121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:34",
      "total_flops_so_far": 724809082355712.0,
      "budget_used_percent": 0.724809082355712
    },
    {
      "type": "training",
      "description": "Training step 122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:34",
      "total_flops_so_far": 730750140407808.0,
      "budget_used_percent": 0.7307501404078081
    },
    {
      "type": "training",
      "description": "Training step 123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:35",
      "total_flops_so_far": 736691198459904.0,
      "budget_used_percent": 0.7366911984599039
    },
    {
      "type": "training",
      "description": "Training step 124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:35",
      "total_flops_so_far": 742632256512000.0,
      "budget_used_percent": 0.742632256512
    },
    {
      "type": "training",
      "description": "Training step 125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:35",
      "total_flops_so_far": 748573314564096.0,
      "budget_used_percent": 0.748573314564096
    },
    {
      "type": "training",
      "description": "Training step 126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:35",
      "total_flops_so_far": 754514372616192.0,
      "budget_used_percent": 0.754514372616192
    },
    {
      "type": "training",
      "description": "Training step 127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:36",
      "total_flops_so_far": 760455430668288.0,
      "budget_used_percent": 0.760455430668288
    },
    {
      "type": "training",
      "description": "Training step 128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:36",
      "total_flops_so_far": 766396488720384.0,
      "budget_used_percent": 0.7663964887203839
    },
    {
      "type": "training",
      "description": "Training step 129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:36",
      "total_flops_so_far": 772337546772480.0,
      "budget_used_percent": 0.77233754677248
    },
    {
      "type": "training",
      "description": "Training step 130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:37",
      "total_flops_so_far": 778278604824576.0,
      "budget_used_percent": 0.778278604824576
    },
    {
      "type": "training",
      "description": "Training step 131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:37",
      "total_flops_so_far": 784219662876672.0,
      "budget_used_percent": 0.784219662876672
    },
    {
      "type": "training",
      "description": "Training step 132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:37",
      "total_flops_so_far": 790160720928768.0,
      "budget_used_percent": 0.7901607209287681
    },
    {
      "type": "training",
      "description": "Training step 133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:38",
      "total_flops_so_far": 796101778980864.0,
      "budget_used_percent": 0.796101778980864
    },
    {
      "type": "training",
      "description": "Training step 134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:38",
      "total_flops_so_far": 802042837032960.0,
      "budget_used_percent": 0.80204283703296
    },
    {
      "type": "training",
      "description": "Training step 135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:38",
      "total_flops_so_far": 807983895085056.0,
      "budget_used_percent": 0.8079838950850561
    },
    {
      "type": "training",
      "description": "Training step 136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:39",
      "total_flops_so_far": 813924953137152.0,
      "budget_used_percent": 0.813924953137152
    },
    {
      "type": "training",
      "description": "Training step 137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:39",
      "total_flops_so_far": 819866011189248.0,
      "budget_used_percent": 0.819866011189248
    },
    {
      "type": "training",
      "description": "Training step 138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:39",
      "total_flops_so_far": 825807069241344.0,
      "budget_used_percent": 0.8258070692413441
    },
    {
      "type": "training",
      "description": "Training step 139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:39",
      "total_flops_so_far": 831748127293440.0,
      "budget_used_percent": 0.83174812729344
    },
    {
      "type": "training",
      "description": "Training step 140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:40",
      "total_flops_so_far": 837689185345536.0,
      "budget_used_percent": 0.8376891853455359
    },
    {
      "type": "training",
      "description": "Training step 141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:40",
      "total_flops_so_far": 843630243397632.0,
      "budget_used_percent": 0.8436302433976319
    },
    {
      "type": "training",
      "description": "Training step 142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:40",
      "total_flops_so_far": 849571301449728.0,
      "budget_used_percent": 0.849571301449728
    },
    {
      "type": "training",
      "description": "Training step 143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:41",
      "total_flops_so_far": 855512359501824.0,
      "budget_used_percent": 0.8555123595018241
    },
    {
      "type": "training",
      "description": "Training step 144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:41",
      "total_flops_so_far": 861453417553920.0,
      "budget_used_percent": 0.8614534175539199
    },
    {
      "type": "training",
      "description": "Training step 145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:41",
      "total_flops_so_far": 867394475606016.0,
      "budget_used_percent": 0.867394475606016
    },
    {
      "type": "training",
      "description": "Training step 146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:42",
      "total_flops_so_far": 873335533658112.0,
      "budget_used_percent": 0.873335533658112
    },
    {
      "type": "training",
      "description": "Training step 147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:42",
      "total_flops_so_far": 879276591710208.0,
      "budget_used_percent": 0.8792765917102079
    },
    {
      "type": "training",
      "description": "Training step 148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:42",
      "total_flops_so_far": 885217649762304.0,
      "budget_used_percent": 0.885217649762304
    },
    {
      "type": "training",
      "description": "Training step 149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:42",
      "total_flops_so_far": 891158707814400.0,
      "budget_used_percent": 0.8911587078144
    },
    {
      "type": "training",
      "description": "Training step 150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:43",
      "total_flops_so_far": 897099765866496.0,
      "budget_used_percent": 0.8970997658664961
    },
    {
      "type": "training",
      "description": "Training step 151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:43",
      "total_flops_so_far": 903040823918592.0,
      "budget_used_percent": 0.903040823918592
    },
    {
      "type": "training",
      "description": "Training step 152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:43",
      "total_flops_so_far": 908981881970688.0,
      "budget_used_percent": 0.908981881970688
    },
    {
      "type": "training",
      "description": "Training step 153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:44",
      "total_flops_so_far": 914922940022784.0,
      "budget_used_percent": 0.9149229400227841
    },
    {
      "type": "training",
      "description": "Training step 154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:44",
      "total_flops_so_far": 920863998074880.0,
      "budget_used_percent": 0.92086399807488
    },
    {
      "type": "training",
      "description": "Training step 155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:44",
      "total_flops_so_far": 926805056126976.0,
      "budget_used_percent": 0.926805056126976
    },
    {
      "type": "training",
      "description": "Training step 156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:45",
      "total_flops_so_far": 932746114179072.0,
      "budget_used_percent": 0.932746114179072
    },
    {
      "type": "training",
      "description": "Training step 157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:45",
      "total_flops_so_far": 938687172231168.0,
      "budget_used_percent": 0.938687172231168
    },
    {
      "type": "training",
      "description": "Training step 158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:45",
      "total_flops_so_far": 944628230283264.0,
      "budget_used_percent": 0.9446282302832639
    },
    {
      "type": "training",
      "description": "Training step 159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:46",
      "total_flops_so_far": 950569288335360.0,
      "budget_used_percent": 0.95056928833536
    },
    {
      "type": "training",
      "description": "Training step 160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:46",
      "total_flops_so_far": 956510346387456.0,
      "budget_used_percent": 0.956510346387456
    },
    {
      "type": "training",
      "description": "Training step 161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:46",
      "total_flops_so_far": 962451404439552.0,
      "budget_used_percent": 0.9624514044395519
    },
    {
      "type": "training",
      "description": "Training step 162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:46",
      "total_flops_so_far": 968392462491648.0,
      "budget_used_percent": 0.968392462491648
    },
    {
      "type": "training",
      "description": "Training step 163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:47",
      "total_flops_so_far": 974333520543744.0,
      "budget_used_percent": 0.974333520543744
    },
    {
      "type": "training",
      "description": "Training step 164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:47",
      "total_flops_so_far": 980274578595840.0,
      "budget_used_percent": 0.9802745785958401
    },
    {
      "type": "training",
      "description": "Training step 165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:47",
      "total_flops_so_far": 986215636647936.0,
      "budget_used_percent": 0.986215636647936
    },
    {
      "type": "training",
      "description": "Training step 166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:48",
      "total_flops_so_far": 992156694700032.0,
      "budget_used_percent": 0.992156694700032
    },
    {
      "type": "training",
      "description": "Training step 167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:48",
      "total_flops_so_far": 998097752752128.0,
      "budget_used_percent": 0.9980977527521281
    },
    {
      "type": "training",
      "description": "Training step 168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:48",
      "total_flops_so_far": 1004038810804224.0,
      "budget_used_percent": 1.004038810804224
    },
    {
      "type": "training",
      "description": "Training step 169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:49",
      "total_flops_so_far": 1009979868856320.0,
      "budget_used_percent": 1.00997986885632
    },
    {
      "type": "training",
      "description": "Training step 170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:49",
      "total_flops_so_far": 1015920926908416.0,
      "budget_used_percent": 1.015920926908416
    },
    {
      "type": "training",
      "description": "Training step 171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:49",
      "total_flops_so_far": 1021861984960512.0,
      "budget_used_percent": 1.0218619849605122
    },
    {
      "type": "training",
      "description": "Training step 172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:50",
      "total_flops_so_far": 1027803043012608.0,
      "budget_used_percent": 1.027803043012608
    },
    {
      "type": "training",
      "description": "Training step 173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:50",
      "total_flops_so_far": 1033744101064704.0,
      "budget_used_percent": 1.033744101064704
    },
    {
      "type": "training",
      "description": "Training step 174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:50",
      "total_flops_so_far": 1039685159116800.0,
      "budget_used_percent": 1.0396851591168
    },
    {
      "type": "training",
      "description": "Training step 175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:50",
      "total_flops_so_far": 1045626217168896.0,
      "budget_used_percent": 1.0456262171688961
    },
    {
      "type": "training",
      "description": "Training step 176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:51",
      "total_flops_so_far": 1051567275220992.0,
      "budget_used_percent": 1.051567275220992
    },
    {
      "type": "training",
      "description": "Training step 177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:51",
      "total_flops_so_far": 1057508333273088.0,
      "budget_used_percent": 1.057508333273088
    },
    {
      "type": "training",
      "description": "Training step 178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:51",
      "total_flops_so_far": 1063449391325184.0,
      "budget_used_percent": 1.0634493913251841
    },
    {
      "type": "training",
      "description": "Training step 179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:52",
      "total_flops_so_far": 1069390449377280.0,
      "budget_used_percent": 1.06939044937728
    },
    {
      "type": "training",
      "description": "Training step 180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:52",
      "total_flops_so_far": 1075331507429376.0,
      "budget_used_percent": 1.0753315074293759
    },
    {
      "type": "training",
      "description": "Training step 181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:52",
      "total_flops_so_far": 1081272565481472.0,
      "budget_used_percent": 1.081272565481472
    },
    {
      "type": "training",
      "description": "Training step 182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:53",
      "total_flops_so_far": 1087213623533568.0,
      "budget_used_percent": 1.087213623533568
    },
    {
      "type": "training",
      "description": "Training step 183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:53",
      "total_flops_so_far": 1093154681585664.0,
      "budget_used_percent": 1.0931546815856639
    },
    {
      "type": "training",
      "description": "Training step 184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:53",
      "total_flops_so_far": 1099095739637760.0,
      "budget_used_percent": 1.09909573963776
    },
    {
      "type": "training",
      "description": "Training step 185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:54",
      "total_flops_so_far": 1105036797689856.0,
      "budget_used_percent": 1.105036797689856
    },
    {
      "type": "training",
      "description": "Training step 186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:54",
      "total_flops_so_far": 1110977855741952.0,
      "budget_used_percent": 1.1109778557419518
    },
    {
      "type": "training",
      "description": "Training step 187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:54",
      "total_flops_so_far": 1116918913794048.0,
      "budget_used_percent": 1.116918913794048
    },
    {
      "type": "training",
      "description": "Training step 188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:54",
      "total_flops_so_far": 1122859971846144.0,
      "budget_used_percent": 1.122859971846144
    },
    {
      "type": "training",
      "description": "Training step 189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:55",
      "total_flops_so_far": 1128801029898240.0,
      "budget_used_percent": 1.12880102989824
    },
    {
      "type": "training",
      "description": "Training step 190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:55",
      "total_flops_so_far": 1134742087950336.0,
      "budget_used_percent": 1.134742087950336
    },
    {
      "type": "training",
      "description": "Training step 191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:55",
      "total_flops_so_far": 1140683146002432.0,
      "budget_used_percent": 1.140683146002432
    },
    {
      "type": "training",
      "description": "Training step 192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:56",
      "total_flops_so_far": 1146624204054528.0,
      "budget_used_percent": 1.146624204054528
    },
    {
      "type": "training",
      "description": "Training step 193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:56",
      "total_flops_so_far": 1152565262106624.0,
      "budget_used_percent": 1.152565262106624
    },
    {
      "type": "training",
      "description": "Training step 194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:56",
      "total_flops_so_far": 1158506320158720.0,
      "budget_used_percent": 1.15850632015872
    },
    {
      "type": "training",
      "description": "Training step 195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:57",
      "total_flops_so_far": 1164447378210816.0,
      "budget_used_percent": 1.164447378210816
    },
    {
      "type": "training",
      "description": "Training step 196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:57",
      "total_flops_so_far": 1170388436262912.0,
      "budget_used_percent": 1.170388436262912
    },
    {
      "type": "training",
      "description": "Training step 197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:57",
      "total_flops_so_far": 1176329494315008.0,
      "budget_used_percent": 1.176329494315008
    },
    {
      "type": "training",
      "description": "Training step 198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:58",
      "total_flops_so_far": 1182270552367104.0,
      "budget_used_percent": 1.182270552367104
    },
    {
      "type": "training",
      "description": "Training step 199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:58",
      "total_flops_so_far": 1188211610419200.0,
      "budget_used_percent": 1.1882116104192
    },
    {
      "type": "training",
      "description": "Training step 200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:58",
      "total_flops_so_far": 1194152668471296.0,
      "budget_used_percent": 1.194152668471296
    },
    {
      "type": "training",
      "description": "Training step 201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:58",
      "total_flops_so_far": 1200093726523392.0,
      "budget_used_percent": 1.200093726523392
    },
    {
      "type": "training",
      "description": "Training step 202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:59",
      "total_flops_so_far": 1206034784575488.0,
      "budget_used_percent": 1.206034784575488
    },
    {
      "type": "training",
      "description": "Training step 203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:59",
      "total_flops_so_far": 1211975842627584.0,
      "budget_used_percent": 1.211975842627584
    },
    {
      "type": "training",
      "description": "Training step 204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:18:59",
      "total_flops_so_far": 1217916900679680.0,
      "budget_used_percent": 1.21791690067968
    },
    {
      "type": "training",
      "description": "Training step 205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:00",
      "total_flops_so_far": 1223857958731776.0,
      "budget_used_percent": 1.223857958731776
    },
    {
      "type": "training",
      "description": "Training step 206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:00",
      "total_flops_so_far": 1229799016783872.0,
      "budget_used_percent": 1.229799016783872
    },
    {
      "type": "training",
      "description": "Training step 207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:00",
      "total_flops_so_far": 1235740074835968.0,
      "budget_used_percent": 1.2357400748359681
    },
    {
      "type": "training",
      "description": "Training step 208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:01",
      "total_flops_so_far": 1241681132888064.0,
      "budget_used_percent": 1.241681132888064
    },
    {
      "type": "training",
      "description": "Training step 209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:01",
      "total_flops_so_far": 1247622190940160.0,
      "budget_used_percent": 1.24762219094016
    },
    {
      "type": "training",
      "description": "Training step 210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:01",
      "total_flops_so_far": 1253563248992256.0,
      "budget_used_percent": 1.2535632489922561
    },
    {
      "type": "training",
      "description": "Training step 211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:02",
      "total_flops_so_far": 1259504307044352.0,
      "budget_used_percent": 1.259504307044352
    },
    {
      "type": "training",
      "description": "Training step 212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:02",
      "total_flops_so_far": 1265445365096448.0,
      "budget_used_percent": 1.265445365096448
    },
    {
      "type": "training",
      "description": "Training step 213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:02",
      "total_flops_so_far": 1271386423148544.0,
      "budget_used_percent": 1.271386423148544
    },
    {
      "type": "training",
      "description": "Training step 214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:02",
      "total_flops_so_far": 1277327481200640.0,
      "budget_used_percent": 1.27732748120064
    },
    {
      "type": "training",
      "description": "Training step 215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:03",
      "total_flops_so_far": 1283268539252736.0,
      "budget_used_percent": 1.2832685392527359
    },
    {
      "type": "training",
      "description": "Training step 216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:03",
      "total_flops_so_far": 1289209597304832.0,
      "budget_used_percent": 1.289209597304832
    },
    {
      "type": "training",
      "description": "Training step 217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:03",
      "total_flops_so_far": 1295150655356928.0,
      "budget_used_percent": 1.295150655356928
    },
    {
      "type": "training",
      "description": "Training step 218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:04",
      "total_flops_so_far": 1301091713409024.0,
      "budget_used_percent": 1.3010917134090239
    },
    {
      "type": "training",
      "description": "Training step 219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:04",
      "total_flops_so_far": 1307032771461120.0,
      "budget_used_percent": 1.30703277146112
    },
    {
      "type": "training",
      "description": "Training step 220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:04",
      "total_flops_so_far": 1312973829513216.0,
      "budget_used_percent": 1.312973829513216
    },
    {
      "type": "training",
      "description": "Training step 221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:05",
      "total_flops_so_far": 1318914887565312.0,
      "budget_used_percent": 1.318914887565312
    },
    {
      "type": "training",
      "description": "Training step 222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:05",
      "total_flops_so_far": 1324855945617408.0,
      "budget_used_percent": 1.324855945617408
    },
    {
      "type": "training",
      "description": "Training step 223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:05",
      "total_flops_so_far": 1330797003669504.0,
      "budget_used_percent": 1.330797003669504
    },
    {
      "type": "training",
      "description": "Training step 224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:06",
      "total_flops_so_far": 1336738061721600.0,
      "budget_used_percent": 1.3367380617216
    },
    {
      "type": "training",
      "description": "Training step 225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:06",
      "total_flops_so_far": 1342679119773696.0,
      "budget_used_percent": 1.3426791197736958
    },
    {
      "type": "training",
      "description": "Training step 226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:06",
      "total_flops_so_far": 1348620177825792.0,
      "budget_used_percent": 1.348620177825792
    },
    {
      "type": "training",
      "description": "Training step 227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:06",
      "total_flops_so_far": 1354561235877888.0,
      "budget_used_percent": 1.354561235877888
    },
    {
      "type": "training",
      "description": "Training step 228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:07",
      "total_flops_so_far": 1360502293929984.0,
      "budget_used_percent": 1.360502293929984
    },
    {
      "type": "training",
      "description": "Training step 229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:07",
      "total_flops_so_far": 1366443351982080.0,
      "budget_used_percent": 1.36644335198208
    },
    {
      "type": "training",
      "description": "Training step 230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:07",
      "total_flops_so_far": 1372384410034176.0,
      "budget_used_percent": 1.372384410034176
    },
    {
      "type": "training",
      "description": "Training step 231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:08",
      "total_flops_so_far": 1378325468086272.0,
      "budget_used_percent": 1.378325468086272
    },
    {
      "type": "training",
      "description": "Training step 232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:08",
      "total_flops_so_far": 1384266526138368.0,
      "budget_used_percent": 1.384266526138368
    },
    {
      "type": "training",
      "description": "Training step 233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:08",
      "total_flops_so_far": 1390207584190464.0,
      "budget_used_percent": 1.390207584190464
    },
    {
      "type": "training",
      "description": "Training step 234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:09",
      "total_flops_so_far": 1396148642242560.0,
      "budget_used_percent": 1.39614864224256
    },
    {
      "type": "training",
      "description": "Training step 235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:09",
      "total_flops_so_far": 1402089700294656.0,
      "budget_used_percent": 1.402089700294656
    },
    {
      "type": "training",
      "description": "Training step 236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:09",
      "total_flops_so_far": 1408030758346752.0,
      "budget_used_percent": 1.408030758346752
    },
    {
      "type": "training",
      "description": "Training step 237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:10",
      "total_flops_so_far": 1413971816398848.0,
      "budget_used_percent": 1.413971816398848
    },
    {
      "type": "training",
      "description": "Training step 238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:10",
      "total_flops_so_far": 1419912874450944.0,
      "budget_used_percent": 1.419912874450944
    },
    {
      "type": "training",
      "description": "Training step 239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:10",
      "total_flops_so_far": 1425853932503040.0,
      "budget_used_percent": 1.4258539325030402
    },
    {
      "type": "training",
      "description": "Training step 240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:10",
      "total_flops_so_far": 1431794990555136.0,
      "budget_used_percent": 1.431794990555136
    },
    {
      "type": "training",
      "description": "Training step 241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:11",
      "total_flops_so_far": 1437736048607232.0,
      "budget_used_percent": 1.437736048607232
    },
    {
      "type": "training",
      "description": "Training step 242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:11",
      "total_flops_so_far": 1443677106659328.0,
      "budget_used_percent": 1.4436771066593281
    },
    {
      "type": "training",
      "description": "Training step 243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:11",
      "total_flops_so_far": 1449618164711424.0,
      "budget_used_percent": 1.449618164711424
    },
    {
      "type": "training",
      "description": "Training step 244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:12",
      "total_flops_so_far": 1455559222763520.0,
      "budget_used_percent": 1.45555922276352
    },
    {
      "type": "training",
      "description": "Training step 245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:12",
      "total_flops_so_far": 1461500280815616.0,
      "budget_used_percent": 1.4615002808156161
    },
    {
      "type": "training",
      "description": "Training step 246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:12",
      "total_flops_so_far": 1467441338867712.0,
      "budget_used_percent": 1.467441338867712
    },
    {
      "type": "training",
      "description": "Training step 247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:13",
      "total_flops_so_far": 1473382396919808.0,
      "budget_used_percent": 1.4733823969198079
    },
    {
      "type": "training",
      "description": "Training step 248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:13",
      "total_flops_so_far": 1479323454971904.0,
      "budget_used_percent": 1.479323454971904
    },
    {
      "type": "training",
      "description": "Training step 249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:13",
      "total_flops_so_far": 1485264513024000.0,
      "budget_used_percent": 1.485264513024
    },
    {
      "type": "training",
      "description": "Training step 250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:14",
      "total_flops_so_far": 1491205571076096.0,
      "budget_used_percent": 1.4912055710760959
    },
    {
      "type": "training",
      "description": "Training step 251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:14",
      "total_flops_so_far": 1497146629128192.0,
      "budget_used_percent": 1.497146629128192
    },
    {
      "type": "training",
      "description": "Training step 252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:14",
      "total_flops_so_far": 1503087687180288.0,
      "budget_used_percent": 1.503087687180288
    },
    {
      "type": "training",
      "description": "Training step 253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:14",
      "total_flops_so_far": 1509028745232384.0,
      "budget_used_percent": 1.509028745232384
    },
    {
      "type": "training",
      "description": "Training step 254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:15",
      "total_flops_so_far": 1514969803284480.0,
      "budget_used_percent": 1.51496980328448
    },
    {
      "type": "training",
      "description": "Training step 255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:15",
      "total_flops_so_far": 1520910861336576.0,
      "budget_used_percent": 1.520910861336576
    },
    {
      "type": "training",
      "description": "Training step 256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:15",
      "total_flops_so_far": 1526851919388672.0,
      "budget_used_percent": 1.526851919388672
    },
    {
      "type": "training",
      "description": "Training step 257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:16",
      "total_flops_so_far": 1532792977440768.0,
      "budget_used_percent": 1.5327929774407678
    },
    {
      "type": "training",
      "description": "Training step 258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:16",
      "total_flops_so_far": 1538734035492864.0,
      "budget_used_percent": 1.538734035492864
    },
    {
      "type": "training",
      "description": "Training step 259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:16",
      "total_flops_so_far": 1544675093544960.0,
      "budget_used_percent": 1.54467509354496
    },
    {
      "type": "training",
      "description": "Training step 260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:17",
      "total_flops_so_far": 1550616151597056.0,
      "budget_used_percent": 1.550616151597056
    },
    {
      "type": "training",
      "description": "Training step 261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:17",
      "total_flops_so_far": 1556557209649152.0,
      "budget_used_percent": 1.556557209649152
    },
    {
      "type": "training",
      "description": "Training step 262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:17",
      "total_flops_so_far": 1562498267701248.0,
      "budget_used_percent": 1.562498267701248
    },
    {
      "type": "training",
      "description": "Training step 263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:18",
      "total_flops_so_far": 1568439325753344.0,
      "budget_used_percent": 1.568439325753344
    },
    {
      "type": "training",
      "description": "Training step 264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:18",
      "total_flops_so_far": 1574380383805440.0,
      "budget_used_percent": 1.57438038380544
    },
    {
      "type": "training",
      "description": "Training step 265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:18",
      "total_flops_so_far": 1580321441857536.0,
      "budget_used_percent": 1.5803214418575362
    },
    {
      "type": "training",
      "description": "Training step 266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:18",
      "total_flops_so_far": 1586262499909632.0,
      "budget_used_percent": 1.5862624999096318
    },
    {
      "type": "training",
      "description": "Training step 267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:19",
      "total_flops_so_far": 1592203557961728.0,
      "budget_used_percent": 1.592203557961728
    },
    {
      "type": "training",
      "description": "Training step 268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:19",
      "total_flops_so_far": 1598144616013824.0,
      "budget_used_percent": 1.598144616013824
    },
    {
      "type": "training",
      "description": "Training step 269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:19",
      "total_flops_so_far": 1604085674065920.0,
      "budget_used_percent": 1.60408567406592
    },
    {
      "type": "training",
      "description": "Training step 270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:20",
      "total_flops_so_far": 1610026732118016.0,
      "budget_used_percent": 1.610026732118016
    },
    {
      "type": "training",
      "description": "Training step 271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:20",
      "total_flops_so_far": 1615967790170112.0,
      "budget_used_percent": 1.6159677901701122
    },
    {
      "type": "training",
      "description": "Training step 272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:20",
      "total_flops_so_far": 1621908848222208.0,
      "budget_used_percent": 1.6219088482222082
    },
    {
      "type": "training",
      "description": "Training step 273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:21",
      "total_flops_so_far": 1627849906274304.0,
      "budget_used_percent": 1.627849906274304
    },
    {
      "type": "training",
      "description": "Training step 274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:21",
      "total_flops_so_far": 1633790964326400.0,
      "budget_used_percent": 1.6337909643264
    },
    {
      "type": "training",
      "description": "Training step 275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:21",
      "total_flops_so_far": 1639732022378496.0,
      "budget_used_percent": 1.639732022378496
    },
    {
      "type": "training",
      "description": "Training step 276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:22",
      "total_flops_so_far": 1645673080430592.0,
      "budget_used_percent": 1.645673080430592
    },
    {
      "type": "training",
      "description": "Training step 277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:22",
      "total_flops_so_far": 1651614138482688.0,
      "budget_used_percent": 1.6516141384826881
    },
    {
      "type": "training",
      "description": "Training step 278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:22",
      "total_flops_so_far": 1657555196534784.0,
      "budget_used_percent": 1.6575551965347841
    },
    {
      "type": "training",
      "description": "Training step 279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:22",
      "total_flops_so_far": 1663496254586880.0,
      "budget_used_percent": 1.66349625458688
    },
    {
      "type": "training",
      "description": "Training step 280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:23",
      "total_flops_so_far": 1669437312638976.0,
      "budget_used_percent": 1.669437312638976
    },
    {
      "type": "training",
      "description": "Training step 281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:23",
      "total_flops_so_far": 1675378370691072.0,
      "budget_used_percent": 1.6753783706910719
    },
    {
      "type": "training",
      "description": "Training step 282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:23",
      "total_flops_so_far": 1681319428743168.0,
      "budget_used_percent": 1.6813194287431679
    },
    {
      "type": "training",
      "description": "Training step 283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:24",
      "total_flops_so_far": 1687260486795264.0,
      "budget_used_percent": 1.6872604867952639
    },
    {
      "type": "training",
      "description": "Training step 284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:24",
      "total_flops_so_far": 1693201544847360.0,
      "budget_used_percent": 1.69320154484736
    },
    {
      "type": "training",
      "description": "Training step 285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:24",
      "total_flops_so_far": 1699142602899456.0,
      "budget_used_percent": 1.699142602899456
    },
    {
      "type": "training",
      "description": "Training step 286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:25",
      "total_flops_so_far": 1705083660951552.0,
      "budget_used_percent": 1.705083660951552
    },
    {
      "type": "training",
      "description": "Training step 287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:25",
      "total_flops_so_far": 1711024719003648.0,
      "budget_used_percent": 1.7110247190036483
    },
    {
      "type": "training",
      "description": "Training step 288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:25",
      "total_flops_so_far": 1716965777055744.0,
      "budget_used_percent": 1.7169657770557438
    },
    {
      "type": "training",
      "description": "Training step 289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:26",
      "total_flops_so_far": 1722906835107840.0,
      "budget_used_percent": 1.7229068351078398
    },
    {
      "type": "training",
      "description": "Training step 290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:26",
      "total_flops_so_far": 1728847893159936.0,
      "budget_used_percent": 1.728847893159936
    },
    {
      "type": "training",
      "description": "Training step 291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:26",
      "total_flops_so_far": 1734788951212032.0,
      "budget_used_percent": 1.734788951212032
    },
    {
      "type": "training",
      "description": "Training step 292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:27",
      "total_flops_so_far": 1740730009264128.0,
      "budget_used_percent": 1.740730009264128
    },
    {
      "type": "training",
      "description": "Training step 293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:27",
      "total_flops_so_far": 1746671067316224.0,
      "budget_used_percent": 1.746671067316224
    },
    {
      "type": "training",
      "description": "Training step 294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:27",
      "total_flops_so_far": 1752612125368320.0,
      "budget_used_percent": 1.7526121253683202
    },
    {
      "type": "training",
      "description": "Training step 295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:27",
      "total_flops_so_far": 1758553183420416.0,
      "budget_used_percent": 1.7585531834204158
    },
    {
      "type": "training",
      "description": "Training step 296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:28",
      "total_flops_so_far": 1764494241472512.0,
      "budget_used_percent": 1.764494241472512
    },
    {
      "type": "training",
      "description": "Training step 297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:28",
      "total_flops_so_far": 1770435299524608.0,
      "budget_used_percent": 1.770435299524608
    },
    {
      "type": "training",
      "description": "Training step 298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:28",
      "total_flops_so_far": 1776376357576704.0,
      "budget_used_percent": 1.776376357576704
    },
    {
      "type": "training",
      "description": "Training step 299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:29",
      "total_flops_so_far": 1782317415628800.0,
      "budget_used_percent": 1.7823174156288
    },
    {
      "type": "training",
      "description": "Training step 300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:29",
      "total_flops_so_far": 1788258473680896.0,
      "budget_used_percent": 1.7882584736808962
    },
    {
      "type": "training",
      "description": "Training step 301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:29",
      "total_flops_so_far": 1794199531732992.0,
      "budget_used_percent": 1.7941995317329922
    },
    {
      "type": "training",
      "description": "Training step 302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:30",
      "total_flops_so_far": 1800140589785088.0,
      "budget_used_percent": 1.800140589785088
    },
    {
      "type": "training",
      "description": "Training step 303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:30",
      "total_flops_so_far": 1806081647837184.0,
      "budget_used_percent": 1.806081647837184
    },
    {
      "type": "training",
      "description": "Training step 304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:30",
      "total_flops_so_far": 1812022705889280.0,
      "budget_used_percent": 1.81202270588928
    },
    {
      "type": "training",
      "description": "Training step 305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:31",
      "total_flops_so_far": 1817963763941376.0,
      "budget_used_percent": 1.817963763941376
    },
    {
      "type": "training",
      "description": "Training step 306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:31",
      "total_flops_so_far": 1823904821993472.0,
      "budget_used_percent": 1.8239048219934721
    },
    {
      "type": "training",
      "description": "Training step 307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:31",
      "total_flops_so_far": 1829845880045568.0,
      "budget_used_percent": 1.8298458800455681
    },
    {
      "type": "training",
      "description": "Training step 308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:31",
      "total_flops_so_far": 1835786938097664.0,
      "budget_used_percent": 1.8357869380976641
    },
    {
      "type": "training",
      "description": "Training step 309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:32",
      "total_flops_so_far": 1841727996149760.0,
      "budget_used_percent": 1.84172799614976
    },
    {
      "type": "training",
      "description": "Training step 310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:32",
      "total_flops_so_far": 1847669054201856.0,
      "budget_used_percent": 1.847669054201856
    },
    {
      "type": "training",
      "description": "Training step 311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:32",
      "total_flops_so_far": 1853610112253952.0,
      "budget_used_percent": 1.853610112253952
    },
    {
      "type": "training",
      "description": "Training step 312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:33",
      "total_flops_so_far": 1859551170306048.0,
      "budget_used_percent": 1.859551170306048
    },
    {
      "type": "training",
      "description": "Training step 313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:33",
      "total_flops_so_far": 1865492228358144.0,
      "budget_used_percent": 1.865492228358144
    },
    {
      "type": "training",
      "description": "Training step 314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:33",
      "total_flops_so_far": 1871433286410240.0,
      "budget_used_percent": 1.87143328641024
    },
    {
      "type": "training",
      "description": "Training step 315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:34",
      "total_flops_so_far": 1877374344462336.0,
      "budget_used_percent": 1.877374344462336
    },
    {
      "type": "training",
      "description": "Training step 316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:34",
      "total_flops_so_far": 1883315402514432.0,
      "budget_used_percent": 1.8833154025144319
    },
    {
      "type": "training",
      "description": "Training step 317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:34",
      "total_flops_so_far": 1889256460566528.0,
      "budget_used_percent": 1.8892564605665279
    },
    {
      "type": "training",
      "description": "Training step 318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:35",
      "total_flops_so_far": 1895197518618624.0,
      "budget_used_percent": 1.8951975186186238
    },
    {
      "type": "training",
      "description": "Training step 319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:35",
      "total_flops_so_far": 1901138576670720.0,
      "budget_used_percent": 1.90113857667072
    },
    {
      "type": "training",
      "description": "Training step 320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:35",
      "total_flops_so_far": 1907079634722816.0,
      "budget_used_percent": 1.907079634722816
    },
    {
      "type": "training",
      "description": "Training step 321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:35",
      "total_flops_so_far": 1913020692774912.0,
      "budget_used_percent": 1.913020692774912
    },
    {
      "type": "training",
      "description": "Training step 322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:36",
      "total_flops_so_far": 1918961750827008.0,
      "budget_used_percent": 1.918961750827008
    },
    {
      "type": "training",
      "description": "Training step 323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:36",
      "total_flops_so_far": 1924902808879104.0,
      "budget_used_percent": 1.9249028088791038
    },
    {
      "type": "training",
      "description": "Training step 324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:36",
      "total_flops_so_far": 1930843866931200.0,
      "budget_used_percent": 1.9308438669311998
    },
    {
      "type": "training",
      "description": "Training step 325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:37",
      "total_flops_so_far": 1936784924983296.0,
      "budget_used_percent": 1.936784924983296
    },
    {
      "type": "training",
      "description": "Training step 326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:37",
      "total_flops_so_far": 1942725983035392.0,
      "budget_used_percent": 1.942725983035392
    },
    {
      "type": "training",
      "description": "Training step 327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:37",
      "total_flops_so_far": 1948667041087488.0,
      "budget_used_percent": 1.948667041087488
    },
    {
      "type": "training",
      "description": "Training step 328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:38",
      "total_flops_so_far": 1954608099139584.0,
      "budget_used_percent": 1.954608099139584
    },
    {
      "type": "training",
      "description": "Training step 329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:38",
      "total_flops_so_far": 1960549157191680.0,
      "budget_used_percent": 1.9605491571916802
    },
    {
      "type": "training",
      "description": "Training step 330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:38",
      "total_flops_so_far": 1966490215243776.0,
      "budget_used_percent": 1.9664902152437758
    },
    {
      "type": "training",
      "description": "Training step 331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:39",
      "total_flops_so_far": 1972431273295872.0,
      "budget_used_percent": 1.972431273295872
    },
    {
      "type": "training",
      "description": "Training step 332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:39",
      "total_flops_so_far": 1978372331347968.0,
      "budget_used_percent": 1.978372331347968
    },
    {
      "type": "training",
      "description": "Training step 333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:39",
      "total_flops_so_far": 1984313389400064.0,
      "budget_used_percent": 1.984313389400064
    },
    {
      "type": "training",
      "description": "Training step 334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:40",
      "total_flops_so_far": 1990254447452160.0,
      "budget_used_percent": 1.99025444745216
    },
    {
      "type": "training",
      "description": "Training step 335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:40",
      "total_flops_so_far": 1996195505504256.0,
      "budget_used_percent": 1.9961955055042562
    },
    {
      "type": "training",
      "description": "Training step 336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:40",
      "total_flops_so_far": 2002136563556352.0,
      "budget_used_percent": 2.002136563556352
    },
    {
      "type": "training",
      "description": "Training step 337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:40",
      "total_flops_so_far": 2008077621608448.0,
      "budget_used_percent": 2.008077621608448
    },
    {
      "type": "training",
      "description": "Training step 338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:41",
      "total_flops_so_far": 2014018679660544.0,
      "budget_used_percent": 2.014018679660544
    },
    {
      "type": "training",
      "description": "Training step 339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:41",
      "total_flops_so_far": 2019959737712640.0,
      "budget_used_percent": 2.01995973771264
    },
    {
      "type": "training",
      "description": "Training step 340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:41",
      "total_flops_so_far": 2025900795764736.0,
      "budget_used_percent": 2.025900795764736
    },
    {
      "type": "training",
      "description": "Training step 341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:42",
      "total_flops_so_far": 2031841853816832.0,
      "budget_used_percent": 2.031841853816832
    },
    {
      "type": "training",
      "description": "Training step 342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:42",
      "total_flops_so_far": 2037782911868928.0,
      "budget_used_percent": 2.037782911868928
    },
    {
      "type": "training",
      "description": "Training step 343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:42",
      "total_flops_so_far": 2043723969921024.0,
      "budget_used_percent": 2.0437239699210243
    },
    {
      "type": "training",
      "description": "Training step 344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:43",
      "total_flops_so_far": 2049665027973120.0,
      "budget_used_percent": 2.0496650279731203
    },
    {
      "type": "training",
      "description": "Training step 345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:43",
      "total_flops_so_far": 2055606086025216.0,
      "budget_used_percent": 2.055606086025216
    },
    {
      "type": "training",
      "description": "Training step 346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:43",
      "total_flops_so_far": 2061547144077312.0,
      "budget_used_percent": 2.061547144077312
    },
    {
      "type": "training",
      "description": "Training step 347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:44",
      "total_flops_so_far": 2067488202129408.0,
      "budget_used_percent": 2.067488202129408
    },
    {
      "type": "training",
      "description": "Training step 348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:44",
      "total_flops_so_far": 2073429260181504.0,
      "budget_used_percent": 2.073429260181504
    },
    {
      "type": "training",
      "description": "Training step 349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:44",
      "total_flops_so_far": 2079370318233600.0,
      "budget_used_percent": 2.0793703182336
    },
    {
      "type": "training",
      "description": "Training step 350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:45",
      "total_flops_so_far": 2085311376285696.0,
      "budget_used_percent": 2.0853113762856963
    },
    {
      "type": "training",
      "description": "Training step 351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:45",
      "total_flops_so_far": 2091252434337792.0,
      "budget_used_percent": 2.0912524343377923
    },
    {
      "type": "training",
      "description": "Training step 352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:45",
      "total_flops_so_far": 2097193492389888.0,
      "budget_used_percent": 2.097193492389888
    },
    {
      "type": "training",
      "description": "Training step 353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:45",
      "total_flops_so_far": 2103134550441984.0,
      "budget_used_percent": 2.103134550441984
    },
    {
      "type": "training",
      "description": "Training step 354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:46",
      "total_flops_so_far": 2109075608494080.0,
      "budget_used_percent": 2.10907560849408
    },
    {
      "type": "training",
      "description": "Training step 355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:46",
      "total_flops_so_far": 2115016666546176.0,
      "budget_used_percent": 2.115016666546176
    },
    {
      "type": "training",
      "description": "Training step 356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:46",
      "total_flops_so_far": 2120957724598272.0,
      "budget_used_percent": 2.1209577245982723
    },
    {
      "type": "training",
      "description": "Training step 357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:47",
      "total_flops_so_far": 2126898782650368.0,
      "budget_used_percent": 2.1268987826503682
    },
    {
      "type": "training",
      "description": "Training step 358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:47",
      "total_flops_so_far": 2132839840702464.0,
      "budget_used_percent": 2.1328398407024642
    },
    {
      "type": "training",
      "description": "Training step 359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:47",
      "total_flops_so_far": 2138780898754560.0,
      "budget_used_percent": 2.13878089875456
    },
    {
      "type": "training",
      "description": "Training step 360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:47",
      "total_flops_so_far": 2144721956806656.0,
      "budget_used_percent": 2.144721956806656
    },
    {
      "type": "training",
      "description": "Training step 361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:48",
      "total_flops_so_far": 2150663014858752.0,
      "budget_used_percent": 2.1506630148587518
    },
    {
      "type": "training",
      "description": "Training step 362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:48",
      "total_flops_so_far": 2156604072910848.0,
      "budget_used_percent": 2.156604072910848
    },
    {
      "type": "training",
      "description": "Training step 363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:48",
      "total_flops_so_far": 2162545130962944.0,
      "budget_used_percent": 2.162545130962944
    },
    {
      "type": "training",
      "description": "Training step 364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:49",
      "total_flops_so_far": 2168486189015040.0,
      "budget_used_percent": 2.16848618901504
    },
    {
      "type": "training",
      "description": "Training step 365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:49",
      "total_flops_so_far": 2174427247067136.0,
      "budget_used_percent": 2.174427247067136
    },
    {
      "type": "training",
      "description": "Training step 366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:49",
      "total_flops_so_far": 2180368305119232.0,
      "budget_used_percent": 2.1803683051192317
    },
    {
      "type": "training",
      "description": "Training step 367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:50",
      "total_flops_so_far": 2186309363171328.0,
      "budget_used_percent": 2.1863093631713277
    },
    {
      "type": "training",
      "description": "Training step 368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:50",
      "total_flops_so_far": 2192250421223424.0,
      "budget_used_percent": 2.1922504212234237
    },
    {
      "type": "training",
      "description": "Training step 369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:50",
      "total_flops_so_far": 2198191479275520.0,
      "budget_used_percent": 2.19819147927552
    },
    {
      "type": "training",
      "description": "Training step 370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:51",
      "total_flops_so_far": 2204132537327616.0,
      "budget_used_percent": 2.204132537327616
    },
    {
      "type": "training",
      "description": "Training step 371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:51",
      "total_flops_so_far": 2210073595379712.0,
      "budget_used_percent": 2.210073595379712
    },
    {
      "type": "training",
      "description": "Training step 372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:51",
      "total_flops_so_far": 2216014653431808.0,
      "budget_used_percent": 2.216014653431808
    },
    {
      "type": "training",
      "description": "Training step 373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:52",
      "total_flops_so_far": 2221955711483904.0,
      "budget_used_percent": 2.2219557114839037
    },
    {
      "type": "training",
      "description": "Training step 374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:52",
      "total_flops_so_far": 2227896769536000.0,
      "budget_used_percent": 2.2278967695359997
    },
    {
      "type": "training",
      "description": "Training step 375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:52",
      "total_flops_so_far": 2233837827588096.0,
      "budget_used_percent": 2.233837827588096
    },
    {
      "type": "training",
      "description": "Training step 376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:52",
      "total_flops_so_far": 2239778885640192.0,
      "budget_used_percent": 2.239778885640192
    },
    {
      "type": "training",
      "description": "Training step 377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:53",
      "total_flops_so_far": 2245719943692288.0,
      "budget_used_percent": 2.245719943692288
    },
    {
      "type": "training",
      "description": "Training step 378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:53",
      "total_flops_so_far": 2251661001744384.0,
      "budget_used_percent": 2.251661001744384
    },
    {
      "type": "training",
      "description": "Training step 379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:53",
      "total_flops_so_far": 2257602059796480.0,
      "budget_used_percent": 2.25760205979648
    },
    {
      "type": "training",
      "description": "Training step 380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:54",
      "total_flops_so_far": 2263543117848576.0,
      "budget_used_percent": 2.2635431178485756
    },
    {
      "type": "training",
      "description": "Training step 381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:54",
      "total_flops_so_far": 2269484175900672.0,
      "budget_used_percent": 2.269484175900672
    },
    {
      "type": "training",
      "description": "Training step 382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:54",
      "total_flops_so_far": 2275425233952768.0,
      "budget_used_percent": 2.275425233952768
    },
    {
      "type": "training",
      "description": "Training step 383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:55",
      "total_flops_so_far": 2281366292004864.0,
      "budget_used_percent": 2.281366292004864
    },
    {
      "type": "training",
      "description": "Training step 384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:55",
      "total_flops_so_far": 2287307350056960.0,
      "budget_used_percent": 2.28730735005696
    },
    {
      "type": "training",
      "description": "Training step 385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:55",
      "total_flops_so_far": 2293248408109056.0,
      "budget_used_percent": 2.293248408109056
    },
    {
      "type": "training",
      "description": "Training step 386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:56",
      "total_flops_so_far": 2299189466161152.0,
      "budget_used_percent": 2.299189466161152
    },
    {
      "type": "training",
      "description": "Training step 387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:56",
      "total_flops_so_far": 2305130524213248.0,
      "budget_used_percent": 2.305130524213248
    },
    {
      "type": "training",
      "description": "Training step 388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:56",
      "total_flops_so_far": 2311071582265344.0,
      "budget_used_percent": 2.311071582265344
    },
    {
      "type": "training",
      "description": "Training step 389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:57",
      "total_flops_so_far": 2317012640317440.0,
      "budget_used_percent": 2.31701264031744
    },
    {
      "type": "training",
      "description": "Training step 390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:57",
      "total_flops_so_far": 2322953698369536.0,
      "budget_used_percent": 2.322953698369536
    },
    {
      "type": "training",
      "description": "Training step 391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:57",
      "total_flops_so_far": 2328894756421632.0,
      "budget_used_percent": 2.328894756421632
    },
    {
      "type": "training",
      "description": "Training step 392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:57",
      "total_flops_so_far": 2334835814473728.0,
      "budget_used_percent": 2.334835814473728
    },
    {
      "type": "training",
      "description": "Training step 393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:58",
      "total_flops_so_far": 2340776872525824.0,
      "budget_used_percent": 2.340776872525824
    },
    {
      "type": "training",
      "description": "Training step 394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:58",
      "total_flops_so_far": 2346717930577920.0,
      "budget_used_percent": 2.34671793057792
    },
    {
      "type": "training",
      "description": "Training step 395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:58",
      "total_flops_so_far": 2352658988630016.0,
      "budget_used_percent": 2.352658988630016
    },
    {
      "type": "training",
      "description": "Training step 396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:59",
      "total_flops_so_far": 2358600046682112.0,
      "budget_used_percent": 2.358600046682112
    },
    {
      "type": "training",
      "description": "Training step 397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:59",
      "total_flops_so_far": 2364541104734208.0,
      "budget_used_percent": 2.364541104734208
    },
    {
      "type": "training",
      "description": "Training step 398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:19:59",
      "total_flops_so_far": 2370482162786304.0,
      "budget_used_percent": 2.370482162786304
    },
    {
      "type": "training",
      "description": "Training step 399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:00",
      "total_flops_so_far": 2376423220838400.0,
      "budget_used_percent": 2.3764232208384
    },
    {
      "type": "training",
      "description": "Training step 400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:00",
      "total_flops_so_far": 2382364278890496.0,
      "budget_used_percent": 2.382364278890496
    },
    {
      "type": "training",
      "description": "Training step 401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:00",
      "total_flops_so_far": 2388305336942592.0,
      "budget_used_percent": 2.388305336942592
    },
    {
      "type": "training",
      "description": "Training step 402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:01",
      "total_flops_so_far": 2394246394994688.0,
      "budget_used_percent": 2.394246394994688
    },
    {
      "type": "training",
      "description": "Training step 403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:01",
      "total_flops_so_far": 2400187453046784.0,
      "budget_used_percent": 2.400187453046784
    },
    {
      "type": "training",
      "description": "Training step 404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:01",
      "total_flops_so_far": 2406128511098880.0,
      "budget_used_percent": 2.40612851109888
    },
    {
      "type": "training",
      "description": "Training step 405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:02",
      "total_flops_so_far": 2412069569150976.0,
      "budget_used_percent": 2.412069569150976
    },
    {
      "type": "training",
      "description": "Training step 406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:02",
      "total_flops_so_far": 2418010627203072.0,
      "budget_used_percent": 2.418010627203072
    },
    {
      "type": "training",
      "description": "Training step 407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:02",
      "total_flops_so_far": 2423951685255168.0,
      "budget_used_percent": 2.423951685255168
    },
    {
      "type": "training",
      "description": "Training step 408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:02",
      "total_flops_so_far": 2429892743307264.0,
      "budget_used_percent": 2.4298927433072643
    },
    {
      "type": "training",
      "description": "Training step 409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:03",
      "total_flops_so_far": 2435833801359360.0,
      "budget_used_percent": 2.43583380135936
    },
    {
      "type": "training",
      "description": "Training step 410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:03",
      "total_flops_so_far": 2441774859411456.0,
      "budget_used_percent": 2.441774859411456
    },
    {
      "type": "training",
      "description": "Training step 411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:03",
      "total_flops_so_far": 2447715917463552.0,
      "budget_used_percent": 2.447715917463552
    },
    {
      "type": "training",
      "description": "Training step 412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:04",
      "total_flops_so_far": 2453656975515648.0,
      "budget_used_percent": 2.453656975515648
    },
    {
      "type": "training",
      "description": "Training step 413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:04",
      "total_flops_so_far": 2459598033567744.0,
      "budget_used_percent": 2.459598033567744
    },
    {
      "type": "training",
      "description": "Training step 414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:04",
      "total_flops_so_far": 2465539091619840.0,
      "budget_used_percent": 2.4655390916198403
    },
    {
      "type": "training",
      "description": "Training step 415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:05",
      "total_flops_so_far": 2471480149671936.0,
      "budget_used_percent": 2.4714801496719363
    },
    {
      "type": "training",
      "description": "Training step 416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:05",
      "total_flops_so_far": 2477421207724032.0,
      "budget_used_percent": 2.477421207724032
    },
    {
      "type": "training",
      "description": "Training step 417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:05",
      "total_flops_so_far": 2483362265776128.0,
      "budget_used_percent": 2.483362265776128
    },
    {
      "type": "training",
      "description": "Training step 418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:06",
      "total_flops_so_far": 2489303323828224.0,
      "budget_used_percent": 2.489303323828224
    },
    {
      "type": "training",
      "description": "Training step 419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:06",
      "total_flops_so_far": 2495244381880320.0,
      "budget_used_percent": 2.49524438188032
    },
    {
      "type": "training",
      "description": "Training step 420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:06",
      "total_flops_so_far": 2501185439932416.0,
      "budget_used_percent": 2.5011854399324163
    },
    {
      "type": "training",
      "description": "Training step 421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:07",
      "total_flops_so_far": 2507126497984512.0,
      "budget_used_percent": 2.5071264979845123
    },
    {
      "type": "training",
      "description": "Training step 422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:07",
      "total_flops_so_far": 2513067556036608.0,
      "budget_used_percent": 2.5130675560366083
    },
    {
      "type": "training",
      "description": "Training step 423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:07",
      "total_flops_so_far": 2519008614088704.0,
      "budget_used_percent": 2.519008614088704
    },
    {
      "type": "training",
      "description": "Training step 424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:07",
      "total_flops_so_far": 2524949672140800.0,
      "budget_used_percent": 2.5249496721408
    },
    {
      "type": "training",
      "description": "Training step 425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:08",
      "total_flops_so_far": 2530890730192896.0,
      "budget_used_percent": 2.530890730192896
    },
    {
      "type": "training",
      "description": "Training step 426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:08",
      "total_flops_so_far": 2536831788244992.0,
      "budget_used_percent": 2.5368317882449922
    },
    {
      "type": "training",
      "description": "Training step 427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:08",
      "total_flops_so_far": 2542772846297088.0,
      "budget_used_percent": 2.542772846297088
    },
    {
      "type": "training",
      "description": "Training step 428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:09",
      "total_flops_so_far": 2548713904349184.0,
      "budget_used_percent": 2.548713904349184
    },
    {
      "type": "training",
      "description": "Training step 429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:09",
      "total_flops_so_far": 2554654962401280.0,
      "budget_used_percent": 2.55465496240128
    },
    {
      "type": "training",
      "description": "Training step 430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:09",
      "total_flops_so_far": 2560596020453376.0,
      "budget_used_percent": 2.5605960204533758
    },
    {
      "type": "training",
      "description": "Training step 431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:10",
      "total_flops_so_far": 2566537078505472.0,
      "budget_used_percent": 2.5665370785054717
    },
    {
      "type": "training",
      "description": "Training step 432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:10",
      "total_flops_so_far": 2572478136557568.0,
      "budget_used_percent": 2.5724781365575677
    },
    {
      "type": "training",
      "description": "Training step 433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:10",
      "total_flops_so_far": 2578419194609664.0,
      "budget_used_percent": 2.578419194609664
    },
    {
      "type": "training",
      "description": "Training step 434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:11",
      "total_flops_so_far": 2584360252661760.0,
      "budget_used_percent": 2.58436025266176
    },
    {
      "type": "training",
      "description": "Training step 435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:11",
      "total_flops_so_far": 2590301310713856.0,
      "budget_used_percent": 2.590301310713856
    },
    {
      "type": "training",
      "description": "Training step 436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:11",
      "total_flops_so_far": 2596242368765952.0,
      "budget_used_percent": 2.596242368765952
    },
    {
      "type": "training",
      "description": "Training step 437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:12",
      "total_flops_so_far": 2602183426818048.0,
      "budget_used_percent": 2.6021834268180477
    },
    {
      "type": "training",
      "description": "Training step 438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:12",
      "total_flops_so_far": 2608124484870144.0,
      "budget_used_percent": 2.6081244848701437
    },
    {
      "type": "training",
      "description": "Training step 439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:12",
      "total_flops_so_far": 2614065542922240.0,
      "budget_used_percent": 2.61406554292224
    },
    {
      "type": "training",
      "description": "Training step 440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:12",
      "total_flops_so_far": 2620006600974336.0,
      "budget_used_percent": 2.620006600974336
    },
    {
      "type": "training",
      "description": "Training step 441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:13",
      "total_flops_so_far": 2625947659026432.0,
      "budget_used_percent": 2.625947659026432
    },
    {
      "type": "training",
      "description": "Training step 442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:13",
      "total_flops_so_far": 2631888717078528.0,
      "budget_used_percent": 2.631888717078528
    },
    {
      "type": "training",
      "description": "Training step 443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:13",
      "total_flops_so_far": 2637829775130624.0,
      "budget_used_percent": 2.637829775130624
    },
    {
      "type": "training",
      "description": "Training step 444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:14",
      "total_flops_so_far": 2643770833182720.0,
      "budget_used_percent": 2.6437708331827197
    },
    {
      "type": "training",
      "description": "Training step 445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:14",
      "total_flops_so_far": 2649711891234816.0,
      "budget_used_percent": 2.649711891234816
    },
    {
      "type": "training",
      "description": "Training step 446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:14",
      "total_flops_so_far": 2655652949286912.0,
      "budget_used_percent": 2.655652949286912
    },
    {
      "type": "training",
      "description": "Training step 447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:15",
      "total_flops_so_far": 2661594007339008.0,
      "budget_used_percent": 2.661594007339008
    },
    {
      "type": "training",
      "description": "Training step 448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:15",
      "total_flops_so_far": 2667535065391104.0,
      "budget_used_percent": 2.667535065391104
    },
    {
      "type": "training",
      "description": "Training step 449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:15",
      "total_flops_so_far": 2673476123443200.0,
      "budget_used_percent": 2.6734761234432
    },
    {
      "type": "training",
      "description": "Training step 450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:16",
      "total_flops_so_far": 2679417181495296.0,
      "budget_used_percent": 2.679417181495296
    },
    {
      "type": "training",
      "description": "Training step 451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:16",
      "total_flops_so_far": 2685358239547392.0,
      "budget_used_percent": 2.6853582395473916
    },
    {
      "type": "training",
      "description": "Training step 452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:16",
      "total_flops_so_far": 2691299297599488.0,
      "budget_used_percent": 2.691299297599488
    },
    {
      "type": "training",
      "description": "Training step 453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:17",
      "total_flops_so_far": 2697240355651584.0,
      "budget_used_percent": 2.697240355651584
    },
    {
      "type": "training",
      "description": "Training step 454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:17",
      "total_flops_so_far": 2703181413703680.0,
      "budget_used_percent": 2.70318141370368
    },
    {
      "type": "training",
      "description": "Training step 455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:17",
      "total_flops_so_far": 2709122471755776.0,
      "budget_used_percent": 2.709122471755776
    },
    {
      "type": "training",
      "description": "Training step 456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:17",
      "total_flops_so_far": 2715063529807872.0,
      "budget_used_percent": 2.715063529807872
    },
    {
      "type": "training",
      "description": "Training step 457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:18",
      "total_flops_so_far": 2721004587859968.0,
      "budget_used_percent": 2.721004587859968
    },
    {
      "type": "training",
      "description": "Training step 458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:18",
      "total_flops_so_far": 2726945645912064.0,
      "budget_used_percent": 2.726945645912064
    },
    {
      "type": "training",
      "description": "Training step 459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:18",
      "total_flops_so_far": 2732886703964160.0,
      "budget_used_percent": 2.73288670396416
    },
    {
      "type": "training",
      "description": "Training step 460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:19",
      "total_flops_so_far": 2738827762016256.0,
      "budget_used_percent": 2.738827762016256
    },
    {
      "type": "training",
      "description": "Training step 461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:19",
      "total_flops_so_far": 2744768820068352.0,
      "budget_used_percent": 2.744768820068352
    },
    {
      "type": "training",
      "description": "Training step 462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:19",
      "total_flops_so_far": 2750709878120448.0,
      "budget_used_percent": 2.750709878120448
    },
    {
      "type": "training",
      "description": "Training step 463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:20",
      "total_flops_so_far": 2756650936172544.0,
      "budget_used_percent": 2.756650936172544
    },
    {
      "type": "training",
      "description": "Training step 464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:20",
      "total_flops_so_far": 2762591994224640.0,
      "budget_used_percent": 2.76259199422464
    },
    {
      "type": "training",
      "description": "Training step 465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:20",
      "total_flops_so_far": 2768533052276736.0,
      "budget_used_percent": 2.768533052276736
    },
    {
      "type": "training",
      "description": "Training step 466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:21",
      "total_flops_so_far": 2774474110328832.0,
      "budget_used_percent": 2.774474110328832
    },
    {
      "type": "training",
      "description": "Training step 467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:21",
      "total_flops_so_far": 2780415168380928.0,
      "budget_used_percent": 2.780415168380928
    },
    {
      "type": "training",
      "description": "Training step 468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:21",
      "total_flops_so_far": 2786356226433024.0,
      "budget_used_percent": 2.786356226433024
    },
    {
      "type": "training",
      "description": "Training step 469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:22",
      "total_flops_so_far": 2792297284485120.0,
      "budget_used_percent": 2.79229728448512
    },
    {
      "type": "training",
      "description": "Training step 470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:22",
      "total_flops_so_far": 2798238342537216.0,
      "budget_used_percent": 2.798238342537216
    },
    {
      "type": "training",
      "description": "Training step 471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:22",
      "total_flops_so_far": 2804179400589312.0,
      "budget_used_percent": 2.804179400589312
    },
    {
      "type": "training",
      "description": "Training step 472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:23",
      "total_flops_so_far": 2810120458641408.0,
      "budget_used_percent": 2.8101204586414084
    },
    {
      "type": "training",
      "description": "Training step 473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:23",
      "total_flops_so_far": 2816061516693504.0,
      "budget_used_percent": 2.816061516693504
    },
    {
      "type": "training",
      "description": "Training step 474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:23",
      "total_flops_so_far": 2822002574745600.0,
      "budget_used_percent": 2.8220025747456
    },
    {
      "type": "training",
      "description": "Training step 475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:23",
      "total_flops_so_far": 2827943632797696.0,
      "budget_used_percent": 2.827943632797696
    },
    {
      "type": "training",
      "description": "Training step 476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:24",
      "total_flops_so_far": 2833884690849792.0,
      "budget_used_percent": 2.833884690849792
    },
    {
      "type": "training",
      "description": "Training step 477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:24",
      "total_flops_so_far": 2839825748901888.0,
      "budget_used_percent": 2.839825748901888
    },
    {
      "type": "training",
      "description": "Training step 478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:24",
      "total_flops_so_far": 2845766806953984.0,
      "budget_used_percent": 2.8457668069539843
    },
    {
      "type": "training",
      "description": "Training step 479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:25",
      "total_flops_so_far": 2851707865006080.0,
      "budget_used_percent": 2.8517078650060803
    },
    {
      "type": "training",
      "description": "Training step 480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:25",
      "total_flops_so_far": 2857648923058176.0,
      "budget_used_percent": 2.857648923058176
    },
    {
      "type": "training",
      "description": "Training step 481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:25",
      "total_flops_so_far": 2863589981110272.0,
      "budget_used_percent": 2.863589981110272
    },
    {
      "type": "training",
      "description": "Training step 482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:26",
      "total_flops_so_far": 2869531039162368.0,
      "budget_used_percent": 2.869531039162368
    },
    {
      "type": "training",
      "description": "Training step 483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:26",
      "total_flops_so_far": 2875472097214464.0,
      "budget_used_percent": 2.875472097214464
    },
    {
      "type": "training",
      "description": "Training step 484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:26",
      "total_flops_so_far": 2881413155266560.0,
      "budget_used_percent": 2.88141315526656
    },
    {
      "type": "training",
      "description": "Training step 485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:27",
      "total_flops_so_far": 2887354213318656.0,
      "budget_used_percent": 2.8873542133186563
    },
    {
      "type": "training",
      "description": "Training step 486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:27",
      "total_flops_so_far": 2893295271370752.0,
      "budget_used_percent": 2.8932952713707523
    },
    {
      "type": "training",
      "description": "Training step 487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:27",
      "total_flops_so_far": 2899236329422848.0,
      "budget_used_percent": 2.899236329422848
    },
    {
      "type": "training",
      "description": "Training step 488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:28",
      "total_flops_so_far": 2905177387474944.0,
      "budget_used_percent": 2.905177387474944
    },
    {
      "type": "training",
      "description": "Training step 489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:28",
      "total_flops_so_far": 2911118445527040.0,
      "budget_used_percent": 2.91111844552704
    },
    {
      "type": "training",
      "description": "Training step 490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:28",
      "total_flops_so_far": 2917059503579136.0,
      "budget_used_percent": 2.917059503579136
    },
    {
      "type": "training",
      "description": "Training step 491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:28",
      "total_flops_so_far": 2923000561631232.0,
      "budget_used_percent": 2.9230005616312322
    },
    {
      "type": "training",
      "description": "Training step 492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:29",
      "total_flops_so_far": 2928941619683328.0,
      "budget_used_percent": 2.9289416196833282
    },
    {
      "type": "training",
      "description": "Training step 493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:29",
      "total_flops_so_far": 2934882677735424.0,
      "budget_used_percent": 2.934882677735424
    },
    {
      "type": "training",
      "description": "Training step 494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:29",
      "total_flops_so_far": 2940823735787520.0,
      "budget_used_percent": 2.9408237357875198
    },
    {
      "type": "training",
      "description": "Training step 495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:30",
      "total_flops_so_far": 2946764793839616.0,
      "budget_used_percent": 2.9467647938396158
    },
    {
      "type": "training",
      "description": "Training step 496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:30",
      "total_flops_so_far": 2952705851891712.0,
      "budget_used_percent": 2.9527058518917118
    },
    {
      "type": "training",
      "description": "Training step 497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:30",
      "total_flops_so_far": 2958646909943808.0,
      "budget_used_percent": 2.958646909943808
    },
    {
      "type": "training",
      "description": "Training step 498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:31",
      "total_flops_so_far": 2964587967995904.0,
      "budget_used_percent": 2.964587967995904
    },
    {
      "type": "training",
      "description": "Training step 499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:20:31",
      "total_flops_so_far": 2970529026048000.0,
      "budget_used_percent": 2.970529026048
    },
    {
      "type": "training",
      "description": "Training step 500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:54",
      "total_flops_so_far": 2976470084100096.0,
      "budget_used_percent": 2.976470084100096
    },
    {
      "type": "training",
      "description": "Training step 501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:55",
      "total_flops_so_far": 2982411142152192.0,
      "budget_used_percent": 2.9824111421521917
    },
    {
      "type": "training",
      "description": "Training step 502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:55",
      "total_flops_so_far": 2988352200204288.0,
      "budget_used_percent": 2.9883522002042877
    },
    {
      "type": "training",
      "description": "Training step 503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:55",
      "total_flops_so_far": 2994293258256384.0,
      "budget_used_percent": 2.994293258256384
    },
    {
      "type": "training",
      "description": "Training step 504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:56",
      "total_flops_so_far": 3000234316308480.0,
      "budget_used_percent": 3.00023431630848
    },
    {
      "type": "training",
      "description": "Training step 505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:56",
      "total_flops_so_far": 3006175374360576.0,
      "budget_used_percent": 3.006175374360576
    },
    {
      "type": "training",
      "description": "Training step 506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:56",
      "total_flops_so_far": 3012116432412672.0,
      "budget_used_percent": 3.012116432412672
    },
    {
      "type": "training",
      "description": "Training step 507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:57",
      "total_flops_so_far": 3018057490464768.0,
      "budget_used_percent": 3.018057490464768
    },
    {
      "type": "training",
      "description": "Training step 508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:57",
      "total_flops_so_far": 3023998548516864.0,
      "budget_used_percent": 3.0239985485168637
    },
    {
      "type": "training",
      "description": "Training step 509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:57",
      "total_flops_so_far": 3029939606568960.0,
      "budget_used_percent": 3.02993960656896
    },
    {
      "type": "training",
      "description": "Training step 510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:57",
      "total_flops_so_far": 3035880664621056.0,
      "budget_used_percent": 3.035880664621056
    },
    {
      "type": "training",
      "description": "Training step 511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:58",
      "total_flops_so_far": 3041821722673152.0,
      "budget_used_percent": 3.041821722673152
    },
    {
      "type": "training",
      "description": "Training step 512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:58",
      "total_flops_so_far": 3047762780725248.0,
      "budget_used_percent": 3.047762780725248
    },
    {
      "type": "training",
      "description": "Training step 513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:58",
      "total_flops_so_far": 3053703838777344.0,
      "budget_used_percent": 3.053703838777344
    },
    {
      "type": "training",
      "description": "Training step 514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:59",
      "total_flops_so_far": 3059644896829440.0,
      "budget_used_percent": 3.05964489682944
    },
    {
      "type": "training",
      "description": "Training step 515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:59",
      "total_flops_so_far": 3065585954881536.0,
      "budget_used_percent": 3.0655859548815356
    },
    {
      "type": "training",
      "description": "Training step 516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:22:59",
      "total_flops_so_far": 3071527012933632.0,
      "budget_used_percent": 3.071527012933632
    },
    {
      "type": "training",
      "description": "Training step 517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:00",
      "total_flops_so_far": 3077468070985728.0,
      "budget_used_percent": 3.077468070985728
    },
    {
      "type": "training",
      "description": "Training step 518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:00",
      "total_flops_so_far": 3083409129037824.0,
      "budget_used_percent": 3.083409129037824
    },
    {
      "type": "training",
      "description": "Training step 519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:00",
      "total_flops_so_far": 3089350187089920.0,
      "budget_used_percent": 3.08935018708992
    },
    {
      "type": "training",
      "description": "Training step 520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:01",
      "total_flops_so_far": 3095291245142016.0,
      "budget_used_percent": 3.095291245142016
    },
    {
      "type": "training",
      "description": "Training step 521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:01",
      "total_flops_so_far": 3101232303194112.0,
      "budget_used_percent": 3.101232303194112
    },
    {
      "type": "training",
      "description": "Training step 522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:01",
      "total_flops_so_far": 3107173361246208.0,
      "budget_used_percent": 3.107173361246208
    },
    {
      "type": "training",
      "description": "Training step 523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:02",
      "total_flops_so_far": 3113114419298304.0,
      "budget_used_percent": 3.113114419298304
    },
    {
      "type": "training",
      "description": "Training step 524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:02",
      "total_flops_so_far": 3119055477350400.0,
      "budget_used_percent": 3.1190554773504
    },
    {
      "type": "training",
      "description": "Training step 525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:02",
      "total_flops_so_far": 3124996535402496.0,
      "budget_used_percent": 3.124996535402496
    },
    {
      "type": "training",
      "description": "Training step 526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:03",
      "total_flops_so_far": 3130937593454592.0,
      "budget_used_percent": 3.130937593454592
    },
    {
      "type": "training",
      "description": "Training step 527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:03",
      "total_flops_so_far": 3136878651506688.0,
      "budget_used_percent": 3.136878651506688
    },
    {
      "type": "training",
      "description": "Training step 528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:03",
      "total_flops_so_far": 3142819709558784.0,
      "budget_used_percent": 3.142819709558784
    },
    {
      "type": "training",
      "description": "Training step 529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:03",
      "total_flops_so_far": 3148760767610880.0,
      "budget_used_percent": 3.14876076761088
    },
    {
      "type": "training",
      "description": "Training step 530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:04",
      "total_flops_so_far": 3154701825662976.0,
      "budget_used_percent": 3.1547018256629764
    },
    {
      "type": "training",
      "description": "Training step 531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:04",
      "total_flops_so_far": 3160642883715072.0,
      "budget_used_percent": 3.1606428837150724
    },
    {
      "type": "training",
      "description": "Training step 532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:04",
      "total_flops_so_far": 3166583941767168.0,
      "budget_used_percent": 3.1665839417671684
    },
    {
      "type": "training",
      "description": "Training step 533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:05",
      "total_flops_so_far": 3172524999819264.0,
      "budget_used_percent": 3.1725249998192635
    },
    {
      "type": "training",
      "description": "Training step 534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:05",
      "total_flops_so_far": 3178466057871360.0,
      "budget_used_percent": 3.1784660578713595
    },
    {
      "type": "training",
      "description": "Training step 535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:05",
      "total_flops_so_far": 3184407115923456.0,
      "budget_used_percent": 3.184407115923456
    },
    {
      "type": "training",
      "description": "Training step 536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:06",
      "total_flops_so_far": 3190348173975552.0,
      "budget_used_percent": 3.190348173975552
    },
    {
      "type": "training",
      "description": "Training step 537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:06",
      "total_flops_so_far": 3196289232027648.0,
      "budget_used_percent": 3.196289232027648
    },
    {
      "type": "training",
      "description": "Training step 538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:06",
      "total_flops_so_far": 3202230290079744.0,
      "budget_used_percent": 3.202230290079744
    },
    {
      "type": "training",
      "description": "Training step 539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:07",
      "total_flops_so_far": 3208171348131840.0,
      "budget_used_percent": 3.20817134813184
    },
    {
      "type": "training",
      "description": "Training step 540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:07",
      "total_flops_so_far": 3214112406183936.0,
      "budget_used_percent": 3.214112406183936
    },
    {
      "type": "training",
      "description": "Training step 541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:07",
      "total_flops_so_far": 3220053464236032.0,
      "budget_used_percent": 3.220053464236032
    },
    {
      "type": "training",
      "description": "Training step 542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:08",
      "total_flops_so_far": 3225994522288128.0,
      "budget_used_percent": 3.2259945222881283
    },
    {
      "type": "training",
      "description": "Training step 543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:08",
      "total_flops_so_far": 3231935580340224.0,
      "budget_used_percent": 3.2319355803402243
    },
    {
      "type": "training",
      "description": "Training step 544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:08",
      "total_flops_so_far": 3237876638392320.0,
      "budget_used_percent": 3.2378766383923203
    },
    {
      "type": "training",
      "description": "Training step 545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:09",
      "total_flops_so_far": 3243817696444416.0,
      "budget_used_percent": 3.2438176964444163
    },
    {
      "type": "training",
      "description": "Training step 546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:09",
      "total_flops_so_far": 3249758754496512.0,
      "budget_used_percent": 3.2497587544965123
    },
    {
      "type": "training",
      "description": "Training step 547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:09",
      "total_flops_so_far": 3255699812548608.0,
      "budget_used_percent": 3.255699812548608
    },
    {
      "type": "training",
      "description": "Training step 548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:09",
      "total_flops_so_far": 3261640870600704.0,
      "budget_used_percent": 3.261640870600704
    },
    {
      "type": "training",
      "description": "Training step 549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:10",
      "total_flops_so_far": 3267581928652800.0,
      "budget_used_percent": 3.2675819286528
    },
    {
      "type": "training",
      "description": "Training step 550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:10",
      "total_flops_so_far": 3273522986704896.0,
      "budget_used_percent": 3.273522986704896
    },
    {
      "type": "training",
      "description": "Training step 551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:10",
      "total_flops_so_far": 3279464044756992.0,
      "budget_used_percent": 3.279464044756992
    },
    {
      "type": "training",
      "description": "Training step 552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:11",
      "total_flops_so_far": 3285405102809088.0,
      "budget_used_percent": 3.285405102809088
    },
    {
      "type": "training",
      "description": "Training step 553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:11",
      "total_flops_so_far": 3291346160861184.0,
      "budget_used_percent": 3.291346160861184
    },
    {
      "type": "training",
      "description": "Training step 554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:11",
      "total_flops_so_far": 3297287218913280.0,
      "budget_used_percent": 3.29728721891328
    },
    {
      "type": "training",
      "description": "Training step 555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:12",
      "total_flops_so_far": 3303228276965376.0,
      "budget_used_percent": 3.3032282769653762
    },
    {
      "type": "training",
      "description": "Training step 556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:12",
      "total_flops_so_far": 3309169335017472.0,
      "budget_used_percent": 3.3091693350174722
    },
    {
      "type": "training",
      "description": "Training step 557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:12",
      "total_flops_so_far": 3315110393069568.0,
      "budget_used_percent": 3.3151103930695682
    },
    {
      "type": "training",
      "description": "Training step 558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:13",
      "total_flops_so_far": 3321051451121664.0,
      "budget_used_percent": 3.321051451121664
    },
    {
      "type": "training",
      "description": "Training step 559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:13",
      "total_flops_so_far": 3326992509173760.0,
      "budget_used_percent": 3.32699250917376
    },
    {
      "type": "training",
      "description": "Training step 560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:13",
      "total_flops_so_far": 3332933567225856.0,
      "budget_used_percent": 3.332933567225856
    },
    {
      "type": "training",
      "description": "Training step 561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:14",
      "total_flops_so_far": 3338874625277952.0,
      "budget_used_percent": 3.338874625277952
    },
    {
      "type": "training",
      "description": "Training step 562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:14",
      "total_flops_so_far": 3344815683330048.0,
      "budget_used_percent": 3.3448156833300478
    },
    {
      "type": "training",
      "description": "Training step 563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:14",
      "total_flops_so_far": 3350756741382144.0,
      "budget_used_percent": 3.3507567413821437
    },
    {
      "type": "training",
      "description": "Training step 564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:15",
      "total_flops_so_far": 3356697799434240.0,
      "budget_used_percent": 3.3566977994342397
    },
    {
      "type": "training",
      "description": "Training step 565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:15",
      "total_flops_so_far": 3362638857486336.0,
      "budget_used_percent": 3.3626388574863357
    },
    {
      "type": "training",
      "description": "Training step 566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:15",
      "total_flops_so_far": 3368579915538432.0,
      "budget_used_percent": 3.3685799155384317
    },
    {
      "type": "training",
      "description": "Training step 567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:16",
      "total_flops_so_far": 3374520973590528.0,
      "budget_used_percent": 3.3745209735905277
    },
    {
      "type": "training",
      "description": "Training step 568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:16",
      "total_flops_so_far": 3380462031642624.0,
      "budget_used_percent": 3.380462031642624
    },
    {
      "type": "training",
      "description": "Training step 569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:16",
      "total_flops_so_far": 3386403089694720.0,
      "budget_used_percent": 3.38640308969472
    },
    {
      "type": "training",
      "description": "Training step 570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:16",
      "total_flops_so_far": 3392344147746816.0,
      "budget_used_percent": 3.392344147746816
    },
    {
      "type": "training",
      "description": "Training step 571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:17",
      "total_flops_so_far": 3398285205798912.0,
      "budget_used_percent": 3.398285205798912
    },
    {
      "type": "training",
      "description": "Training step 572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:17",
      "total_flops_so_far": 3404226263851008.0,
      "budget_used_percent": 3.404226263851008
    },
    {
      "type": "training",
      "description": "Training step 573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:17",
      "total_flops_so_far": 3410167321903104.0,
      "budget_used_percent": 3.410167321903104
    },
    {
      "type": "training",
      "description": "Training step 574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:18",
      "total_flops_so_far": 3416108379955200.0,
      "budget_used_percent": 3.4161083799552
    },
    {
      "type": "training",
      "description": "Training step 575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:18",
      "total_flops_so_far": 3422049438007296.0,
      "budget_used_percent": 3.4220494380072966
    },
    {
      "type": "training",
      "description": "Training step 576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:18",
      "total_flops_so_far": 3427990496059392.0,
      "budget_used_percent": 3.4279904960593917
    },
    {
      "type": "training",
      "description": "Training step 577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:19",
      "total_flops_so_far": 3433931554111488.0,
      "budget_used_percent": 3.4339315541114876
    },
    {
      "type": "training",
      "description": "Training step 578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:19",
      "total_flops_so_far": 3439872612163584.0,
      "budget_used_percent": 3.4398726121635836
    },
    {
      "type": "training",
      "description": "Training step 579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:19",
      "total_flops_so_far": 3445813670215680.0,
      "budget_used_percent": 3.4458136702156796
    },
    {
      "type": "training",
      "description": "Training step 580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:20",
      "total_flops_so_far": 3451754728267776.0,
      "budget_used_percent": 3.451754728267776
    },
    {
      "type": "training",
      "description": "Training step 581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:20",
      "total_flops_so_far": 3457695786319872.0,
      "budget_used_percent": 3.457695786319872
    },
    {
      "type": "training",
      "description": "Training step 582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:20",
      "total_flops_so_far": 3463636844371968.0,
      "budget_used_percent": 3.463636844371968
    },
    {
      "type": "training",
      "description": "Training step 583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:21",
      "total_flops_so_far": 3469577902424064.0,
      "budget_used_percent": 3.469577902424064
    },
    {
      "type": "training",
      "description": "Training step 584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:21",
      "total_flops_so_far": 3475518960476160.0,
      "budget_used_percent": 3.47551896047616
    },
    {
      "type": "training",
      "description": "Training step 585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:21",
      "total_flops_so_far": 3481460018528256.0,
      "budget_used_percent": 3.481460018528256
    },
    {
      "type": "training",
      "description": "Training step 586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:22",
      "total_flops_so_far": 3487401076580352.0,
      "budget_used_percent": 3.487401076580352
    },
    {
      "type": "training",
      "description": "Training step 587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:22",
      "total_flops_so_far": 3493342134632448.0,
      "budget_used_percent": 3.493342134632448
    },
    {
      "type": "training",
      "description": "Training step 588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:22",
      "total_flops_so_far": 3499283192684544.0,
      "budget_used_percent": 3.4992831926845445
    },
    {
      "type": "training",
      "description": "Training step 589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:22",
      "total_flops_so_far": 3505224250736640.0,
      "budget_used_percent": 3.5052242507366405
    },
    {
      "type": "training",
      "description": "Training step 590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:23",
      "total_flops_so_far": 3511165308788736.0,
      "budget_used_percent": 3.5111653087887356
    },
    {
      "type": "training",
      "description": "Training step 591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:23",
      "total_flops_so_far": 3517106366840832.0,
      "budget_used_percent": 3.5171063668408316
    },
    {
      "type": "training",
      "description": "Training step 592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:23",
      "total_flops_so_far": 3523047424892928.0,
      "budget_used_percent": 3.523047424892928
    },
    {
      "type": "training",
      "description": "Training step 593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:24",
      "total_flops_so_far": 3528988482945024.0,
      "budget_used_percent": 3.528988482945024
    },
    {
      "type": "training",
      "description": "Training step 594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:24",
      "total_flops_so_far": 3534929540997120.0,
      "budget_used_percent": 3.53492954099712
    },
    {
      "type": "training",
      "description": "Training step 595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:24",
      "total_flops_so_far": 3540870599049216.0,
      "budget_used_percent": 3.540870599049216
    },
    {
      "type": "training",
      "description": "Training step 596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:25",
      "total_flops_so_far": 3546811657101312.0,
      "budget_used_percent": 3.546811657101312
    },
    {
      "type": "training",
      "description": "Training step 597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:25",
      "total_flops_so_far": 3552752715153408.0,
      "budget_used_percent": 3.552752715153408
    },
    {
      "type": "training",
      "description": "Training step 598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:25",
      "total_flops_so_far": 3558693773205504.0,
      "budget_used_percent": 3.558693773205504
    },
    {
      "type": "training",
      "description": "Training step 599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:26",
      "total_flops_so_far": 3564634831257600.0,
      "budget_used_percent": 3.5646348312576
    },
    {
      "type": "training",
      "description": "Training step 600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:26",
      "total_flops_so_far": 3570575889309696.0,
      "budget_used_percent": 3.570575889309696
    },
    {
      "type": "training",
      "description": "Training step 601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:26",
      "total_flops_so_far": 3576516947361792.0,
      "budget_used_percent": 3.5765169473617924
    },
    {
      "type": "training",
      "description": "Training step 602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:27",
      "total_flops_so_far": 3582458005413888.0,
      "budget_used_percent": 3.5824580054138884
    },
    {
      "type": "training",
      "description": "Training step 603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:27",
      "total_flops_so_far": 3588399063465984.0,
      "budget_used_percent": 3.5883990634659844
    },
    {
      "type": "training",
      "description": "Training step 604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:27",
      "total_flops_so_far": 3594340121518080.0,
      "budget_used_percent": 3.5943401215180795
    },
    {
      "type": "training",
      "description": "Training step 605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:28",
      "total_flops_so_far": 3600281179570176.0,
      "budget_used_percent": 3.600281179570176
    },
    {
      "type": "training",
      "description": "Training step 606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:28",
      "total_flops_so_far": 3606222237622272.0,
      "budget_used_percent": 3.606222237622272
    },
    {
      "type": "training",
      "description": "Training step 607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:28",
      "total_flops_so_far": 3612163295674368.0,
      "budget_used_percent": 3.612163295674368
    },
    {
      "type": "training",
      "description": "Training step 608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:29",
      "total_flops_so_far": 3618104353726464.0,
      "budget_used_percent": 3.618104353726464
    },
    {
      "type": "training",
      "description": "Training step 609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:29",
      "total_flops_so_far": 3624045411778560.0,
      "budget_used_percent": 3.62404541177856
    },
    {
      "type": "training",
      "description": "Training step 610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:29",
      "total_flops_so_far": 3629986469830656.0,
      "budget_used_percent": 3.629986469830656
    },
    {
      "type": "training",
      "description": "Training step 611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:29",
      "total_flops_so_far": 3635927527882752.0,
      "budget_used_percent": 3.635927527882752
    },
    {
      "type": "training",
      "description": "Training step 612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:30",
      "total_flops_so_far": 3641868585934848.0,
      "budget_used_percent": 3.641868585934848
    },
    {
      "type": "training",
      "description": "Training step 613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:30",
      "total_flops_so_far": 3647809643986944.0,
      "budget_used_percent": 3.6478096439869443
    },
    {
      "type": "training",
      "description": "Training step 614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:30",
      "total_flops_so_far": 3653750702039040.0,
      "budget_used_percent": 3.6537507020390403
    },
    {
      "type": "training",
      "description": "Training step 615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:31",
      "total_flops_so_far": 3659691760091136.0,
      "budget_used_percent": 3.6596917600911363
    },
    {
      "type": "training",
      "description": "Training step 616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:31",
      "total_flops_so_far": 3665632818143232.0,
      "budget_used_percent": 3.6656328181432323
    },
    {
      "type": "training",
      "description": "Training step 617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:31",
      "total_flops_so_far": 3671573876195328.0,
      "budget_used_percent": 3.6715738761953283
    },
    {
      "type": "training",
      "description": "Training step 618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:32",
      "total_flops_so_far": 3677514934247424.0,
      "budget_used_percent": 3.6775149342474243
    },
    {
      "type": "training",
      "description": "Training step 619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:32",
      "total_flops_so_far": 3683455992299520.0,
      "budget_used_percent": 3.68345599229952
    },
    {
      "type": "training",
      "description": "Training step 620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:32",
      "total_flops_so_far": 3689397050351616.0,
      "budget_used_percent": 3.689397050351616
    },
    {
      "type": "training",
      "description": "Training step 621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:33",
      "total_flops_so_far": 3695338108403712.0,
      "budget_used_percent": 3.695338108403712
    },
    {
      "type": "training",
      "description": "Training step 622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:33",
      "total_flops_so_far": 3701279166455808.0,
      "budget_used_percent": 3.701279166455808
    },
    {
      "type": "training",
      "description": "Training step 623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:33",
      "total_flops_so_far": 3707220224507904.0,
      "budget_used_percent": 3.707220224507904
    },
    {
      "type": "training",
      "description": "Training step 624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:34",
      "total_flops_so_far": 3713161282560000.0,
      "budget_used_percent": 3.7131612825599998
    },
    {
      "type": "training",
      "description": "Training step 625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:34",
      "total_flops_so_far": 3719102340612096.0,
      "budget_used_percent": 3.719102340612096
    },
    {
      "type": "training",
      "description": "Training step 626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:34",
      "total_flops_so_far": 3725043398664192.0,
      "budget_used_percent": 3.725043398664192
    },
    {
      "type": "training",
      "description": "Training step 627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:35",
      "total_flops_so_far": 3730984456716288.0,
      "budget_used_percent": 3.730984456716288
    },
    {
      "type": "training",
      "description": "Training step 628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:35",
      "total_flops_so_far": 3736925514768384.0,
      "budget_used_percent": 3.736925514768384
    },
    {
      "type": "training",
      "description": "Training step 629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:35",
      "total_flops_so_far": 3742866572820480.0,
      "budget_used_percent": 3.74286657282048
    },
    {
      "type": "training",
      "description": "Training step 630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:36",
      "total_flops_so_far": 3748807630872576.0,
      "budget_used_percent": 3.748807630872576
    },
    {
      "type": "training",
      "description": "Training step 631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:36",
      "total_flops_so_far": 3754748688924672.0,
      "budget_used_percent": 3.754748688924672
    },
    {
      "type": "training",
      "description": "Training step 632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:36",
      "total_flops_so_far": 3760689746976768.0,
      "budget_used_percent": 3.760689746976768
    },
    {
      "type": "training",
      "description": "Training step 633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:36",
      "total_flops_so_far": 3766630805028864.0,
      "budget_used_percent": 3.7666308050288637
    },
    {
      "type": "training",
      "description": "Training step 634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:37",
      "total_flops_so_far": 3772571863080960.0,
      "budget_used_percent": 3.7725718630809597
    },
    {
      "type": "training",
      "description": "Training step 635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:37",
      "total_flops_so_far": 3778512921133056.0,
      "budget_used_percent": 3.7785129211330557
    },
    {
      "type": "training",
      "description": "Training step 636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:37",
      "total_flops_so_far": 3784453979185152.0,
      "budget_used_percent": 3.7844539791851517
    },
    {
      "type": "training",
      "description": "Training step 637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:38",
      "total_flops_so_far": 3790395037237248.0,
      "budget_used_percent": 3.7903950372372477
    },
    {
      "type": "training",
      "description": "Training step 638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:38",
      "total_flops_so_far": 3796336095289344.0,
      "budget_used_percent": 3.796336095289344
    },
    {
      "type": "training",
      "description": "Training step 639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:38",
      "total_flops_so_far": 3802277153341440.0,
      "budget_used_percent": 3.80227715334144
    },
    {
      "type": "training",
      "description": "Training step 640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:39",
      "total_flops_so_far": 3808218211393536.0,
      "budget_used_percent": 3.808218211393536
    },
    {
      "type": "training",
      "description": "Training step 641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:39",
      "total_flops_so_far": 3814159269445632.0,
      "budget_used_percent": 3.814159269445632
    },
    {
      "type": "training",
      "description": "Training step 642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:39",
      "total_flops_so_far": 3820100327497728.0,
      "budget_used_percent": 3.820100327497728
    },
    {
      "type": "training",
      "description": "Training step 643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:40",
      "total_flops_so_far": 3826041385549824.0,
      "budget_used_percent": 3.826041385549824
    },
    {
      "type": "training",
      "description": "Training step 644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:40",
      "total_flops_so_far": 3831982443601920.0,
      "budget_used_percent": 3.83198244360192
    },
    {
      "type": "training",
      "description": "Training step 645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:40",
      "total_flops_so_far": 3837923501654016.0,
      "budget_used_percent": 3.837923501654016
    },
    {
      "type": "training",
      "description": "Training step 646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:41",
      "total_flops_so_far": 3843864559706112.0,
      "budget_used_percent": 3.8438645597061125
    },
    {
      "type": "training",
      "description": "Training step 647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:41",
      "total_flops_so_far": 3849805617758208.0,
      "budget_used_percent": 3.8498056177582076
    },
    {
      "type": "training",
      "description": "Training step 648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:41",
      "total_flops_so_far": 3855746675810304.0,
      "budget_used_percent": 3.8557466758103036
    },
    {
      "type": "training",
      "description": "Training step 649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:42",
      "total_flops_so_far": 3861687733862400.0,
      "budget_used_percent": 3.8616877338623996
    },
    {
      "type": "training",
      "description": "Training step 650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:42",
      "total_flops_so_far": 3867628791914496.0,
      "budget_used_percent": 3.8676287919144956
    },
    {
      "type": "training",
      "description": "Training step 651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:42",
      "total_flops_so_far": 3873569849966592.0,
      "budget_used_percent": 3.873569849966592
    },
    {
      "type": "training",
      "description": "Training step 652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:43",
      "total_flops_so_far": 3879510908018688.0,
      "budget_used_percent": 3.879510908018688
    },
    {
      "type": "training",
      "description": "Training step 653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:43",
      "total_flops_so_far": 3885451966070784.0,
      "budget_used_percent": 3.885451966070784
    },
    {
      "type": "training",
      "description": "Training step 654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:43",
      "total_flops_so_far": 3891393024122880.0,
      "budget_used_percent": 3.89139302412288
    },
    {
      "type": "training",
      "description": "Training step 655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:43",
      "total_flops_so_far": 3897334082174976.0,
      "budget_used_percent": 3.897334082174976
    },
    {
      "type": "training",
      "description": "Training step 656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:44",
      "total_flops_so_far": 3903275140227072.0,
      "budget_used_percent": 3.903275140227072
    },
    {
      "type": "training",
      "description": "Training step 657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:44",
      "total_flops_so_far": 3909216198279168.0,
      "budget_used_percent": 3.909216198279168
    },
    {
      "type": "training",
      "description": "Training step 658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:44",
      "total_flops_so_far": 3915157256331264.0,
      "budget_used_percent": 3.9151572563312644
    },
    {
      "type": "training",
      "description": "Training step 659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:45",
      "total_flops_so_far": 3921098314383360.0,
      "budget_used_percent": 3.9210983143833604
    },
    {
      "type": "training",
      "description": "Training step 660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:45",
      "total_flops_so_far": 3927039372435456.0,
      "budget_used_percent": 3.9270393724354564
    },
    {
      "type": "training",
      "description": "Training step 661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:45",
      "total_flops_so_far": 3932980430487552.0,
      "budget_used_percent": 3.9329804304875515
    },
    {
      "type": "training",
      "description": "Training step 662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:46",
      "total_flops_so_far": 3938921488539648.0,
      "budget_used_percent": 3.9389214885396475
    },
    {
      "type": "training",
      "description": "Training step 663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:46",
      "total_flops_so_far": 3944862546591744.0,
      "budget_used_percent": 3.944862546591744
    },
    {
      "type": "training",
      "description": "Training step 664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:46",
      "total_flops_so_far": 3950803604643840.0,
      "budget_used_percent": 3.95080360464384
    },
    {
      "type": "training",
      "description": "Training step 665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:47",
      "total_flops_so_far": 3956744662695936.0,
      "budget_used_percent": 3.956744662695936
    },
    {
      "type": "training",
      "description": "Training step 666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:47",
      "total_flops_so_far": 3962685720748032.0,
      "budget_used_percent": 3.962685720748032
    },
    {
      "type": "training",
      "description": "Training step 667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:47",
      "total_flops_so_far": 3968626778800128.0,
      "budget_used_percent": 3.968626778800128
    },
    {
      "type": "training",
      "description": "Training step 668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:48",
      "total_flops_so_far": 3974567836852224.0,
      "budget_used_percent": 3.974567836852224
    },
    {
      "type": "training",
      "description": "Training step 669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:48",
      "total_flops_so_far": 3980508894904320.0,
      "budget_used_percent": 3.98050889490432
    },
    {
      "type": "training",
      "description": "Training step 670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:48",
      "total_flops_so_far": 3986449952956416.0,
      "budget_used_percent": 3.986449952956416
    },
    {
      "type": "training",
      "description": "Training step 671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:49",
      "total_flops_so_far": 3992391011008512.0,
      "budget_used_percent": 3.9923910110085123
    },
    {
      "type": "training",
      "description": "Training step 672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:49",
      "total_flops_so_far": 3998332069060608.0,
      "budget_used_percent": 3.9983320690606083
    },
    {
      "type": "training",
      "description": "Training step 673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:49",
      "total_flops_so_far": 4004273127112704.0,
      "budget_used_percent": 4.004273127112704
    },
    {
      "type": "training",
      "description": "Training step 674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:50",
      "total_flops_so_far": 4010214185164800.0,
      "budget_used_percent": 4.0102141851648
    },
    {
      "type": "training",
      "description": "Training step 675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:50",
      "total_flops_so_far": 4016155243216896.0,
      "budget_used_percent": 4.016155243216896
    },
    {
      "type": "training",
      "description": "Training step 676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:50",
      "total_flops_so_far": 4022096301268992.0,
      "budget_used_percent": 4.022096301268991
    },
    {
      "type": "training",
      "description": "Training step 677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:50",
      "total_flops_so_far": 4028037359321088.0,
      "budget_used_percent": 4.028037359321088
    },
    {
      "type": "training",
      "description": "Training step 678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:51",
      "total_flops_so_far": 4033978417373184.0,
      "budget_used_percent": 4.033978417373183
    },
    {
      "type": "training",
      "description": "Training step 679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:51",
      "total_flops_so_far": 4039919475425280.0,
      "budget_used_percent": 4.03991947542528
    },
    {
      "type": "training",
      "description": "Training step 680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:51",
      "total_flops_so_far": 4045860533477376.0,
      "budget_used_percent": 4.045860533477376
    },
    {
      "type": "training",
      "description": "Training step 681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:52",
      "total_flops_so_far": 4051801591529472.0,
      "budget_used_percent": 4.051801591529472
    },
    {
      "type": "training",
      "description": "Training step 682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:52",
      "total_flops_so_far": 4057742649581568.0,
      "budget_used_percent": 4.057742649581568
    },
    {
      "type": "training",
      "description": "Training step 683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:52",
      "total_flops_so_far": 4063683707633664.0,
      "budget_used_percent": 4.063683707633664
    },
    {
      "type": "training",
      "description": "Training step 684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:53",
      "total_flops_so_far": 4069624765685760.0,
      "budget_used_percent": 4.06962476568576
    },
    {
      "type": "training",
      "description": "Training step 685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:53",
      "total_flops_so_far": 4075565823737856.0,
      "budget_used_percent": 4.075565823737856
    },
    {
      "type": "training",
      "description": "Training step 686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:53",
      "total_flops_so_far": 4081506881789952.0,
      "budget_used_percent": 4.081506881789952
    },
    {
      "type": "training",
      "description": "Training step 687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:54",
      "total_flops_so_far": 4087447939842048.0,
      "budget_used_percent": 4.087447939842049
    },
    {
      "type": "training",
      "description": "Training step 688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:54",
      "total_flops_so_far": 4093388997894144.0,
      "budget_used_percent": 4.093388997894144
    },
    {
      "type": "training",
      "description": "Training step 689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:54",
      "total_flops_so_far": 4099330055946240.0,
      "budget_used_percent": 4.099330055946241
    },
    {
      "type": "training",
      "description": "Training step 690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:55",
      "total_flops_so_far": 4105271113998336.0,
      "budget_used_percent": 4.105271113998335
    },
    {
      "type": "training",
      "description": "Training step 691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:55",
      "total_flops_so_far": 4111212172050432.0,
      "budget_used_percent": 4.111212172050432
    },
    {
      "type": "training",
      "description": "Training step 692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:55",
      "total_flops_so_far": 4117153230102528.0,
      "budget_used_percent": 4.117153230102528
    },
    {
      "type": "training",
      "description": "Training step 693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:56",
      "total_flops_so_far": 4123094288154624.0,
      "budget_used_percent": 4.123094288154624
    },
    {
      "type": "training",
      "description": "Training step 694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:56",
      "total_flops_so_far": 4129035346206720.0,
      "budget_used_percent": 4.12903534620672
    },
    {
      "type": "training",
      "description": "Training step 695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:56",
      "total_flops_so_far": 4134976404258816.0,
      "budget_used_percent": 4.134976404258816
    },
    {
      "type": "training",
      "description": "Training step 696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:57",
      "total_flops_so_far": 4140917462310912.0,
      "budget_used_percent": 4.140917462310912
    },
    {
      "type": "training",
      "description": "Training step 697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:57",
      "total_flops_so_far": 4146858520363008.0,
      "budget_used_percent": 4.146858520363008
    },
    {
      "type": "training",
      "description": "Training step 698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:57",
      "total_flops_so_far": 4152799578415104.0,
      "budget_used_percent": 4.152799578415104
    },
    {
      "type": "training",
      "description": "Training step 699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:57",
      "total_flops_so_far": 4158740636467200.0,
      "budget_used_percent": 4.1587406364672
    },
    {
      "type": "training",
      "description": "Training step 700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:58",
      "total_flops_so_far": 4164681694519296.0,
      "budget_used_percent": 4.164681694519296
    },
    {
      "type": "training",
      "description": "Training step 701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:58",
      "total_flops_so_far": 4170622752571392.0,
      "budget_used_percent": 4.170622752571393
    },
    {
      "type": "training",
      "description": "Training step 702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:58",
      "total_flops_so_far": 4176563810623488.0,
      "budget_used_percent": 4.176563810623488
    },
    {
      "type": "training",
      "description": "Training step 703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:59",
      "total_flops_so_far": 4182504868675584.0,
      "budget_used_percent": 4.182504868675585
    },
    {
      "type": "training",
      "description": "Training step 704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:59",
      "total_flops_so_far": 4188445926727680.0,
      "budget_used_percent": 4.188445926727679
    },
    {
      "type": "training",
      "description": "Training step 705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:23:59",
      "total_flops_so_far": 4194386984779776.0,
      "budget_used_percent": 4.194386984779776
    },
    {
      "type": "training",
      "description": "Training step 706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:00",
      "total_flops_so_far": 4200328042831872.0,
      "budget_used_percent": 4.200328042831872
    },
    {
      "type": "training",
      "description": "Training step 707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:00",
      "total_flops_so_far": 4206269100883968.0,
      "budget_used_percent": 4.206269100883968
    },
    {
      "type": "training",
      "description": "Training step 708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:00",
      "total_flops_so_far": 4212210158936064.0,
      "budget_used_percent": 4.212210158936064
    },
    {
      "type": "training",
      "description": "Training step 709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:01",
      "total_flops_so_far": 4218151216988160.0,
      "budget_used_percent": 4.21815121698816
    },
    {
      "type": "training",
      "description": "Training step 710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:01",
      "total_flops_so_far": 4224092275040256.0,
      "budget_used_percent": 4.224092275040256
    },
    {
      "type": "training",
      "description": "Training step 711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:01",
      "total_flops_so_far": 4230033333092352.0,
      "budget_used_percent": 4.230033333092352
    },
    {
      "type": "training",
      "description": "Training step 712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:02",
      "total_flops_so_far": 4235974391144448.0,
      "budget_used_percent": 4.235974391144448
    },
    {
      "type": "training",
      "description": "Training step 713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:02",
      "total_flops_so_far": 4241915449196544.0,
      "budget_used_percent": 4.2419154491965445
    },
    {
      "type": "training",
      "description": "Training step 714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:02",
      "total_flops_so_far": 4247856507248640.0,
      "budget_used_percent": 4.24785650724864
    },
    {
      "type": "training",
      "description": "Training step 715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:02",
      "total_flops_so_far": 4253797565300736.0,
      "budget_used_percent": 4.2537975653007365
    },
    {
      "type": "training",
      "description": "Training step 716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:03",
      "total_flops_so_far": 4259738623352832.0,
      "budget_used_percent": 4.259738623352832
    },
    {
      "type": "training",
      "description": "Training step 717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:03",
      "total_flops_so_far": 4265679681404928.0,
      "budget_used_percent": 4.2656796814049285
    },
    {
      "type": "training",
      "description": "Training step 718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:03",
      "total_flops_so_far": 4271620739457024.0,
      "budget_used_percent": 4.271620739457024
    },
    {
      "type": "training",
      "description": "Training step 719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:04",
      "total_flops_so_far": 4277561797509120.0,
      "budget_used_percent": 4.27756179750912
    },
    {
      "type": "training",
      "description": "Training step 720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:04",
      "total_flops_so_far": 4283502855561216.0,
      "budget_used_percent": 4.283502855561216
    },
    {
      "type": "training",
      "description": "Training step 721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:04",
      "total_flops_so_far": 4289443913613312.0,
      "budget_used_percent": 4.289443913613312
    },
    {
      "type": "training",
      "description": "Training step 722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:05",
      "total_flops_so_far": 4295384971665408.0,
      "budget_used_percent": 4.295384971665408
    },
    {
      "type": "training",
      "description": "Training step 723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:05",
      "total_flops_so_far": 4301326029717504.0,
      "budget_used_percent": 4.3013260297175036
    },
    {
      "type": "training",
      "description": "Training step 724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:05",
      "total_flops_so_far": 4307267087769600.0,
      "budget_used_percent": 4.3072670877696
    },
    {
      "type": "training",
      "description": "Training step 725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:06",
      "total_flops_so_far": 4313208145821696.0,
      "budget_used_percent": 4.313208145821696
    },
    {
      "type": "training",
      "description": "Training step 726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:06",
      "total_flops_so_far": 4319149203873792.0,
      "budget_used_percent": 4.319149203873792
    },
    {
      "type": "training",
      "description": "Training step 727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:06",
      "total_flops_so_far": 4325090261925888.0,
      "budget_used_percent": 4.325090261925888
    },
    {
      "type": "training",
      "description": "Training step 728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:07",
      "total_flops_so_far": 4331031319977984.0,
      "budget_used_percent": 4.331031319977984
    },
    {
      "type": "training",
      "description": "Training step 729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:07",
      "total_flops_so_far": 4336972378030080.0,
      "budget_used_percent": 4.33697237803008
    },
    {
      "type": "training",
      "description": "Training step 730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:07",
      "total_flops_so_far": 4342913436082176.0,
      "budget_used_percent": 4.342913436082176
    },
    {
      "type": "training",
      "description": "Training step 731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:08",
      "total_flops_so_far": 4348854494134272.0,
      "budget_used_percent": 4.348854494134272
    },
    {
      "type": "training",
      "description": "Training step 732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:08",
      "total_flops_so_far": 4354795552186368.0,
      "budget_used_percent": 4.354795552186368
    },
    {
      "type": "training",
      "description": "Training step 733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:08",
      "total_flops_so_far": 4360736610238464.0,
      "budget_used_percent": 4.3607366102384635
    },
    {
      "type": "training",
      "description": "Training step 734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:09",
      "total_flops_so_far": 4366677668290560.0,
      "budget_used_percent": 4.36667766829056
    },
    {
      "type": "training",
      "description": "Training step 735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:09",
      "total_flops_so_far": 4372618726342656.0,
      "budget_used_percent": 4.3726187263426555
    },
    {
      "type": "training",
      "description": "Training step 736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:09",
      "total_flops_so_far": 4378559784394752.0,
      "budget_used_percent": 4.378559784394752
    },
    {
      "type": "training",
      "description": "Training step 737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:10",
      "total_flops_so_far": 4384500842446848.0,
      "budget_used_percent": 4.3845008424468475
    },
    {
      "type": "training",
      "description": "Training step 738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:10",
      "total_flops_so_far": 4390441900498944.0,
      "budget_used_percent": 4.390441900498944
    },
    {
      "type": "training",
      "description": "Training step 739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:10",
      "total_flops_so_far": 4396382958551040.0,
      "budget_used_percent": 4.39638295855104
    },
    {
      "type": "training",
      "description": "Training step 740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:10",
      "total_flops_so_far": 4402324016603136.0,
      "budget_used_percent": 4.402324016603136
    },
    {
      "type": "training",
      "description": "Training step 741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:11",
      "total_flops_so_far": 4408265074655232.0,
      "budget_used_percent": 4.408265074655232
    },
    {
      "type": "training",
      "description": "Training step 742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:11",
      "total_flops_so_far": 4414206132707328.0,
      "budget_used_percent": 4.414206132707328
    },
    {
      "type": "training",
      "description": "Training step 743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:11",
      "total_flops_so_far": 4420147190759424.0,
      "budget_used_percent": 4.420147190759424
    },
    {
      "type": "training",
      "description": "Training step 744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:12",
      "total_flops_so_far": 4426088248811520.0,
      "budget_used_percent": 4.42608824881152
    },
    {
      "type": "training",
      "description": "Training step 745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:12",
      "total_flops_so_far": 4432029306863616.0,
      "budget_used_percent": 4.432029306863616
    },
    {
      "type": "training",
      "description": "Training step 746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:12",
      "total_flops_so_far": 4437970364915712.0,
      "budget_used_percent": 4.437970364915713
    },
    {
      "type": "training",
      "description": "Training step 747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:13",
      "total_flops_so_far": 4443911422967808.0,
      "budget_used_percent": 4.443911422967807
    },
    {
      "type": "training",
      "description": "Training step 748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:13",
      "total_flops_so_far": 4449852481019904.0,
      "budget_used_percent": 4.449852481019904
    },
    {
      "type": "training",
      "description": "Training step 749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:13",
      "total_flops_so_far": 4455793539072000.0,
      "budget_used_percent": 4.455793539071999
    },
    {
      "type": "training",
      "description": "Training step 750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:14",
      "total_flops_so_far": 4461734597124096.0,
      "budget_used_percent": 4.461734597124096
    },
    {
      "type": "training",
      "description": "Training step 751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:14",
      "total_flops_so_far": 4467675655176192.0,
      "budget_used_percent": 4.467675655176192
    },
    {
      "type": "training",
      "description": "Training step 752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:14",
      "total_flops_so_far": 4473616713228288.0,
      "budget_used_percent": 4.473616713228288
    },
    {
      "type": "training",
      "description": "Training step 753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:15",
      "total_flops_so_far": 4479557771280384.0,
      "budget_used_percent": 4.479557771280384
    },
    {
      "type": "training",
      "description": "Training step 754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:15",
      "total_flops_so_far": 4485498829332480.0,
      "budget_used_percent": 4.48549882933248
    },
    {
      "type": "training",
      "description": "Training step 755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:15",
      "total_flops_so_far": 4491439887384576.0,
      "budget_used_percent": 4.491439887384576
    },
    {
      "type": "training",
      "description": "Training step 756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:16",
      "total_flops_so_far": 4497380945436672.0,
      "budget_used_percent": 4.497380945436672
    },
    {
      "type": "training",
      "description": "Training step 757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:16",
      "total_flops_so_far": 4503322003488768.0,
      "budget_used_percent": 4.503322003488768
    },
    {
      "type": "training",
      "description": "Training step 758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:16",
      "total_flops_so_far": 4509263061540864.0,
      "budget_used_percent": 4.509263061540865
    },
    {
      "type": "training",
      "description": "Training step 759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:17",
      "total_flops_so_far": 4515204119592960.0,
      "budget_used_percent": 4.51520411959296
    },
    {
      "type": "training",
      "description": "Training step 760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:17",
      "total_flops_so_far": 4521145177645056.0,
      "budget_used_percent": 4.521145177645057
    },
    {
      "type": "training",
      "description": "Training step 761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:17",
      "total_flops_so_far": 4527086235697152.0,
      "budget_used_percent": 4.527086235697151
    },
    {
      "type": "training",
      "description": "Training step 762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:18",
      "total_flops_so_far": 4533027293749248.0,
      "budget_used_percent": 4.533027293749248
    },
    {
      "type": "training",
      "description": "Training step 763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:18",
      "total_flops_so_far": 4538968351801344.0,
      "budget_used_percent": 4.538968351801344
    },
    {
      "type": "training",
      "description": "Training step 764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:18",
      "total_flops_so_far": 4544909409853440.0,
      "budget_used_percent": 4.54490940985344
    },
    {
      "type": "training",
      "description": "Training step 765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:18",
      "total_flops_so_far": 4550850467905536.0,
      "budget_used_percent": 4.550850467905536
    },
    {
      "type": "training",
      "description": "Training step 766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:19",
      "total_flops_so_far": 4556791525957632.0,
      "budget_used_percent": 4.556791525957632
    },
    {
      "type": "training",
      "description": "Training step 767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:19",
      "total_flops_so_far": 4562732584009728.0,
      "budget_used_percent": 4.562732584009728
    },
    {
      "type": "training",
      "description": "Training step 768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:19",
      "total_flops_so_far": 4568673642061824.0,
      "budget_used_percent": 4.568673642061824
    },
    {
      "type": "training",
      "description": "Training step 769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:20",
      "total_flops_so_far": 4574614700113920.0,
      "budget_used_percent": 4.57461470011392
    },
    {
      "type": "training",
      "description": "Training step 770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:20",
      "total_flops_so_far": 4580555758166016.0,
      "budget_used_percent": 4.580555758166016
    },
    {
      "type": "training",
      "description": "Training step 771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:20",
      "total_flops_so_far": 4586496816218112.0,
      "budget_used_percent": 4.586496816218112
    },
    {
      "type": "training",
      "description": "Training step 772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:21",
      "total_flops_so_far": 4592437874270208.0,
      "budget_used_percent": 4.5924378742702086
    },
    {
      "type": "training",
      "description": "Training step 773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:21",
      "total_flops_so_far": 4598378932322304.0,
      "budget_used_percent": 4.598378932322304
    },
    {
      "type": "training",
      "description": "Training step 774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:21",
      "total_flops_so_far": 4604319990374400.0,
      "budget_used_percent": 4.6043199903744005
    },
    {
      "type": "training",
      "description": "Training step 775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:22",
      "total_flops_so_far": 4610261048426496.0,
      "budget_used_percent": 4.610261048426496
    },
    {
      "type": "training",
      "description": "Training step 776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:22",
      "total_flops_so_far": 4616202106478592.0,
      "budget_used_percent": 4.616202106478592
    },
    {
      "type": "training",
      "description": "Training step 777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:22",
      "total_flops_so_far": 4622143164530688.0,
      "budget_used_percent": 4.622143164530688
    },
    {
      "type": "training",
      "description": "Training step 778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:23",
      "total_flops_so_far": 4628084222582784.0,
      "budget_used_percent": 4.628084222582784
    },
    {
      "type": "training",
      "description": "Training step 779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:23",
      "total_flops_so_far": 4634025280634880.0,
      "budget_used_percent": 4.63402528063488
    },
    {
      "type": "training",
      "description": "Training step 780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:23",
      "total_flops_so_far": 4639966338686976.0,
      "budget_used_percent": 4.639966338686976
    },
    {
      "type": "training",
      "description": "Training step 781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:24",
      "total_flops_so_far": 4645907396739072.0,
      "budget_used_percent": 4.645907396739072
    },
    {
      "type": "training",
      "description": "Training step 782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:24",
      "total_flops_so_far": 4651848454791168.0,
      "budget_used_percent": 4.651848454791168
    },
    {
      "type": "training",
      "description": "Training step 783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:24",
      "total_flops_so_far": 4657789512843264.0,
      "budget_used_percent": 4.657789512843264
    },
    {
      "type": "training",
      "description": "Training step 784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:25",
      "total_flops_so_far": 4663730570895360.0,
      "budget_used_percent": 4.6637305708953605
    },
    {
      "type": "training",
      "description": "Training step 785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:25",
      "total_flops_so_far": 4669671628947456.0,
      "budget_used_percent": 4.669671628947456
    },
    {
      "type": "training",
      "description": "Training step 786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:25",
      "total_flops_so_far": 4675612686999552.0,
      "budget_used_percent": 4.6756126869995525
    },
    {
      "type": "training",
      "description": "Training step 787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:26",
      "total_flops_so_far": 4681553745051648.0,
      "budget_used_percent": 4.681553745051648
    },
    {
      "type": "training",
      "description": "Training step 788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:26",
      "total_flops_so_far": 4687494803103744.0,
      "budget_used_percent": 4.6874948031037444
    },
    {
      "type": "training",
      "description": "Training step 789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:26",
      "total_flops_so_far": 4693435861155840.0,
      "budget_used_percent": 4.69343586115584
    },
    {
      "type": "training",
      "description": "Training step 790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:27",
      "total_flops_so_far": 4699376919207936.0,
      "budget_used_percent": 4.6993769192079355
    },
    {
      "type": "training",
      "description": "Training step 791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:27",
      "total_flops_so_far": 4705317977260032.0,
      "budget_used_percent": 4.705317977260032
    },
    {
      "type": "training",
      "description": "Training step 792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:27",
      "total_flops_so_far": 4711259035312128.0,
      "budget_used_percent": 4.7112590353121275
    },
    {
      "type": "training",
      "description": "Training step 793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:27",
      "total_flops_so_far": 4717200093364224.0,
      "budget_used_percent": 4.717200093364224
    },
    {
      "type": "training",
      "description": "Training step 794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:28",
      "total_flops_so_far": 4723141151416320.0,
      "budget_used_percent": 4.7231411514163195
    },
    {
      "type": "training",
      "description": "Training step 795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:28",
      "total_flops_so_far": 4729082209468416.0,
      "budget_used_percent": 4.729082209468416
    },
    {
      "type": "training",
      "description": "Training step 796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:28",
      "total_flops_so_far": 4735023267520512.0,
      "budget_used_percent": 4.735023267520512
    },
    {
      "type": "training",
      "description": "Training step 797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:29",
      "total_flops_so_far": 4740964325572608.0,
      "budget_used_percent": 4.740964325572608
    },
    {
      "type": "training",
      "description": "Training step 798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:29",
      "total_flops_so_far": 4746905383624704.0,
      "budget_used_percent": 4.746905383624704
    },
    {
      "type": "training",
      "description": "Training step 799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:29",
      "total_flops_so_far": 4752846441676800.0,
      "budget_used_percent": 4.7528464416768
    },
    {
      "type": "training",
      "description": "Training step 800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:30",
      "total_flops_so_far": 4758787499728896.0,
      "budget_used_percent": 4.758787499728896
    },
    {
      "type": "training",
      "description": "Training step 801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:30",
      "total_flops_so_far": 4764728557780992.0,
      "budget_used_percent": 4.764728557780992
    },
    {
      "type": "training",
      "description": "Training step 802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:30",
      "total_flops_so_far": 4770669615833088.0,
      "budget_used_percent": 4.770669615833088
    },
    {
      "type": "training",
      "description": "Training step 803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:31",
      "total_flops_so_far": 4776610673885184.0,
      "budget_used_percent": 4.776610673885184
    },
    {
      "type": "training",
      "description": "Training step 804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:31",
      "total_flops_so_far": 4782551731937280.0,
      "budget_used_percent": 4.7825517319372794
    },
    {
      "type": "training",
      "description": "Training step 805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:31",
      "total_flops_so_far": 4788492789989376.0,
      "budget_used_percent": 4.788492789989376
    },
    {
      "type": "training",
      "description": "Training step 806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:32",
      "total_flops_so_far": 4794433848041472.0,
      "budget_used_percent": 4.794433848041471
    },
    {
      "type": "training",
      "description": "Training step 807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:32",
      "total_flops_so_far": 4800374906093568.0,
      "budget_used_percent": 4.800374906093568
    },
    {
      "type": "training",
      "description": "Training step 808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:32",
      "total_flops_so_far": 4806315964145664.0,
      "budget_used_percent": 4.806315964145664
    },
    {
      "type": "training",
      "description": "Training step 809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:33",
      "total_flops_so_far": 4812257022197760.0,
      "budget_used_percent": 4.81225702219776
    },
    {
      "type": "training",
      "description": "Training step 810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:33",
      "total_flops_so_far": 4818198080249856.0,
      "budget_used_percent": 4.818198080249856
    },
    {
      "type": "training",
      "description": "Training step 811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:33",
      "total_flops_so_far": 4824139138301952.0,
      "budget_used_percent": 4.824139138301952
    },
    {
      "type": "training",
      "description": "Training step 812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:34",
      "total_flops_so_far": 4830080196354048.0,
      "budget_used_percent": 4.830080196354048
    },
    {
      "type": "training",
      "description": "Training step 813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:34",
      "total_flops_so_far": 4836021254406144.0,
      "budget_used_percent": 4.836021254406144
    },
    {
      "type": "training",
      "description": "Training step 814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:34",
      "total_flops_so_far": 4841962312458240.0,
      "budget_used_percent": 4.84196231245824
    },
    {
      "type": "training",
      "description": "Training step 815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:35",
      "total_flops_so_far": 4847903370510336.0,
      "budget_used_percent": 4.847903370510336
    },
    {
      "type": "training",
      "description": "Training step 816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:35",
      "total_flops_so_far": 4853844428562432.0,
      "budget_used_percent": 4.853844428562432
    },
    {
      "type": "training",
      "description": "Training step 817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:35",
      "total_flops_so_far": 4859785486614528.0,
      "budget_used_percent": 4.859785486614529
    },
    {
      "type": "training",
      "description": "Training step 818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:36",
      "total_flops_so_far": 4865726544666624.0,
      "budget_used_percent": 4.865726544666623
    },
    {
      "type": "training",
      "description": "Training step 819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:36",
      "total_flops_so_far": 4871667602718720.0,
      "budget_used_percent": 4.87166760271872
    },
    {
      "type": "training",
      "description": "Training step 820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:36",
      "total_flops_so_far": 4877608660770816.0,
      "budget_used_percent": 4.877608660770816
    },
    {
      "type": "training",
      "description": "Training step 821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:36",
      "total_flops_so_far": 4883549718822912.0,
      "budget_used_percent": 4.883549718822912
    },
    {
      "type": "training",
      "description": "Training step 822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:37",
      "total_flops_so_far": 4889490776875008.0,
      "budget_used_percent": 4.889490776875008
    },
    {
      "type": "training",
      "description": "Training step 823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:37",
      "total_flops_so_far": 4895431834927104.0,
      "budget_used_percent": 4.895431834927104
    },
    {
      "type": "training",
      "description": "Training step 824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:37",
      "total_flops_so_far": 4901372892979200.0,
      "budget_used_percent": 4.9013728929792
    },
    {
      "type": "training",
      "description": "Training step 825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:38",
      "total_flops_so_far": 4907313951031296.0,
      "budget_used_percent": 4.907313951031296
    },
    {
      "type": "training",
      "description": "Training step 826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:38",
      "total_flops_so_far": 4913255009083392.0,
      "budget_used_percent": 4.913255009083392
    },
    {
      "type": "training",
      "description": "Training step 827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:38",
      "total_flops_so_far": 4919196067135488.0,
      "budget_used_percent": 4.919196067135488
    },
    {
      "type": "training",
      "description": "Training step 828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:39",
      "total_flops_so_far": 4925137125187584.0,
      "budget_used_percent": 4.925137125187584
    },
    {
      "type": "training",
      "description": "Training step 829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:39",
      "total_flops_so_far": 4931078183239680.0,
      "budget_used_percent": 4.931078183239681
    },
    {
      "type": "training",
      "description": "Training step 830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:39",
      "total_flops_so_far": 4937019241291776.0,
      "budget_used_percent": 4.937019241291776
    },
    {
      "type": "training",
      "description": "Training step 831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:40",
      "total_flops_so_far": 4942960299343872.0,
      "budget_used_percent": 4.942960299343873
    },
    {
      "type": "training",
      "description": "Training step 832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:40",
      "total_flops_so_far": 4948901357395968.0,
      "budget_used_percent": 4.948901357395967
    },
    {
      "type": "training",
      "description": "Training step 833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:40",
      "total_flops_so_far": 4954842415448064.0,
      "budget_used_percent": 4.954842415448064
    },
    {
      "type": "training",
      "description": "Training step 834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:41",
      "total_flops_so_far": 4960783473500160.0,
      "budget_used_percent": 4.96078347350016
    },
    {
      "type": "training",
      "description": "Training step 835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:41",
      "total_flops_so_far": 4966724531552256.0,
      "budget_used_percent": 4.966724531552256
    },
    {
      "type": "training",
      "description": "Training step 836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:41",
      "total_flops_so_far": 4972665589604352.0,
      "budget_used_percent": 4.972665589604352
    },
    {
      "type": "training",
      "description": "Training step 837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:42",
      "total_flops_so_far": 4978606647656448.0,
      "budget_used_percent": 4.978606647656448
    },
    {
      "type": "training",
      "description": "Training step 838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:42",
      "total_flops_so_far": 4984547705708544.0,
      "budget_used_percent": 4.984547705708544
    },
    {
      "type": "training",
      "description": "Training step 839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:42",
      "total_flops_so_far": 4990488763760640.0,
      "budget_used_percent": 4.99048876376064
    },
    {
      "type": "training",
      "description": "Training step 840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:43",
      "total_flops_so_far": 4996429821812736.0,
      "budget_used_percent": 4.996429821812736
    },
    {
      "type": "training",
      "description": "Training step 841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:43",
      "total_flops_so_far": 5002370879864832.0,
      "budget_used_percent": 5.0023708798648325
    },
    {
      "type": "training",
      "description": "Training step 842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:43",
      "total_flops_so_far": 5008311937916928.0,
      "budget_used_percent": 5.008311937916928
    },
    {
      "type": "training",
      "description": "Training step 843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:44",
      "total_flops_so_far": 5014252995969024.0,
      "budget_used_percent": 5.0142529959690245
    },
    {
      "type": "training",
      "description": "Training step 844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:44",
      "total_flops_so_far": 5020194054021120.0,
      "budget_used_percent": 5.02019405402112
    },
    {
      "type": "training",
      "description": "Training step 845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:44",
      "total_flops_so_far": 5026135112073216.0,
      "budget_used_percent": 5.0261351120732165
    },
    {
      "type": "training",
      "description": "Training step 846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:45",
      "total_flops_so_far": 5032076170125312.0,
      "budget_used_percent": 5.032076170125312
    },
    {
      "type": "training",
      "description": "Training step 847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:45",
      "total_flops_so_far": 5038017228177408.0,
      "budget_used_percent": 5.038017228177408
    },
    {
      "type": "training",
      "description": "Training step 848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:45",
      "total_flops_so_far": 5043958286229504.0,
      "budget_used_percent": 5.043958286229504
    },
    {
      "type": "training",
      "description": "Training step 849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:46",
      "total_flops_so_far": 5049899344281600.0,
      "budget_used_percent": 5.0498993442816
    },
    {
      "type": "training",
      "description": "Training step 850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:46",
      "total_flops_so_far": 5055840402333696.0,
      "budget_used_percent": 5.055840402333696
    },
    {
      "type": "training",
      "description": "Training step 851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:46",
      "total_flops_so_far": 5061781460385792.0,
      "budget_used_percent": 5.061781460385792
    },
    {
      "type": "training",
      "description": "Training step 852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:46",
      "total_flops_so_far": 5067722518437888.0,
      "budget_used_percent": 5.067722518437888
    },
    {
      "type": "training",
      "description": "Training step 853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:47",
      "total_flops_so_far": 5073663576489984.0,
      "budget_used_percent": 5.0736635764899845
    },
    {
      "type": "training",
      "description": "Training step 854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:47",
      "total_flops_so_far": 5079604634542080.0,
      "budget_used_percent": 5.07960463454208
    },
    {
      "type": "training",
      "description": "Training step 855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:47",
      "total_flops_so_far": 5085545692594176.0,
      "budget_used_percent": 5.085545692594176
    },
    {
      "type": "training",
      "description": "Training step 856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:48",
      "total_flops_so_far": 5091486750646272.0,
      "budget_used_percent": 5.091486750646272
    },
    {
      "type": "training",
      "description": "Training step 857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:48",
      "total_flops_so_far": 5097427808698368.0,
      "budget_used_percent": 5.097427808698368
    },
    {
      "type": "training",
      "description": "Training step 858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:48",
      "total_flops_so_far": 5103368866750464.0,
      "budget_used_percent": 5.103368866750464
    },
    {
      "type": "training",
      "description": "Training step 859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:49",
      "total_flops_so_far": 5109309924802560.0,
      "budget_used_percent": 5.10930992480256
    },
    {
      "type": "training",
      "description": "Training step 860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:49",
      "total_flops_so_far": 5115250982854656.0,
      "budget_used_percent": 5.115250982854656
    },
    {
      "type": "training",
      "description": "Training step 861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:49",
      "total_flops_so_far": 5121192040906752.0,
      "budget_used_percent": 5.1211920409067515
    },
    {
      "type": "training",
      "description": "Training step 862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:50",
      "total_flops_so_far": 5127133098958848.0,
      "budget_used_percent": 5.127133098958848
    },
    {
      "type": "training",
      "description": "Training step 863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:50",
      "total_flops_so_far": 5133074157010944.0,
      "budget_used_percent": 5.1330741570109435
    },
    {
      "type": "training",
      "description": "Training step 864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:50",
      "total_flops_so_far": 5139015215063040.0,
      "budget_used_percent": 5.13901521506304
    },
    {
      "type": "training",
      "description": "Training step 865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:51",
      "total_flops_so_far": 5144956273115136.0,
      "budget_used_percent": 5.1449562731151355
    },
    {
      "type": "training",
      "description": "Training step 866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:51",
      "total_flops_so_far": 5150897331167232.0,
      "budget_used_percent": 5.150897331167232
    },
    {
      "type": "training",
      "description": "Training step 867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:51",
      "total_flops_so_far": 5156838389219328.0,
      "budget_used_percent": 5.156838389219328
    },
    {
      "type": "training",
      "description": "Training step 868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:52",
      "total_flops_so_far": 5162779447271424.0,
      "budget_used_percent": 5.162779447271424
    },
    {
      "type": "training",
      "description": "Training step 869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:52",
      "total_flops_so_far": 5168720505323520.0,
      "budget_used_percent": 5.16872050532352
    },
    {
      "type": "training",
      "description": "Training step 870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:52",
      "total_flops_so_far": 5174661563375616.0,
      "budget_used_percent": 5.174661563375616
    },
    {
      "type": "training",
      "description": "Training step 871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:53",
      "total_flops_so_far": 5180602621427712.0,
      "budget_used_percent": 5.180602621427712
    },
    {
      "type": "training",
      "description": "Training step 872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:53",
      "total_flops_so_far": 5186543679479808.0,
      "budget_used_percent": 5.186543679479808
    },
    {
      "type": "training",
      "description": "Training step 873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:53",
      "total_flops_so_far": 5192484737531904.0,
      "budget_used_percent": 5.192484737531904
    },
    {
      "type": "training",
      "description": "Training step 874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:54",
      "total_flops_so_far": 5198425795584000.0,
      "budget_used_percent": 5.198425795584001
    },
    {
      "type": "training",
      "description": "Training step 875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:54",
      "total_flops_so_far": 5204366853636096.0,
      "budget_used_percent": 5.204366853636095
    },
    {
      "type": "training",
      "description": "Training step 876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:54",
      "total_flops_so_far": 5210307911688192.0,
      "budget_used_percent": 5.210307911688192
    },
    {
      "type": "training",
      "description": "Training step 877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:55",
      "total_flops_so_far": 5216248969740288.0,
      "budget_used_percent": 5.216248969740287
    },
    {
      "type": "training",
      "description": "Training step 878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:55",
      "total_flops_so_far": 5222190027792384.0,
      "budget_used_percent": 5.222190027792384
    },
    {
      "type": "training",
      "description": "Training step 879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:55",
      "total_flops_so_far": 5228131085844480.0,
      "budget_used_percent": 5.22813108584448
    },
    {
      "type": "training",
      "description": "Training step 880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:56",
      "total_flops_so_far": 5234072143896576.0,
      "budget_used_percent": 5.234072143896576
    },
    {
      "type": "training",
      "description": "Training step 881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:56",
      "total_flops_so_far": 5240013201948672.0,
      "budget_used_percent": 5.240013201948672
    },
    {
      "type": "training",
      "description": "Training step 882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:56",
      "total_flops_so_far": 5245954260000768.0,
      "budget_used_percent": 5.245954260000768
    },
    {
      "type": "training",
      "description": "Training step 883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:57",
      "total_flops_so_far": 5251895318052864.0,
      "budget_used_percent": 5.251895318052864
    },
    {
      "type": "training",
      "description": "Training step 884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:57",
      "total_flops_so_far": 5257836376104960.0,
      "budget_used_percent": 5.25783637610496
    },
    {
      "type": "training",
      "description": "Training step 885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:57",
      "total_flops_so_far": 5263777434157056.0,
      "budget_used_percent": 5.263777434157056
    },
    {
      "type": "training",
      "description": "Training step 886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:57",
      "total_flops_so_far": 5269718492209152.0,
      "budget_used_percent": 5.269718492209153
    },
    {
      "type": "training",
      "description": "Training step 887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:58",
      "total_flops_so_far": 5275659550261248.0,
      "budget_used_percent": 5.275659550261248
    },
    {
      "type": "training",
      "description": "Training step 888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:58",
      "total_flops_so_far": 5281600608313344.0,
      "budget_used_percent": 5.281600608313345
    },
    {
      "type": "training",
      "description": "Training step 889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:58",
      "total_flops_so_far": 5287541666365440.0,
      "budget_used_percent": 5.287541666365439
    },
    {
      "type": "training",
      "description": "Training step 890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:59",
      "total_flops_so_far": 5293482724417536.0,
      "budget_used_percent": 5.293482724417536
    },
    {
      "type": "training",
      "description": "Training step 891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:59",
      "total_flops_so_far": 5299423782469632.0,
      "budget_used_percent": 5.299423782469632
    },
    {
      "type": "training",
      "description": "Training step 892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:24:59",
      "total_flops_so_far": 5305364840521728.0,
      "budget_used_percent": 5.305364840521728
    },
    {
      "type": "training",
      "description": "Training step 893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:00",
      "total_flops_so_far": 5311305898573824.0,
      "budget_used_percent": 5.311305898573824
    },
    {
      "type": "training",
      "description": "Training step 894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:00",
      "total_flops_so_far": 5317246956625920.0,
      "budget_used_percent": 5.31724695662592
    },
    {
      "type": "training",
      "description": "Training step 895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:00",
      "total_flops_so_far": 5323188014678016.0,
      "budget_used_percent": 5.323188014678016
    },
    {
      "type": "training",
      "description": "Training step 896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:01",
      "total_flops_so_far": 5329129072730112.0,
      "budget_used_percent": 5.329129072730112
    },
    {
      "type": "training",
      "description": "Training step 897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:01",
      "total_flops_so_far": 5335070130782208.0,
      "budget_used_percent": 5.335070130782208
    },
    {
      "type": "training",
      "description": "Training step 898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:01",
      "total_flops_so_far": 5341011188834304.0,
      "budget_used_percent": 5.341011188834304
    },
    {
      "type": "training",
      "description": "Training step 899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:02",
      "total_flops_so_far": 5346952246886400.0,
      "budget_used_percent": 5.3469522468864
    },
    {
      "type": "training",
      "description": "Training step 900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:02",
      "total_flops_so_far": 5352893304938496.0,
      "budget_used_percent": 5.352893304938497
    },
    {
      "type": "training",
      "description": "Training step 901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:02",
      "total_flops_so_far": 5358834362990592.0,
      "budget_used_percent": 5.358834362990592
    },
    {
      "type": "training",
      "description": "Training step 902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:03",
      "total_flops_so_far": 5364775421042688.0,
      "budget_used_percent": 5.364775421042689
    },
    {
      "type": "training",
      "description": "Training step 903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:03",
      "total_flops_so_far": 5370716479094784.0,
      "budget_used_percent": 5.370716479094783
    },
    {
      "type": "training",
      "description": "Training step 904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:03",
      "total_flops_so_far": 5376657537146880.0,
      "budget_used_percent": 5.37665753714688
    },
    {
      "type": "training",
      "description": "Training step 905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:04",
      "total_flops_so_far": 5382598595198976.0,
      "budget_used_percent": 5.382598595198976
    },
    {
      "type": "training",
      "description": "Training step 906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:04",
      "total_flops_so_far": 5388539653251072.0,
      "budget_used_percent": 5.388539653251072
    },
    {
      "type": "training",
      "description": "Training step 907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:04",
      "total_flops_so_far": 5394480711303168.0,
      "budget_used_percent": 5.394480711303168
    },
    {
      "type": "training",
      "description": "Training step 908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:05",
      "total_flops_so_far": 5400421769355264.0,
      "budget_used_percent": 5.400421769355264
    },
    {
      "type": "training",
      "description": "Training step 909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:05",
      "total_flops_so_far": 5406362827407360.0,
      "budget_used_percent": 5.40636282740736
    },
    {
      "type": "training",
      "description": "Training step 910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:05",
      "total_flops_so_far": 5412303885459456.0,
      "budget_used_percent": 5.412303885459456
    },
    {
      "type": "training",
      "description": "Training step 911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:06",
      "total_flops_so_far": 5418244943511552.0,
      "budget_used_percent": 5.418244943511552
    },
    {
      "type": "training",
      "description": "Training step 912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:06",
      "total_flops_so_far": 5424186001563648.0,
      "budget_used_percent": 5.4241860015636485
    },
    {
      "type": "training",
      "description": "Training step 913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:06",
      "total_flops_so_far": 5430127059615744.0,
      "budget_used_percent": 5.430127059615744
    },
    {
      "type": "training",
      "description": "Training step 914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:07",
      "total_flops_so_far": 5436068117667840.0,
      "budget_used_percent": 5.4360681176678405
    },
    {
      "type": "training",
      "description": "Training step 915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:07",
      "total_flops_so_far": 5442009175719936.0,
      "budget_used_percent": 5.442009175719936
    },
    {
      "type": "training",
      "description": "Training step 916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:07",
      "total_flops_so_far": 5447950233772032.0,
      "budget_used_percent": 5.4479502337720325
    },
    {
      "type": "training",
      "description": "Training step 917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:08",
      "total_flops_so_far": 5453891291824128.0,
      "budget_used_percent": 5.453891291824128
    },
    {
      "type": "training",
      "description": "Training step 918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:08",
      "total_flops_so_far": 5459832349876224.0,
      "budget_used_percent": 5.459832349876224
    },
    {
      "type": "training",
      "description": "Training step 919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:08",
      "total_flops_so_far": 5465773407928320.0,
      "budget_used_percent": 5.46577340792832
    },
    {
      "type": "training",
      "description": "Training step 920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:08",
      "total_flops_so_far": 5471714465980416.0,
      "budget_used_percent": 5.4717144659804156
    },
    {
      "type": "training",
      "description": "Training step 921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:09",
      "total_flops_so_far": 5477655524032512.0,
      "budget_used_percent": 5.477655524032512
    },
    {
      "type": "training",
      "description": "Training step 922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:09",
      "total_flops_so_far": 5483596582084608.0,
      "budget_used_percent": 5.4835965820846075
    },
    {
      "type": "training",
      "description": "Training step 923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:09",
      "total_flops_so_far": 5489537640136704.0,
      "budget_used_percent": 5.489537640136704
    },
    {
      "type": "training",
      "description": "Training step 924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:10",
      "total_flops_so_far": 5495478698188800.0,
      "budget_used_percent": 5.4954786981888
    },
    {
      "type": "training",
      "description": "Training step 925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:10",
      "total_flops_so_far": 5501419756240896.0,
      "budget_used_percent": 5.501419756240896
    },
    {
      "type": "training",
      "description": "Training step 926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:10",
      "total_flops_so_far": 5507360814292992.0,
      "budget_used_percent": 5.507360814292992
    },
    {
      "type": "training",
      "description": "Training step 927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:11",
      "total_flops_so_far": 5513301872345088.0,
      "budget_used_percent": 5.513301872345088
    },
    {
      "type": "training",
      "description": "Training step 928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:11",
      "total_flops_so_far": 5519242930397184.0,
      "budget_used_percent": 5.519242930397184
    },
    {
      "type": "training",
      "description": "Training step 929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:11",
      "total_flops_so_far": 5525183988449280.0,
      "budget_used_percent": 5.52518398844928
    },
    {
      "type": "training",
      "description": "Training step 930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:12",
      "total_flops_so_far": 5531125046501376.0,
      "budget_used_percent": 5.531125046501376
    },
    {
      "type": "training",
      "description": "Training step 931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:12",
      "total_flops_so_far": 5537066104553472.0,
      "budget_used_percent": 5.537066104553472
    },
    {
      "type": "training",
      "description": "Training step 932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:12",
      "total_flops_so_far": 5543007162605568.0,
      "budget_used_percent": 5.5430071626055675
    },
    {
      "type": "training",
      "description": "Training step 933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:13",
      "total_flops_so_far": 5548948220657664.0,
      "budget_used_percent": 5.548948220657664
    },
    {
      "type": "training",
      "description": "Training step 934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:13",
      "total_flops_so_far": 5554889278709760.0,
      "budget_used_percent": 5.5548892787097595
    },
    {
      "type": "training",
      "description": "Training step 935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:13",
      "total_flops_so_far": 5560830336761856.0,
      "budget_used_percent": 5.560830336761856
    },
    {
      "type": "training",
      "description": "Training step 936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:14",
      "total_flops_so_far": 5566771394813952.0,
      "budget_used_percent": 5.5667713948139514
    },
    {
      "type": "training",
      "description": "Training step 937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:14",
      "total_flops_so_far": 5572712452866048.0,
      "budget_used_percent": 5.572712452866048
    },
    {
      "type": "training",
      "description": "Training step 938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:14",
      "total_flops_so_far": 5578653510918144.0,
      "budget_used_percent": 5.578653510918144
    },
    {
      "type": "training",
      "description": "Training step 939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:15",
      "total_flops_so_far": 5584594568970240.0,
      "budget_used_percent": 5.58459456897024
    },
    {
      "type": "training",
      "description": "Training step 940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:15",
      "total_flops_so_far": 5590535627022336.0,
      "budget_used_percent": 5.590535627022336
    },
    {
      "type": "training",
      "description": "Training step 941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:15",
      "total_flops_so_far": 5596476685074432.0,
      "budget_used_percent": 5.596476685074432
    },
    {
      "type": "training",
      "description": "Training step 942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:16",
      "total_flops_so_far": 5602417743126528.0,
      "budget_used_percent": 5.602417743126528
    },
    {
      "type": "training",
      "description": "Training step 943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:16",
      "total_flops_so_far": 5608358801178624.0,
      "budget_used_percent": 5.608358801178624
    },
    {
      "type": "training",
      "description": "Training step 944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:16",
      "total_flops_so_far": 5614299859230720.0,
      "budget_used_percent": 5.61429985923072
    },
    {
      "type": "training",
      "description": "Training step 945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:17",
      "total_flops_so_far": 5620240917282816.0,
      "budget_used_percent": 5.620240917282817
    },
    {
      "type": "training",
      "description": "Training step 946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:17",
      "total_flops_so_far": 5626181975334912.0,
      "budget_used_percent": 5.626181975334911
    },
    {
      "type": "training",
      "description": "Training step 947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:17",
      "total_flops_so_far": 5632123033387008.0,
      "budget_used_percent": 5.632123033387008
    },
    {
      "type": "training",
      "description": "Training step 948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:18",
      "total_flops_so_far": 5638064091439104.0,
      "budget_used_percent": 5.638064091439103
    },
    {
      "type": "training",
      "description": "Training step 949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:18",
      "total_flops_so_far": 5644005149491200.0,
      "budget_used_percent": 5.6440051494912
    },
    {
      "type": "training",
      "description": "Training step 950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:18",
      "total_flops_so_far": 5649946207543296.0,
      "budget_used_percent": 5.649946207543296
    },
    {
      "type": "training",
      "description": "Training step 951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:19",
      "total_flops_so_far": 5655887265595392.0,
      "budget_used_percent": 5.655887265595392
    },
    {
      "type": "training",
      "description": "Training step 952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:19",
      "total_flops_so_far": 5661828323647488.0,
      "budget_used_percent": 5.661828323647488
    },
    {
      "type": "training",
      "description": "Training step 953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:19",
      "total_flops_so_far": 5667769381699584.0,
      "budget_used_percent": 5.667769381699584
    },
    {
      "type": "training",
      "description": "Training step 954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:20",
      "total_flops_so_far": 5673710439751680.0,
      "budget_used_percent": 5.67371043975168
    },
    {
      "type": "training",
      "description": "Training step 955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:20",
      "total_flops_so_far": 5679651497803776.0,
      "budget_used_percent": 5.679651497803776
    },
    {
      "type": "training",
      "description": "Training step 956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:20",
      "total_flops_so_far": 5685592555855872.0,
      "budget_used_percent": 5.685592555855872
    },
    {
      "type": "training",
      "description": "Training step 957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:21",
      "total_flops_so_far": 5691533613907968.0,
      "budget_used_percent": 5.691533613907969
    },
    {
      "type": "training",
      "description": "Training step 958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:21",
      "total_flops_so_far": 5697474671960064.0,
      "budget_used_percent": 5.697474671960064
    },
    {
      "type": "training",
      "description": "Training step 959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:21",
      "total_flops_so_far": 5703415730012160.0,
      "budget_used_percent": 5.703415730012161
    },
    {
      "type": "training",
      "description": "Training step 960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:21",
      "total_flops_so_far": 5709356788064256.0,
      "budget_used_percent": 5.709356788064255
    },
    {
      "type": "training",
      "description": "Training step 961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:22",
      "total_flops_so_far": 5715297846116352.0,
      "budget_used_percent": 5.715297846116352
    },
    {
      "type": "training",
      "description": "Training step 962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:22",
      "total_flops_so_far": 5721238904168448.0,
      "budget_used_percent": 5.721238904168448
    },
    {
      "type": "training",
      "description": "Training step 963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:22",
      "total_flops_so_far": 5727179962220544.0,
      "budget_used_percent": 5.727179962220544
    },
    {
      "type": "training",
      "description": "Training step 964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:23",
      "total_flops_so_far": 5733121020272640.0,
      "budget_used_percent": 5.73312102027264
    },
    {
      "type": "training",
      "description": "Training step 965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:23",
      "total_flops_so_far": 5739062078324736.0,
      "budget_used_percent": 5.739062078324736
    },
    {
      "type": "training",
      "description": "Training step 966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:23",
      "total_flops_so_far": 5745003136376832.0,
      "budget_used_percent": 5.745003136376832
    },
    {
      "type": "training",
      "description": "Training step 967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:24",
      "total_flops_so_far": 5750944194428928.0,
      "budget_used_percent": 5.750944194428928
    },
    {
      "type": "training",
      "description": "Training step 968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:24",
      "total_flops_so_far": 5756885252481024.0,
      "budget_used_percent": 5.756885252481024
    },
    {
      "type": "training",
      "description": "Training step 969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:24",
      "total_flops_so_far": 5762826310533120.0,
      "budget_used_percent": 5.76282631053312
    },
    {
      "type": "training",
      "description": "Training step 970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:25",
      "total_flops_so_far": 5768767368585216.0,
      "budget_used_percent": 5.768767368585216
    },
    {
      "type": "training",
      "description": "Training step 971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:25",
      "total_flops_so_far": 5774708426637312.0,
      "budget_used_percent": 5.7747084266373125
    },
    {
      "type": "training",
      "description": "Training step 972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:25",
      "total_flops_so_far": 5780649484689408.0,
      "budget_used_percent": 5.780649484689408
    },
    {
      "type": "training",
      "description": "Training step 973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:26",
      "total_flops_so_far": 5786590542741504.0,
      "budget_used_percent": 5.7865905427415045
    },
    {
      "type": "training",
      "description": "Training step 974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:26",
      "total_flops_so_far": 5792531600793600.0,
      "budget_used_percent": 5.7925316007936
    },
    {
      "type": "training",
      "description": "Training step 975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:26",
      "total_flops_so_far": 5798472658845696.0,
      "budget_used_percent": 5.798472658845696
    },
    {
      "type": "training",
      "description": "Training step 976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:27",
      "total_flops_so_far": 5804413716897792.0,
      "budget_used_percent": 5.804413716897792
    },
    {
      "type": "training",
      "description": "Training step 977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:27",
      "total_flops_so_far": 5810354774949888.0,
      "budget_used_percent": 5.810354774949888
    },
    {
      "type": "training",
      "description": "Training step 978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:27",
      "total_flops_so_far": 5816295833001984.0,
      "budget_used_percent": 5.816295833001984
    },
    {
      "type": "training",
      "description": "Training step 979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:28",
      "total_flops_so_far": 5822236891054080.0,
      "budget_used_percent": 5.82223689105408
    },
    {
      "type": "training",
      "description": "Training step 980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:28",
      "total_flops_so_far": 5828177949106176.0,
      "budget_used_percent": 5.828177949106176
    },
    {
      "type": "training",
      "description": "Training step 981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:28",
      "total_flops_so_far": 5834119007158272.0,
      "budget_used_percent": 5.834119007158272
    },
    {
      "type": "training",
      "description": "Training step 982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:29",
      "total_flops_so_far": 5840060065210368.0,
      "budget_used_percent": 5.840060065210368
    },
    {
      "type": "training",
      "description": "Training step 983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:29",
      "total_flops_so_far": 5846001123262464.0,
      "budget_used_percent": 5.8460011232624645
    },
    {
      "type": "training",
      "description": "Training step 984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:29",
      "total_flops_so_far": 5851942181314560.0,
      "budget_used_percent": 5.85194218131456
    },
    {
      "type": "training",
      "description": "Training step 985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:30",
      "total_flops_so_far": 5857883239366656.0,
      "budget_used_percent": 5.8578832393666564
    },
    {
      "type": "training",
      "description": "Training step 986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:30",
      "total_flops_so_far": 5863824297418752.0,
      "budget_used_percent": 5.863824297418752
    },
    {
      "type": "training",
      "description": "Training step 987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:30",
      "total_flops_so_far": 5869765355470848.0,
      "budget_used_percent": 5.869765355470848
    },
    {
      "type": "training",
      "description": "Training step 988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:31",
      "total_flops_so_far": 5875706413522944.0,
      "budget_used_percent": 5.875706413522944
    },
    {
      "type": "training",
      "description": "Training step 989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:31",
      "total_flops_so_far": 5881647471575040.0,
      "budget_used_percent": 5.8816474715750395
    },
    {
      "type": "training",
      "description": "Training step 990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:31",
      "total_flops_so_far": 5887588529627136.0,
      "budget_used_percent": 5.887588529627136
    },
    {
      "type": "training",
      "description": "Training step 991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:32",
      "total_flops_so_far": 5893529587679232.0,
      "budget_used_percent": 5.8935295876792315
    },
    {
      "type": "training",
      "description": "Training step 992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:32",
      "total_flops_so_far": 5899470645731328.0,
      "budget_used_percent": 5.899470645731328
    },
    {
      "type": "training",
      "description": "Training step 993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:32",
      "total_flops_so_far": 5905411703783424.0,
      "budget_used_percent": 5.9054117037834235
    },
    {
      "type": "training",
      "description": "Training step 994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:33",
      "total_flops_so_far": 5911352761835520.0,
      "budget_used_percent": 5.91135276183552
    },
    {
      "type": "training",
      "description": "Training step 995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:33",
      "total_flops_so_far": 5917293819887616.0,
      "budget_used_percent": 5.917293819887616
    },
    {
      "type": "training",
      "description": "Training step 996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:33",
      "total_flops_so_far": 5923234877939712.0,
      "budget_used_percent": 5.923234877939712
    },
    {
      "type": "training",
      "description": "Training step 997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:34",
      "total_flops_so_far": 5929175935991808.0,
      "budget_used_percent": 5.929175935991808
    },
    {
      "type": "training",
      "description": "Training step 998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:34",
      "total_flops_so_far": 5935116994043904.0,
      "budget_used_percent": 5.935116994043904
    },
    {
      "type": "training",
      "description": "Training step 999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:25:34",
      "total_flops_so_far": 5941058052096000.0,
      "budget_used_percent": 5.941058052096
    },
    {
      "type": "training",
      "description": "Training step 1000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:27:58",
      "total_flops_so_far": 5946999110148096.0,
      "budget_used_percent": 5.946999110148096
    },
    {
      "type": "training",
      "description": "Training step 1001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:27:59",
      "total_flops_so_far": 5952940168200192.0,
      "budget_used_percent": 5.952940168200192
    },
    {
      "type": "training",
      "description": "Training step 1002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:27:59",
      "total_flops_so_far": 5958881226252288.0,
      "budget_used_percent": 5.958881226252288
    },
    {
      "type": "training",
      "description": "Training step 1003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:27:59",
      "total_flops_so_far": 5964822284304384.0,
      "budget_used_percent": 5.964822284304383
    },
    {
      "type": "training",
      "description": "Training step 1004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:00",
      "total_flops_so_far": 5970763342356480.0,
      "budget_used_percent": 5.97076334235648
    },
    {
      "type": "training",
      "description": "Training step 1005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:00",
      "total_flops_so_far": 5976704400408576.0,
      "budget_used_percent": 5.976704400408575
    },
    {
      "type": "training",
      "description": "Training step 1006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:00",
      "total_flops_so_far": 5982645458460672.0,
      "budget_used_percent": 5.982645458460672
    },
    {
      "type": "training",
      "description": "Training step 1007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:01",
      "total_flops_so_far": 5988586516512768.0,
      "budget_used_percent": 5.988586516512768
    },
    {
      "type": "training",
      "description": "Training step 1008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:01",
      "total_flops_so_far": 5994527574564864.0,
      "budget_used_percent": 5.994527574564864
    },
    {
      "type": "training",
      "description": "Training step 1009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:01",
      "total_flops_so_far": 6000468632616960.0,
      "budget_used_percent": 6.00046863261696
    },
    {
      "type": "training",
      "description": "Training step 1010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:02",
      "total_flops_so_far": 6006409690669056.0,
      "budget_used_percent": 6.006409690669056
    },
    {
      "type": "training",
      "description": "Training step 1011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:02",
      "total_flops_so_far": 6012350748721152.0,
      "budget_used_percent": 6.012350748721152
    },
    {
      "type": "training",
      "description": "Training step 1012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:02",
      "total_flops_so_far": 6018291806773248.0,
      "budget_used_percent": 6.018291806773248
    },
    {
      "type": "training",
      "description": "Training step 1013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:03",
      "total_flops_so_far": 6024232864825344.0,
      "budget_used_percent": 6.024232864825344
    },
    {
      "type": "training",
      "description": "Training step 1014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:03",
      "total_flops_so_far": 6030173922877440.0,
      "budget_used_percent": 6.03017392287744
    },
    {
      "type": "training",
      "description": "Training step 1015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:03",
      "total_flops_so_far": 6036114980929536.0,
      "budget_used_percent": 6.036114980929536
    },
    {
      "type": "training",
      "description": "Training step 1016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:04",
      "total_flops_so_far": 6042056038981632.0,
      "budget_used_percent": 6.042056038981633
    },
    {
      "type": "training",
      "description": "Training step 1017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:04",
      "total_flops_so_far": 6047997097033728.0,
      "budget_used_percent": 6.047997097033727
    },
    {
      "type": "training",
      "description": "Training step 1018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:04",
      "total_flops_so_far": 6053938155085824.0,
      "budget_used_percent": 6.053938155085824
    },
    {
      "type": "training",
      "description": "Training step 1019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:05",
      "total_flops_so_far": 6059879213137920.0,
      "budget_used_percent": 6.05987921313792
    },
    {
      "type": "training",
      "description": "Training step 1020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:05",
      "total_flops_so_far": 6065820271190016.0,
      "budget_used_percent": 6.065820271190016
    },
    {
      "type": "training",
      "description": "Training step 1021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:05",
      "total_flops_so_far": 6071761329242112.0,
      "budget_used_percent": 6.071761329242112
    },
    {
      "type": "training",
      "description": "Training step 1022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:06",
      "total_flops_so_far": 6077702387294208.0,
      "budget_used_percent": 6.077702387294208
    },
    {
      "type": "training",
      "description": "Training step 1023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:06",
      "total_flops_so_far": 6083643445346304.0,
      "budget_used_percent": 6.083643445346304
    },
    {
      "type": "training",
      "description": "Training step 1024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:06",
      "total_flops_so_far": 6089584503398400.0,
      "budget_used_percent": 6.0895845033984
    },
    {
      "type": "training",
      "description": "Training step 1025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:07",
      "total_flops_so_far": 6095525561450496.0,
      "budget_used_percent": 6.095525561450496
    },
    {
      "type": "training",
      "description": "Training step 1026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:07",
      "total_flops_so_far": 6101466619502592.0,
      "budget_used_percent": 6.101466619502592
    },
    {
      "type": "training",
      "description": "Training step 1027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:07",
      "total_flops_so_far": 6107407677554688.0,
      "budget_used_percent": 6.107407677554688
    },
    {
      "type": "training",
      "description": "Training step 1028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:08",
      "total_flops_so_far": 6113348735606784.0,
      "budget_used_percent": 6.113348735606785
    },
    {
      "type": "training",
      "description": "Training step 1029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:08",
      "total_flops_so_far": 6119289793658880.0,
      "budget_used_percent": 6.11928979365888
    },
    {
      "type": "training",
      "description": "Training step 1030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:08",
      "total_flops_so_far": 6125230851710976.0,
      "budget_used_percent": 6.125230851710977
    },
    {
      "type": "training",
      "description": "Training step 1031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:09",
      "total_flops_so_far": 6131171909763072.0,
      "budget_used_percent": 6.131171909763071
    },
    {
      "type": "training",
      "description": "Training step 1032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:09",
      "total_flops_so_far": 6137112967815168.0,
      "budget_used_percent": 6.137112967815168
    },
    {
      "type": "training",
      "description": "Training step 1033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:09",
      "total_flops_so_far": 6143054025867264.0,
      "budget_used_percent": 6.143054025867264
    },
    {
      "type": "training",
      "description": "Training step 1034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:10",
      "total_flops_so_far": 6148995083919360.0,
      "budget_used_percent": 6.14899508391936
    },
    {
      "type": "training",
      "description": "Training step 1035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:10",
      "total_flops_so_far": 6154936141971456.0,
      "budget_used_percent": 6.154936141971456
    },
    {
      "type": "training",
      "description": "Training step 1036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:10",
      "total_flops_so_far": 6160877200023552.0,
      "budget_used_percent": 6.160877200023552
    },
    {
      "type": "training",
      "description": "Training step 1037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:10",
      "total_flops_so_far": 6166818258075648.0,
      "budget_used_percent": 6.166818258075648
    },
    {
      "type": "training",
      "description": "Training step 1038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:11",
      "total_flops_so_far": 6172759316127744.0,
      "budget_used_percent": 6.172759316127744
    },
    {
      "type": "training",
      "description": "Training step 1039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:11",
      "total_flops_so_far": 6178700374179840.0,
      "budget_used_percent": 6.17870037417984
    },
    {
      "type": "training",
      "description": "Training step 1040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:11",
      "total_flops_so_far": 6184641432231936.0,
      "budget_used_percent": 6.1846414322319365
    },
    {
      "type": "training",
      "description": "Training step 1041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:12",
      "total_flops_so_far": 6190582490284032.0,
      "budget_used_percent": 6.190582490284032
    },
    {
      "type": "training",
      "description": "Training step 1042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:12",
      "total_flops_so_far": 6196523548336128.0,
      "budget_used_percent": 6.1965235483361285
    },
    {
      "type": "training",
      "description": "Training step 1043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:12",
      "total_flops_so_far": 6202464606388224.0,
      "budget_used_percent": 6.202464606388224
    },
    {
      "type": "training",
      "description": "Training step 1044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:13",
      "total_flops_so_far": 6208405664440320.0,
      "budget_used_percent": 6.2084056644403205
    },
    {
      "type": "training",
      "description": "Training step 1045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:13",
      "total_flops_so_far": 6214346722492416.0,
      "budget_used_percent": 6.214346722492416
    },
    {
      "type": "training",
      "description": "Training step 1046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:13",
      "total_flops_so_far": 6220287780544512.0,
      "budget_used_percent": 6.220287780544512
    },
    {
      "type": "training",
      "description": "Training step 1047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:14",
      "total_flops_so_far": 6226228838596608.0,
      "budget_used_percent": 6.226228838596608
    },
    {
      "type": "training",
      "description": "Training step 1048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:14",
      "total_flops_so_far": 6232169896648704.0,
      "budget_used_percent": 6.232169896648704
    },
    {
      "type": "training",
      "description": "Training step 1049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:14",
      "total_flops_so_far": 6238110954700800.0,
      "budget_used_percent": 6.2381109547008
    },
    {
      "type": "training",
      "description": "Training step 1050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:15",
      "total_flops_so_far": 6244052012752896.0,
      "budget_used_percent": 6.244052012752896
    },
    {
      "type": "training",
      "description": "Training step 1051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:15",
      "total_flops_so_far": 6249993070804992.0,
      "budget_used_percent": 6.249993070804992
    },
    {
      "type": "training",
      "description": "Training step 1052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:15",
      "total_flops_so_far": 6255934128857088.0,
      "budget_used_percent": 6.255934128857088
    },
    {
      "type": "training",
      "description": "Training step 1053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:16",
      "total_flops_so_far": 6261875186909184.0,
      "budget_used_percent": 6.261875186909184
    },
    {
      "type": "training",
      "description": "Training step 1054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:16",
      "total_flops_so_far": 6267816244961280.0,
      "budget_used_percent": 6.26781624496128
    },
    {
      "type": "training",
      "description": "Training step 1055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:16",
      "total_flops_so_far": 6273757303013376.0,
      "budget_used_percent": 6.273757303013376
    },
    {
      "type": "training",
      "description": "Training step 1056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:17",
      "total_flops_so_far": 6279698361065472.0,
      "budget_used_percent": 6.279698361065472
    },
    {
      "type": "training",
      "description": "Training step 1057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:17",
      "total_flops_so_far": 6285639419117568.0,
      "budget_used_percent": 6.285639419117568
    },
    {
      "type": "training",
      "description": "Training step 1058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:17",
      "total_flops_so_far": 6291580477169664.0,
      "budget_used_percent": 6.291580477169664
    },
    {
      "type": "training",
      "description": "Training step 1059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:18",
      "total_flops_so_far": 6297521535221760.0,
      "budget_used_percent": 6.29752153522176
    },
    {
      "type": "training",
      "description": "Training step 1060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:18",
      "total_flops_so_far": 6303462593273856.0,
      "budget_used_percent": 6.303462593273856
    },
    {
      "type": "training",
      "description": "Training step 1061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:18",
      "total_flops_so_far": 6309403651325952.0,
      "budget_used_percent": 6.309403651325953
    },
    {
      "type": "training",
      "description": "Training step 1062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:19",
      "total_flops_so_far": 6315344709378048.0,
      "budget_used_percent": 6.315344709378048
    },
    {
      "type": "training",
      "description": "Training step 1063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:19",
      "total_flops_so_far": 6321285767430144.0,
      "budget_used_percent": 6.321285767430145
    },
    {
      "type": "training",
      "description": "Training step 1064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:19",
      "total_flops_so_far": 6327226825482240.0,
      "budget_used_percent": 6.32722682548224
    },
    {
      "type": "training",
      "description": "Training step 1065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:20",
      "total_flops_so_far": 6333167883534336.0,
      "budget_used_percent": 6.333167883534337
    },
    {
      "type": "training",
      "description": "Training step 1066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:20",
      "total_flops_so_far": 6339108941586432.0,
      "budget_used_percent": 6.339108941586432
    },
    {
      "type": "training",
      "description": "Training step 1067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:20",
      "total_flops_so_far": 6345049999638528.0,
      "budget_used_percent": 6.345049999638527
    },
    {
      "type": "training",
      "description": "Training step 1068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:21",
      "total_flops_so_far": 6350991057690624.0,
      "budget_used_percent": 6.350991057690623
    },
    {
      "type": "training",
      "description": "Training step 1069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:21",
      "total_flops_so_far": 6356932115742720.0,
      "budget_used_percent": 6.356932115742719
    },
    {
      "type": "training",
      "description": "Training step 1070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:21",
      "total_flops_so_far": 6362873173794816.0,
      "budget_used_percent": 6.362873173794815
    },
    {
      "type": "training",
      "description": "Training step 1071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:22",
      "total_flops_so_far": 6368814231846912.0,
      "budget_used_percent": 6.368814231846912
    },
    {
      "type": "training",
      "description": "Training step 1072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:22",
      "total_flops_so_far": 6374755289899008.0,
      "budget_used_percent": 6.374755289899007
    },
    {
      "type": "training",
      "description": "Training step 1073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:22",
      "total_flops_so_far": 6380696347951104.0,
      "budget_used_percent": 6.380696347951104
    },
    {
      "type": "training",
      "description": "Training step 1074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:23",
      "total_flops_so_far": 6386637406003200.0,
      "budget_used_percent": 6.386637406003199
    },
    {
      "type": "training",
      "description": "Training step 1075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:23",
      "total_flops_so_far": 6392578464055296.0,
      "budget_used_percent": 6.392578464055296
    },
    {
      "type": "training",
      "description": "Training step 1076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:23",
      "total_flops_so_far": 6398519522107392.0,
      "budget_used_percent": 6.398519522107391
    },
    {
      "type": "training",
      "description": "Training step 1077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:23",
      "total_flops_so_far": 6404460580159488.0,
      "budget_used_percent": 6.404460580159488
    },
    {
      "type": "training",
      "description": "Training step 1078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:24",
      "total_flops_so_far": 6410401638211584.0,
      "budget_used_percent": 6.410401638211584
    },
    {
      "type": "training",
      "description": "Training step 1079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:24",
      "total_flops_so_far": 6416342696263680.0,
      "budget_used_percent": 6.41634269626368
    },
    {
      "type": "training",
      "description": "Training step 1080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:24",
      "total_flops_so_far": 6422283754315776.0,
      "budget_used_percent": 6.422283754315776
    },
    {
      "type": "training",
      "description": "Training step 1081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:25",
      "total_flops_so_far": 6428224812367872.0,
      "budget_used_percent": 6.428224812367872
    },
    {
      "type": "training",
      "description": "Training step 1082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:25",
      "total_flops_so_far": 6434165870419968.0,
      "budget_used_percent": 6.434165870419968
    },
    {
      "type": "training",
      "description": "Training step 1083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:25",
      "total_flops_so_far": 6440106928472064.0,
      "budget_used_percent": 6.440106928472064
    },
    {
      "type": "training",
      "description": "Training step 1084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:26",
      "total_flops_so_far": 6446047986524160.0,
      "budget_used_percent": 6.44604798652416
    },
    {
      "type": "training",
      "description": "Training step 1085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:26",
      "total_flops_so_far": 6451989044576256.0,
      "budget_used_percent": 6.451989044576257
    },
    {
      "type": "training",
      "description": "Training step 1086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:27",
      "total_flops_so_far": 6457930102628352.0,
      "budget_used_percent": 6.457930102628352
    },
    {
      "type": "training",
      "description": "Training step 1087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:27",
      "total_flops_so_far": 6463871160680448.0,
      "budget_used_percent": 6.463871160680449
    },
    {
      "type": "training",
      "description": "Training step 1088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:27",
      "total_flops_so_far": 6469812218732544.0,
      "budget_used_percent": 6.469812218732544
    },
    {
      "type": "training",
      "description": "Training step 1089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:28",
      "total_flops_so_far": 6475753276784640.0,
      "budget_used_percent": 6.475753276784641
    },
    {
      "type": "training",
      "description": "Training step 1090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:28",
      "total_flops_so_far": 6481694334836736.0,
      "budget_used_percent": 6.481694334836736
    },
    {
      "type": "training",
      "description": "Training step 1091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:28",
      "total_flops_so_far": 6487635392888832.0,
      "budget_used_percent": 6.487635392888833
    },
    {
      "type": "training",
      "description": "Training step 1092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:29",
      "total_flops_so_far": 6493576450940928.0,
      "budget_used_percent": 6.493576450940928
    },
    {
      "type": "training",
      "description": "Training step 1093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:29",
      "total_flops_so_far": 6499517508993024.0,
      "budget_used_percent": 6.499517508993025
    },
    {
      "type": "training",
      "description": "Training step 1094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:29",
      "total_flops_so_far": 6505458567045120.0,
      "budget_used_percent": 6.505458567045121
    },
    {
      "type": "training",
      "description": "Training step 1095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:30",
      "total_flops_so_far": 6511399625097216.0,
      "budget_used_percent": 6.511399625097216
    },
    {
      "type": "training",
      "description": "Training step 1096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:30",
      "total_flops_so_far": 6517340683149312.0,
      "budget_used_percent": 6.517340683149311
    },
    {
      "type": "training",
      "description": "Training step 1097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:30",
      "total_flops_so_far": 6523281741201408.0,
      "budget_used_percent": 6.523281741201408
    },
    {
      "type": "training",
      "description": "Training step 1098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:31",
      "total_flops_so_far": 6529222799253504.0,
      "budget_used_percent": 6.529222799253503
    },
    {
      "type": "training",
      "description": "Training step 1099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:31",
      "total_flops_so_far": 6535163857305600.0,
      "budget_used_percent": 6.5351638573056
    },
    {
      "type": "training",
      "description": "Training step 1100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:31",
      "total_flops_so_far": 6541104915357696.0,
      "budget_used_percent": 6.541104915357695
    },
    {
      "type": "training",
      "description": "Training step 1101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:32",
      "total_flops_so_far": 6547045973409792.0,
      "budget_used_percent": 6.547045973409792
    },
    {
      "type": "training",
      "description": "Training step 1102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:32",
      "total_flops_so_far": 6552987031461888.0,
      "budget_used_percent": 6.552987031461887
    },
    {
      "type": "training",
      "description": "Training step 1103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:32",
      "total_flops_so_far": 6558928089513984.0,
      "budget_used_percent": 6.558928089513984
    },
    {
      "type": "training",
      "description": "Training step 1104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:33",
      "total_flops_so_far": 6564869147566080.0,
      "budget_used_percent": 6.56486914756608
    },
    {
      "type": "training",
      "description": "Training step 1105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:33",
      "total_flops_so_far": 6570810205618176.0,
      "budget_used_percent": 6.570810205618176
    },
    {
      "type": "training",
      "description": "Training step 1106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:33",
      "total_flops_so_far": 6576751263670272.0,
      "budget_used_percent": 6.576751263670272
    },
    {
      "type": "training",
      "description": "Training step 1107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:34",
      "total_flops_so_far": 6582692321722368.0,
      "budget_used_percent": 6.582692321722368
    },
    {
      "type": "training",
      "description": "Training step 1108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:34",
      "total_flops_so_far": 6588633379774464.0,
      "budget_used_percent": 6.588633379774464
    },
    {
      "type": "training",
      "description": "Training step 1109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:34",
      "total_flops_so_far": 6594574437826560.0,
      "budget_used_percent": 6.59457443782656
    },
    {
      "type": "training",
      "description": "Training step 1110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:35",
      "total_flops_so_far": 6600515495878656.0,
      "budget_used_percent": 6.600515495878656
    },
    {
      "type": "training",
      "description": "Training step 1111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:35",
      "total_flops_so_far": 6606456553930752.0,
      "budget_used_percent": 6.6064565539307525
    },
    {
      "type": "training",
      "description": "Training step 1112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:35",
      "total_flops_so_far": 6612397611982848.0,
      "budget_used_percent": 6.612397611982848
    },
    {
      "type": "training",
      "description": "Training step 1113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:36",
      "total_flops_so_far": 6618338670034944.0,
      "budget_used_percent": 6.6183386700349445
    },
    {
      "type": "training",
      "description": "Training step 1114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:36",
      "total_flops_so_far": 6624279728087040.0,
      "budget_used_percent": 6.62427972808704
    },
    {
      "type": "training",
      "description": "Training step 1115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:36",
      "total_flops_so_far": 6630220786139136.0,
      "budget_used_percent": 6.6302207861391365
    },
    {
      "type": "training",
      "description": "Training step 1116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:37",
      "total_flops_so_far": 6636161844191232.0,
      "budget_used_percent": 6.636161844191232
    },
    {
      "type": "training",
      "description": "Training step 1117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:37",
      "total_flops_so_far": 6642102902243328.0,
      "budget_used_percent": 6.642102902243328
    },
    {
      "type": "training",
      "description": "Training step 1118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:37",
      "total_flops_so_far": 6648043960295424.0,
      "budget_used_percent": 6.648043960295425
    },
    {
      "type": "training",
      "description": "Training step 1119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:38",
      "total_flops_so_far": 6653985018347520.0,
      "budget_used_percent": 6.65398501834752
    },
    {
      "type": "training",
      "description": "Training step 1120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:38",
      "total_flops_so_far": 6659926076399616.0,
      "budget_used_percent": 6.659926076399617
    },
    {
      "type": "training",
      "description": "Training step 1121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:38",
      "total_flops_so_far": 6665867134451712.0,
      "budget_used_percent": 6.665867134451712
    },
    {
      "type": "training",
      "description": "Training step 1122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:39",
      "total_flops_so_far": 6671808192503808.0,
      "budget_used_percent": 6.671808192503809
    },
    {
      "type": "training",
      "description": "Training step 1123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:39",
      "total_flops_so_far": 6677749250555904.0,
      "budget_used_percent": 6.677749250555904
    },
    {
      "type": "training",
      "description": "Training step 1124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:39",
      "total_flops_so_far": 6683690308608000.0,
      "budget_used_percent": 6.683690308607999
    },
    {
      "type": "training",
      "description": "Training step 1125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:40",
      "total_flops_so_far": 6689631366660096.0,
      "budget_used_percent": 6.6896313666600955
    },
    {
      "type": "training",
      "description": "Training step 1126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:40",
      "total_flops_so_far": 6695572424712192.0,
      "budget_used_percent": 6.695572424712191
    },
    {
      "type": "training",
      "description": "Training step 1127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:40",
      "total_flops_so_far": 6701513482764288.0,
      "budget_used_percent": 6.7015134827642875
    },
    {
      "type": "training",
      "description": "Training step 1128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:41",
      "total_flops_so_far": 6707454540816384.0,
      "budget_used_percent": 6.707454540816384
    },
    {
      "type": "training",
      "description": "Training step 1129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:41",
      "total_flops_so_far": 6713395598868480.0,
      "budget_used_percent": 6.7133955988684795
    },
    {
      "type": "training",
      "description": "Training step 1130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:41",
      "total_flops_so_far": 6719336656920576.0,
      "budget_used_percent": 6.719336656920576
    },
    {
      "type": "training",
      "description": "Training step 1131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:42",
      "total_flops_so_far": 6725277714972672.0,
      "budget_used_percent": 6.7252777149726715
    },
    {
      "type": "training",
      "description": "Training step 1132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:42",
      "total_flops_so_far": 6731218773024768.0,
      "budget_used_percent": 6.731218773024768
    },
    {
      "type": "training",
      "description": "Training step 1133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:42",
      "total_flops_so_far": 6737159831076864.0,
      "budget_used_percent": 6.7371598310768634
    },
    {
      "type": "training",
      "description": "Training step 1134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:43",
      "total_flops_so_far": 6743100889128960.0,
      "budget_used_percent": 6.74310088912896
    },
    {
      "type": "training",
      "description": "Training step 1135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:43",
      "total_flops_so_far": 6749041947181056.0,
      "budget_used_percent": 6.749041947181055
    },
    {
      "type": "training",
      "description": "Training step 1136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:43",
      "total_flops_so_far": 6754983005233152.0,
      "budget_used_percent": 6.754983005233152
    },
    {
      "type": "training",
      "description": "Training step 1137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:44",
      "total_flops_so_far": 6760924063285248.0,
      "budget_used_percent": 6.760924063285248
    },
    {
      "type": "training",
      "description": "Training step 1138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:44",
      "total_flops_so_far": 6766865121337344.0,
      "budget_used_percent": 6.766865121337344
    },
    {
      "type": "training",
      "description": "Training step 1139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:44",
      "total_flops_so_far": 6772806179389440.0,
      "budget_used_percent": 6.77280617938944
    },
    {
      "type": "training",
      "description": "Training step 1140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:45",
      "total_flops_so_far": 6778747237441536.0,
      "budget_used_percent": 6.778747237441536
    },
    {
      "type": "training",
      "description": "Training step 1141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:45",
      "total_flops_so_far": 6784688295493632.0,
      "budget_used_percent": 6.784688295493632
    },
    {
      "type": "training",
      "description": "Training step 1142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:45",
      "total_flops_so_far": 6790629353545728.0,
      "budget_used_percent": 6.790629353545728
    },
    {
      "type": "training",
      "description": "Training step 1143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:46",
      "total_flops_so_far": 6796570411597824.0,
      "budget_used_percent": 6.796570411597824
    },
    {
      "type": "training",
      "description": "Training step 1144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:46",
      "total_flops_so_far": 6802511469649920.0,
      "budget_used_percent": 6.802511469649921
    },
    {
      "type": "training",
      "description": "Training step 1145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:46",
      "total_flops_so_far": 6808452527702016.0,
      "budget_used_percent": 6.808452527702016
    },
    {
      "type": "training",
      "description": "Training step 1146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:47",
      "total_flops_so_far": 6814393585754112.0,
      "budget_used_percent": 6.814393585754113
    },
    {
      "type": "training",
      "description": "Training step 1147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:47",
      "total_flops_so_far": 6820334643806208.0,
      "budget_used_percent": 6.820334643806208
    },
    {
      "type": "training",
      "description": "Training step 1148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:47",
      "total_flops_so_far": 6826275701858304.0,
      "budget_used_percent": 6.826275701858305
    },
    {
      "type": "training",
      "description": "Training step 1149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:48",
      "total_flops_so_far": 6832216759910400.0,
      "budget_used_percent": 6.8322167599104
    },
    {
      "type": "training",
      "description": "Training step 1150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:48",
      "total_flops_so_far": 6838157817962496.0,
      "budget_used_percent": 6.838157817962497
    },
    {
      "type": "training",
      "description": "Training step 1151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:48",
      "total_flops_so_far": 6844098876014592.0,
      "budget_used_percent": 6.844098876014593
    },
    {
      "type": "training",
      "description": "Training step 1152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:48",
      "total_flops_so_far": 6850039934066688.0,
      "budget_used_percent": 6.850039934066688
    },
    {
      "type": "training",
      "description": "Training step 1153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:49",
      "total_flops_so_far": 6855980992118784.0,
      "budget_used_percent": 6.855980992118783
    },
    {
      "type": "training",
      "description": "Training step 1154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:49",
      "total_flops_so_far": 6861922050170880.0,
      "budget_used_percent": 6.86192205017088
    },
    {
      "type": "training",
      "description": "Training step 1155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:49",
      "total_flops_so_far": 6867863108222976.0,
      "budget_used_percent": 6.867863108222975
    },
    {
      "type": "training",
      "description": "Training step 1156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:50",
      "total_flops_so_far": 6873804166275072.0,
      "budget_used_percent": 6.873804166275072
    },
    {
      "type": "training",
      "description": "Training step 1157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:50",
      "total_flops_so_far": 6879745224327168.0,
      "budget_used_percent": 6.879745224327167
    },
    {
      "type": "training",
      "description": "Training step 1158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:50",
      "total_flops_so_far": 6885686282379264.0,
      "budget_used_percent": 6.885686282379264
    },
    {
      "type": "training",
      "description": "Training step 1159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:51",
      "total_flops_so_far": 6891627340431360.0,
      "budget_used_percent": 6.891627340431359
    },
    {
      "type": "training",
      "description": "Training step 1160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:51",
      "total_flops_so_far": 6897568398483456.0,
      "budget_used_percent": 6.897568398483456
    },
    {
      "type": "training",
      "description": "Training step 1161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:51",
      "total_flops_so_far": 6903509456535552.0,
      "budget_used_percent": 6.903509456535552
    },
    {
      "type": "training",
      "description": "Training step 1162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:52",
      "total_flops_so_far": 6909450514587648.0,
      "budget_used_percent": 6.909450514587648
    },
    {
      "type": "training",
      "description": "Training step 1163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:52",
      "total_flops_so_far": 6915391572639744.0,
      "budget_used_percent": 6.915391572639744
    },
    {
      "type": "training",
      "description": "Training step 1164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:52",
      "total_flops_so_far": 6921332630691840.0,
      "budget_used_percent": 6.92133263069184
    },
    {
      "type": "training",
      "description": "Training step 1165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:53",
      "total_flops_so_far": 6927273688743936.0,
      "budget_used_percent": 6.927273688743936
    },
    {
      "type": "training",
      "description": "Training step 1166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:53",
      "total_flops_so_far": 6933214746796032.0,
      "budget_used_percent": 6.933214746796032
    },
    {
      "type": "training",
      "description": "Training step 1167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:53",
      "total_flops_so_far": 6939155804848128.0,
      "budget_used_percent": 6.939155804848128
    },
    {
      "type": "training",
      "description": "Training step 1168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:54",
      "total_flops_so_far": 6945096862900224.0,
      "budget_used_percent": 6.945096862900224
    },
    {
      "type": "training",
      "description": "Training step 1169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:54",
      "total_flops_so_far": 6951037920952320.0,
      "budget_used_percent": 6.95103792095232
    },
    {
      "type": "training",
      "description": "Training step 1170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:54",
      "total_flops_so_far": 6956978979004416.0,
      "budget_used_percent": 6.9569789790044165
    },
    {
      "type": "training",
      "description": "Training step 1171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:55",
      "total_flops_so_far": 6962920037056512.0,
      "budget_used_percent": 6.962920037056512
    },
    {
      "type": "training",
      "description": "Training step 1172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:55",
      "total_flops_so_far": 6968861095108608.0,
      "budget_used_percent": 6.9688610951086085
    },
    {
      "type": "training",
      "description": "Training step 1173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:55",
      "total_flops_so_far": 6974802153160704.0,
      "budget_used_percent": 6.974802153160704
    },
    {
      "type": "training",
      "description": "Training step 1174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:56",
      "total_flops_so_far": 6980743211212800.0,
      "budget_used_percent": 6.9807432112128005
    },
    {
      "type": "training",
      "description": "Training step 1175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:56",
      "total_flops_so_far": 6986684269264896.0,
      "budget_used_percent": 6.986684269264896
    },
    {
      "type": "training",
      "description": "Training step 1176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:56",
      "total_flops_so_far": 6992625327316992.0,
      "budget_used_percent": 6.9926253273169925
    },
    {
      "type": "training",
      "description": "Training step 1177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:57",
      "total_flops_so_far": 6998566385369088.0,
      "budget_used_percent": 6.998566385369089
    },
    {
      "type": "training",
      "description": "Training step 1178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:57",
      "total_flops_so_far": 7004507443421184.0,
      "budget_used_percent": 7.0045074434211845
    },
    {
      "type": "training",
      "description": "Training step 1179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:57",
      "total_flops_so_far": 7010448501473280.0,
      "budget_used_percent": 7.010448501473281
    },
    {
      "type": "training",
      "description": "Training step 1180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:58",
      "total_flops_so_far": 7016389559525376.0,
      "budget_used_percent": 7.0163895595253765
    },
    {
      "type": "training",
      "description": "Training step 1181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:58",
      "total_flops_so_far": 7022330617577472.0,
      "budget_used_percent": 7.022330617577471
    },
    {
      "type": "training",
      "description": "Training step 1182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:58",
      "total_flops_so_far": 7028271675629568.0,
      "budget_used_percent": 7.028271675629568
    },
    {
      "type": "training",
      "description": "Training step 1183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:59",
      "total_flops_so_far": 7034212733681664.0,
      "budget_used_percent": 7.034212733681663
    },
    {
      "type": "training",
      "description": "Training step 1184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:59",
      "total_flops_so_far": 7040153791733760.0,
      "budget_used_percent": 7.0401537917337595
    },
    {
      "type": "training",
      "description": "Training step 1185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:28:59",
      "total_flops_so_far": 7046094849785856.0,
      "budget_used_percent": 7.046094849785856
    },
    {
      "type": "training",
      "description": "Training step 1186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:00",
      "total_flops_so_far": 7052035907837952.0,
      "budget_used_percent": 7.0520359078379515
    },
    {
      "type": "training",
      "description": "Training step 1187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:00",
      "total_flops_so_far": 7057976965890048.0,
      "budget_used_percent": 7.057976965890048
    },
    {
      "type": "training",
      "description": "Training step 1188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:00",
      "total_flops_so_far": 7063918023942144.0,
      "budget_used_percent": 7.0639180239421435
    },
    {
      "type": "training",
      "description": "Training step 1189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:01",
      "total_flops_so_far": 7069859081994240.0,
      "budget_used_percent": 7.06985908199424
    },
    {
      "type": "training",
      "description": "Training step 1190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:01",
      "total_flops_so_far": 7075800140046336.0,
      "budget_used_percent": 7.0758001400463355
    },
    {
      "type": "training",
      "description": "Training step 1191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:01",
      "total_flops_so_far": 7081741198098432.0,
      "budget_used_percent": 7.081741198098432
    },
    {
      "type": "training",
      "description": "Training step 1192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:02",
      "total_flops_so_far": 7087682256150528.0,
      "budget_used_percent": 7.0876822561505275
    },
    {
      "type": "training",
      "description": "Training step 1193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:02",
      "total_flops_so_far": 7093623314202624.0,
      "budget_used_percent": 7.093623314202624
    },
    {
      "type": "training",
      "description": "Training step 1194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:02",
      "total_flops_so_far": 7099564372254720.0,
      "budget_used_percent": 7.09956437225472
    },
    {
      "type": "training",
      "description": "Training step 1195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:03",
      "total_flops_so_far": 7105505430306816.0,
      "budget_used_percent": 7.105505430306816
    },
    {
      "type": "training",
      "description": "Training step 1196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:03",
      "total_flops_so_far": 7111446488358912.0,
      "budget_used_percent": 7.111446488358912
    },
    {
      "type": "training",
      "description": "Training step 1197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:03",
      "total_flops_so_far": 7117387546411008.0,
      "budget_used_percent": 7.117387546411008
    },
    {
      "type": "training",
      "description": "Training step 1198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:04",
      "total_flops_so_far": 7123328604463104.0,
      "budget_used_percent": 7.123328604463104
    },
    {
      "type": "training",
      "description": "Training step 1199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:04",
      "total_flops_so_far": 7129269662515200.0,
      "budget_used_percent": 7.1292696625152
    },
    {
      "type": "training",
      "description": "Training step 1200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:04",
      "total_flops_so_far": 7135210720567296.0,
      "budget_used_percent": 7.135210720567296
    },
    {
      "type": "training",
      "description": "Training step 1201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:05",
      "total_flops_so_far": 7141151778619392.0,
      "budget_used_percent": 7.141151778619392
    },
    {
      "type": "training",
      "description": "Training step 1202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:05",
      "total_flops_so_far": 7147092836671488.0,
      "budget_used_percent": 7.147092836671488
    },
    {
      "type": "training",
      "description": "Training step 1203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:05",
      "total_flops_so_far": 7153033894723584.0,
      "budget_used_percent": 7.153033894723585
    },
    {
      "type": "training",
      "description": "Training step 1204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:06",
      "total_flops_so_far": 7158974952775680.0,
      "budget_used_percent": 7.15897495277568
    },
    {
      "type": "training",
      "description": "Training step 1205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:06",
      "total_flops_so_far": 7164916010827776.0,
      "budget_used_percent": 7.164916010827777
    },
    {
      "type": "training",
      "description": "Training step 1206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:06",
      "total_flops_so_far": 7170857068879872.0,
      "budget_used_percent": 7.170857068879872
    },
    {
      "type": "training",
      "description": "Training step 1207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:07",
      "total_flops_so_far": 7176798126931968.0,
      "budget_used_percent": 7.176798126931969
    },
    {
      "type": "training",
      "description": "Training step 1208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:07",
      "total_flops_so_far": 7182739184984064.0,
      "budget_used_percent": 7.182739184984064
    },
    {
      "type": "training",
      "description": "Training step 1209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:07",
      "total_flops_so_far": 7188680243036160.0,
      "budget_used_percent": 7.188680243036159
    },
    {
      "type": "training",
      "description": "Training step 1210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:08",
      "total_flops_so_far": 7194621301088256.0,
      "budget_used_percent": 7.194621301088255
    },
    {
      "type": "training",
      "description": "Training step 1211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:08",
      "total_flops_so_far": 7200562359140352.0,
      "budget_used_percent": 7.200562359140352
    },
    {
      "type": "training",
      "description": "Training step 1212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:08",
      "total_flops_so_far": 7206503417192448.0,
      "budget_used_percent": 7.206503417192447
    },
    {
      "type": "training",
      "description": "Training step 1213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:09",
      "total_flops_so_far": 7212444475244544.0,
      "budget_used_percent": 7.212444475244544
    },
    {
      "type": "training",
      "description": "Training step 1214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:09",
      "total_flops_so_far": 7218385533296640.0,
      "budget_used_percent": 7.218385533296639
    },
    {
      "type": "training",
      "description": "Training step 1215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:09",
      "total_flops_so_far": 7224326591348736.0,
      "budget_used_percent": 7.224326591348736
    },
    {
      "type": "training",
      "description": "Training step 1216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:10",
      "total_flops_so_far": 7230267649400832.0,
      "budget_used_percent": 7.230267649400831
    },
    {
      "type": "training",
      "description": "Training step 1217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:10",
      "total_flops_so_far": 7236208707452928.0,
      "budget_used_percent": 7.236208707452928
    },
    {
      "type": "training",
      "description": "Training step 1218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:10",
      "total_flops_so_far": 7242149765505024.0,
      "budget_used_percent": 7.242149765505024
    },
    {
      "type": "training",
      "description": "Training step 1219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:11",
      "total_flops_so_far": 7248090823557120.0,
      "budget_used_percent": 7.24809082355712
    },
    {
      "type": "training",
      "description": "Training step 1220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:11",
      "total_flops_so_far": 7254031881609216.0,
      "budget_used_percent": 7.254031881609216
    },
    {
      "type": "training",
      "description": "Training step 1221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:11",
      "total_flops_so_far": 7259972939661312.0,
      "budget_used_percent": 7.259972939661312
    },
    {
      "type": "training",
      "description": "Training step 1222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:12",
      "total_flops_so_far": 7265913997713408.0,
      "budget_used_percent": 7.265913997713408
    },
    {
      "type": "training",
      "description": "Training step 1223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:12",
      "total_flops_so_far": 7271855055765504.0,
      "budget_used_percent": 7.271855055765504
    },
    {
      "type": "training",
      "description": "Training step 1224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:12",
      "total_flops_so_far": 7277796113817600.0,
      "budget_used_percent": 7.2777961138176
    },
    {
      "type": "training",
      "description": "Training step 1225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:13",
      "total_flops_so_far": 7283737171869696.0,
      "budget_used_percent": 7.283737171869696
    },
    {
      "type": "training",
      "description": "Training step 1226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:13",
      "total_flops_so_far": 7289678229921792.0,
      "budget_used_percent": 7.289678229921792
    },
    {
      "type": "training",
      "description": "Training step 1227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:13",
      "total_flops_so_far": 7295619287973888.0,
      "budget_used_percent": 7.295619287973889
    },
    {
      "type": "training",
      "description": "Training step 1228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:14",
      "total_flops_so_far": 7301560346025984.0,
      "budget_used_percent": 7.301560346025984
    },
    {
      "type": "training",
      "description": "Training step 1229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:14",
      "total_flops_so_far": 7307501404078080.0,
      "budget_used_percent": 7.307501404078081
    },
    {
      "type": "training",
      "description": "Training step 1230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:14",
      "total_flops_so_far": 7313442462130176.0,
      "budget_used_percent": 7.313442462130176
    },
    {
      "type": "training",
      "description": "Training step 1231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:15",
      "total_flops_so_far": 7319383520182272.0,
      "budget_used_percent": 7.319383520182273
    },
    {
      "type": "training",
      "description": "Training step 1232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:15",
      "total_flops_so_far": 7325324578234368.0,
      "budget_used_percent": 7.325324578234368
    },
    {
      "type": "training",
      "description": "Training step 1233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:15",
      "total_flops_so_far": 7331265636286464.0,
      "budget_used_percent": 7.3312656362864645
    },
    {
      "type": "training",
      "description": "Training step 1234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:16",
      "total_flops_so_far": 7337206694338560.0,
      "budget_used_percent": 7.33720669433856
    },
    {
      "type": "training",
      "description": "Training step 1235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:16",
      "total_flops_so_far": 7343147752390656.0,
      "budget_used_percent": 7.3431477523906565
    },
    {
      "type": "training",
      "description": "Training step 1236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:16",
      "total_flops_so_far": 7349088810442752.0,
      "budget_used_percent": 7.349088810442753
    },
    {
      "type": "training",
      "description": "Training step 1237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:17",
      "total_flops_so_far": 7355029868494848.0,
      "budget_used_percent": 7.3550298684948485
    },
    {
      "type": "training",
      "description": "Training step 1238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:17",
      "total_flops_so_far": 7360970926546944.0,
      "budget_used_percent": 7.360970926546943
    },
    {
      "type": "training",
      "description": "Training step 1239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:17",
      "total_flops_so_far": 7366911984599040.0,
      "budget_used_percent": 7.36691198459904
    },
    {
      "type": "training",
      "description": "Training step 1240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:18",
      "total_flops_so_far": 7372853042651136.0,
      "budget_used_percent": 7.372853042651135
    },
    {
      "type": "training",
      "description": "Training step 1241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:18",
      "total_flops_so_far": 7378794100703232.0,
      "budget_used_percent": 7.378794100703232
    },
    {
      "type": "training",
      "description": "Training step 1242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:18",
      "total_flops_so_far": 7384735158755328.0,
      "budget_used_percent": 7.384735158755327
    },
    {
      "type": "training",
      "description": "Training step 1243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:19",
      "total_flops_so_far": 7390676216807424.0,
      "budget_used_percent": 7.390676216807424
    },
    {
      "type": "training",
      "description": "Training step 1244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:19",
      "total_flops_so_far": 7396617274859520.0,
      "budget_used_percent": 7.39661727485952
    },
    {
      "type": "training",
      "description": "Training step 1245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:19",
      "total_flops_so_far": 7402558332911616.0,
      "budget_used_percent": 7.402558332911616
    },
    {
      "type": "training",
      "description": "Training step 1246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:20",
      "total_flops_so_far": 7408499390963712.0,
      "budget_used_percent": 7.408499390963712
    },
    {
      "type": "training",
      "description": "Training step 1247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:20",
      "total_flops_so_far": 7414440449015808.0,
      "budget_used_percent": 7.414440449015808
    },
    {
      "type": "training",
      "description": "Training step 1248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:20",
      "total_flops_so_far": 7420381507067904.0,
      "budget_used_percent": 7.420381507067904
    },
    {
      "type": "training",
      "description": "Training step 1249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:21",
      "total_flops_so_far": 7426322565120000.0,
      "budget_used_percent": 7.4263225651199996
    },
    {
      "type": "training",
      "description": "Training step 1250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:21",
      "total_flops_so_far": 7432263623172096.0,
      "budget_used_percent": 7.432263623172096
    },
    {
      "type": "training",
      "description": "Training step 1251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:21",
      "total_flops_so_far": 7438204681224192.0,
      "budget_used_percent": 7.438204681224192
    },
    {
      "type": "training",
      "description": "Training step 1252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:22",
      "total_flops_so_far": 7444145739276288.0,
      "budget_used_percent": 7.444145739276288
    },
    {
      "type": "training",
      "description": "Training step 1253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:22",
      "total_flops_so_far": 7450086797328384.0,
      "budget_used_percent": 7.450086797328384
    },
    {
      "type": "training",
      "description": "Training step 1254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:22",
      "total_flops_so_far": 7456027855380480.0,
      "budget_used_percent": 7.45602785538048
    },
    {
      "type": "training",
      "description": "Training step 1255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:23",
      "total_flops_so_far": 7461968913432576.0,
      "budget_used_percent": 7.461968913432576
    },
    {
      "type": "training",
      "description": "Training step 1256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:23",
      "total_flops_so_far": 7467909971484672.0,
      "budget_used_percent": 7.467909971484672
    },
    {
      "type": "training",
      "description": "Training step 1257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:23",
      "total_flops_so_far": 7473851029536768.0,
      "budget_used_percent": 7.473851029536768
    },
    {
      "type": "training",
      "description": "Training step 1258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:24",
      "total_flops_so_far": 7479792087588864.0,
      "budget_used_percent": 7.479792087588864
    },
    {
      "type": "training",
      "description": "Training step 1259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:24",
      "total_flops_so_far": 7485733145640960.0,
      "budget_used_percent": 7.48573314564096
    },
    {
      "type": "training",
      "description": "Training step 1260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:24",
      "total_flops_so_far": 7491674203693056.0,
      "budget_used_percent": 7.491674203693057
    },
    {
      "type": "training",
      "description": "Training step 1261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:25",
      "total_flops_so_far": 7497615261745152.0,
      "budget_used_percent": 7.497615261745152
    },
    {
      "type": "training",
      "description": "Training step 1262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:25",
      "total_flops_so_far": 7503556319797248.0,
      "budget_used_percent": 7.503556319797249
    },
    {
      "type": "training",
      "description": "Training step 1263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:25",
      "total_flops_so_far": 7509497377849344.0,
      "budget_used_percent": 7.509497377849344
    },
    {
      "type": "training",
      "description": "Training step 1264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:26",
      "total_flops_so_far": 7515438435901440.0,
      "budget_used_percent": 7.515438435901441
    },
    {
      "type": "training",
      "description": "Training step 1265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:26",
      "total_flops_so_far": 7521379493953536.0,
      "budget_used_percent": 7.521379493953536
    },
    {
      "type": "training",
      "description": "Training step 1266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:26",
      "total_flops_so_far": 7527320552005632.0,
      "budget_used_percent": 7.527320552005631
    },
    {
      "type": "training",
      "description": "Training step 1267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:27",
      "total_flops_so_far": 7533261610057728.0,
      "budget_used_percent": 7.533261610057727
    },
    {
      "type": "training",
      "description": "Training step 1268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:27",
      "total_flops_so_far": 7539202668109824.0,
      "budget_used_percent": 7.539202668109823
    },
    {
      "type": "training",
      "description": "Training step 1269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:27",
      "total_flops_so_far": 7545143726161920.0,
      "budget_used_percent": 7.545143726161919
    },
    {
      "type": "training",
      "description": "Training step 1270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:28",
      "total_flops_so_far": 7551084784214016.0,
      "budget_used_percent": 7.551084784214016
    },
    {
      "type": "training",
      "description": "Training step 1271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:28",
      "total_flops_so_far": 7557025842266112.0,
      "budget_used_percent": 7.557025842266111
    },
    {
      "type": "training",
      "description": "Training step 1272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:28",
      "total_flops_so_far": 7562966900318208.0,
      "budget_used_percent": 7.562966900318208
    },
    {
      "type": "training",
      "description": "Training step 1273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:29",
      "total_flops_so_far": 7568907958370304.0,
      "budget_used_percent": 7.568907958370303
    },
    {
      "type": "training",
      "description": "Training step 1274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:29",
      "total_flops_so_far": 7574849016422400.0,
      "budget_used_percent": 7.5748490164224
    },
    {
      "type": "training",
      "description": "Training step 1275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:29",
      "total_flops_so_far": 7580790074474496.0,
      "budget_used_percent": 7.580790074474495
    },
    {
      "type": "training",
      "description": "Training step 1276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:30",
      "total_flops_so_far": 7586731132526592.0,
      "budget_used_percent": 7.586731132526592
    },
    {
      "type": "training",
      "description": "Training step 1277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:30",
      "total_flops_so_far": 7592672190578688.0,
      "budget_used_percent": 7.592672190578688
    },
    {
      "type": "training",
      "description": "Training step 1278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:30",
      "total_flops_so_far": 7598613248630784.0,
      "budget_used_percent": 7.598613248630784
    },
    {
      "type": "training",
      "description": "Training step 1279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:31",
      "total_flops_so_far": 7604554306682880.0,
      "budget_used_percent": 7.60455430668288
    },
    {
      "type": "training",
      "description": "Training step 1280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:31",
      "total_flops_so_far": 7610495364734976.0,
      "budget_used_percent": 7.610495364734976
    },
    {
      "type": "training",
      "description": "Training step 1281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:31",
      "total_flops_so_far": 7616436422787072.0,
      "budget_used_percent": 7.616436422787072
    },
    {
      "type": "training",
      "description": "Training step 1282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:32",
      "total_flops_so_far": 7622377480839168.0,
      "budget_used_percent": 7.622377480839168
    },
    {
      "type": "training",
      "description": "Training step 1283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:32",
      "total_flops_so_far": 7628318538891264.0,
      "budget_used_percent": 7.628318538891264
    },
    {
      "type": "training",
      "description": "Training step 1284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:32",
      "total_flops_so_far": 7634259596943360.0,
      "budget_used_percent": 7.634259596943361
    },
    {
      "type": "training",
      "description": "Training step 1285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:33",
      "total_flops_so_far": 7640200654995456.0,
      "budget_used_percent": 7.640200654995456
    },
    {
      "type": "training",
      "description": "Training step 1286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:33",
      "total_flops_so_far": 7646141713047552.0,
      "budget_used_percent": 7.646141713047553
    },
    {
      "type": "training",
      "description": "Training step 1287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:33",
      "total_flops_so_far": 7652082771099648.0,
      "budget_used_percent": 7.652082771099648
    },
    {
      "type": "training",
      "description": "Training step 1288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:34",
      "total_flops_so_far": 7658023829151744.0,
      "budget_used_percent": 7.658023829151745
    },
    {
      "type": "training",
      "description": "Training step 1289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:34",
      "total_flops_so_far": 7663964887203840.0,
      "budget_used_percent": 7.66396488720384
    },
    {
      "type": "training",
      "description": "Training step 1290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:34",
      "total_flops_so_far": 7669905945255936.0,
      "budget_used_percent": 7.669905945255937
    },
    {
      "type": "training",
      "description": "Training step 1291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:35",
      "total_flops_so_far": 7675847003308032.0,
      "budget_used_percent": 7.675847003308032
    },
    {
      "type": "training",
      "description": "Training step 1292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:35",
      "total_flops_so_far": 7681788061360128.0,
      "budget_used_percent": 7.681788061360129
    },
    {
      "type": "training",
      "description": "Training step 1293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:35",
      "total_flops_so_far": 7687729119412224.0,
      "budget_used_percent": 7.687729119412225
    },
    {
      "type": "training",
      "description": "Training step 1294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:36",
      "total_flops_so_far": 7693670177464320.0,
      "budget_used_percent": 7.693670177464321
    },
    {
      "type": "training",
      "description": "Training step 1295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:36",
      "total_flops_so_far": 7699611235516416.0,
      "budget_used_percent": 7.699611235516415
    },
    {
      "type": "training",
      "description": "Training step 1296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:36",
      "total_flops_so_far": 7705552293568512.0,
      "budget_used_percent": 7.705552293568512
    },
    {
      "type": "training",
      "description": "Training step 1297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:37",
      "total_flops_so_far": 7711493351620608.0,
      "budget_used_percent": 7.711493351620607
    },
    {
      "type": "training",
      "description": "Training step 1298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:37",
      "total_flops_so_far": 7717434409672704.0,
      "budget_used_percent": 7.717434409672704
    },
    {
      "type": "training",
      "description": "Training step 1299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:37",
      "total_flops_so_far": 7723375467724800.0,
      "budget_used_percent": 7.723375467724799
    },
    {
      "type": "training",
      "description": "Training step 1300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:38",
      "total_flops_so_far": 7729316525776896.0,
      "budget_used_percent": 7.729316525776896
    },
    {
      "type": "training",
      "description": "Training step 1301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:38",
      "total_flops_so_far": 7735257583828992.0,
      "budget_used_percent": 7.735257583828991
    },
    {
      "type": "training",
      "description": "Training step 1302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:38",
      "total_flops_so_far": 7741198641881088.0,
      "budget_used_percent": 7.741198641881088
    },
    {
      "type": "training",
      "description": "Training step 1303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:39",
      "total_flops_so_far": 7747139699933184.0,
      "budget_used_percent": 7.747139699933184
    },
    {
      "type": "training",
      "description": "Training step 1304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:39",
      "total_flops_so_far": 7753080757985280.0,
      "budget_used_percent": 7.75308075798528
    },
    {
      "type": "training",
      "description": "Training step 1305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:39",
      "total_flops_so_far": 7759021816037376.0,
      "budget_used_percent": 7.759021816037376
    },
    {
      "type": "training",
      "description": "Training step 1306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:40",
      "total_flops_so_far": 7764962874089472.0,
      "budget_used_percent": 7.764962874089472
    },
    {
      "type": "training",
      "description": "Training step 1307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:40",
      "total_flops_so_far": 7770903932141568.0,
      "budget_used_percent": 7.770903932141568
    },
    {
      "type": "training",
      "description": "Training step 1308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:40",
      "total_flops_so_far": 7776844990193664.0,
      "budget_used_percent": 7.776844990193664
    },
    {
      "type": "training",
      "description": "Training step 1309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:41",
      "total_flops_so_far": 7782786048245760.0,
      "budget_used_percent": 7.78278604824576
    },
    {
      "type": "training",
      "description": "Training step 1310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:41",
      "total_flops_so_far": 7788727106297856.0,
      "budget_used_percent": 7.7887271062978565
    },
    {
      "type": "training",
      "description": "Training step 1311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:41",
      "total_flops_so_far": 7794668164349952.0,
      "budget_used_percent": 7.794668164349952
    },
    {
      "type": "training",
      "description": "Training step 1312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:42",
      "total_flops_so_far": 7800609222402048.0,
      "budget_used_percent": 7.8006092224020485
    },
    {
      "type": "training",
      "description": "Training step 1313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:42",
      "total_flops_so_far": 7806550280454144.0,
      "budget_used_percent": 7.806550280454144
    },
    {
      "type": "training",
      "description": "Training step 1314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:42",
      "total_flops_so_far": 7812491338506240.0,
      "budget_used_percent": 7.8124913385062404
    },
    {
      "type": "training",
      "description": "Training step 1315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:43",
      "total_flops_so_far": 7818432396558336.0,
      "budget_used_percent": 7.818432396558336
    },
    {
      "type": "training",
      "description": "Training step 1316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:43",
      "total_flops_so_far": 7824373454610432.0,
      "budget_used_percent": 7.824373454610432
    },
    {
      "type": "training",
      "description": "Training step 1317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:43",
      "total_flops_so_far": 7830314512662528.0,
      "budget_used_percent": 7.830314512662529
    },
    {
      "type": "training",
      "description": "Training step 1318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:44",
      "total_flops_so_far": 7836255570714624.0,
      "budget_used_percent": 7.836255570714624
    },
    {
      "type": "training",
      "description": "Training step 1319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:44",
      "total_flops_so_far": 7842196628766720.0,
      "budget_used_percent": 7.842196628766721
    },
    {
      "type": "training",
      "description": "Training step 1320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:44",
      "total_flops_so_far": 7848137686818816.0,
      "budget_used_percent": 7.848137686818816
    },
    {
      "type": "training",
      "description": "Training step 1321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:45",
      "total_flops_so_far": 7854078744870912.0,
      "budget_used_percent": 7.854078744870913
    },
    {
      "type": "training",
      "description": "Training step 1322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:45",
      "total_flops_so_far": 7860019802923008.0,
      "budget_used_percent": 7.860019802923008
    },
    {
      "type": "training",
      "description": "Training step 1323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:45",
      "total_flops_so_far": 7865960860975104.0,
      "budget_used_percent": 7.865960860975103
    },
    {
      "type": "training",
      "description": "Training step 1324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:46",
      "total_flops_so_far": 7871901919027200.0,
      "budget_used_percent": 7.8719019190271995
    },
    {
      "type": "training",
      "description": "Training step 1325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:46",
      "total_flops_so_far": 7877842977079296.0,
      "budget_used_percent": 7.877842977079295
    },
    {
      "type": "training",
      "description": "Training step 1326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:46",
      "total_flops_so_far": 7883784035131392.0,
      "budget_used_percent": 7.8837840351313915
    },
    {
      "type": "training",
      "description": "Training step 1327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:47",
      "total_flops_so_far": 7889725093183488.0,
      "budget_used_percent": 7.889725093183488
    },
    {
      "type": "training",
      "description": "Training step 1328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:47",
      "total_flops_so_far": 7895666151235584.0,
      "budget_used_percent": 7.8956661512355835
    },
    {
      "type": "training",
      "description": "Training step 1329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:47",
      "total_flops_so_far": 7901607209287680.0,
      "budget_used_percent": 7.90160720928768
    },
    {
      "type": "training",
      "description": "Training step 1330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:48",
      "total_flops_so_far": 7907548267339776.0,
      "budget_used_percent": 7.9075482673397754
    },
    {
      "type": "training",
      "description": "Training step 1331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:48",
      "total_flops_so_far": 7913489325391872.0,
      "budget_used_percent": 7.913489325391872
    },
    {
      "type": "training",
      "description": "Training step 1332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:48",
      "total_flops_so_far": 7919430383443968.0,
      "budget_used_percent": 7.919430383443967
    },
    {
      "type": "training",
      "description": "Training step 1333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:49",
      "total_flops_so_far": 7925371441496064.0,
      "budget_used_percent": 7.925371441496064
    },
    {
      "type": "training",
      "description": "Training step 1334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:49",
      "total_flops_so_far": 7931312499548160.0,
      "budget_used_percent": 7.931312499548159
    },
    {
      "type": "training",
      "description": "Training step 1335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:49",
      "total_flops_so_far": 7937253557600256.0,
      "budget_used_percent": 7.937253557600256
    },
    {
      "type": "training",
      "description": "Training step 1336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:50",
      "total_flops_so_far": 7943194615652352.0,
      "budget_used_percent": 7.943194615652352
    },
    {
      "type": "training",
      "description": "Training step 1337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:50",
      "total_flops_so_far": 7949135673704448.0,
      "budget_used_percent": 7.949135673704448
    },
    {
      "type": "training",
      "description": "Training step 1338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:50",
      "total_flops_so_far": 7955076731756544.0,
      "budget_used_percent": 7.955076731756544
    },
    {
      "type": "training",
      "description": "Training step 1339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:51",
      "total_flops_so_far": 7961017789808640.0,
      "budget_used_percent": 7.96101778980864
    },
    {
      "type": "training",
      "description": "Training step 1340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:51",
      "total_flops_so_far": 7966958847860736.0,
      "budget_used_percent": 7.966958847860736
    },
    {
      "type": "training",
      "description": "Training step 1341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:51",
      "total_flops_so_far": 7972899905912832.0,
      "budget_used_percent": 7.972899905912832
    },
    {
      "type": "training",
      "description": "Training step 1342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:52",
      "total_flops_so_far": 7978840963964928.0,
      "budget_used_percent": 7.978840963964928
    },
    {
      "type": "training",
      "description": "Training step 1343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:52",
      "total_flops_so_far": 7984782022017024.0,
      "budget_used_percent": 7.984782022017025
    },
    {
      "type": "training",
      "description": "Training step 1344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:52",
      "total_flops_so_far": 7990723080069120.0,
      "budget_used_percent": 7.99072308006912
    },
    {
      "type": "training",
      "description": "Training step 1345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:53",
      "total_flops_so_far": 7996664138121216.0,
      "budget_used_percent": 7.996664138121217
    },
    {
      "type": "training",
      "description": "Training step 1346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:53",
      "total_flops_so_far": 8002605196173312.0,
      "budget_used_percent": 8.002605196173313
    },
    {
      "type": "training",
      "description": "Training step 1347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:53",
      "total_flops_so_far": 8008546254225408.0,
      "budget_used_percent": 8.008546254225408
    },
    {
      "type": "training",
      "description": "Training step 1348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:54",
      "total_flops_so_far": 8014487312277504.0,
      "budget_used_percent": 8.014487312277504
    },
    {
      "type": "training",
      "description": "Training step 1349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:54",
      "total_flops_so_far": 8020428370329600.0,
      "budget_used_percent": 8.0204283703296
    },
    {
      "type": "training",
      "description": "Training step 1350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:54",
      "total_flops_so_far": 8026369428381696.0,
      "budget_used_percent": 8.026369428381697
    },
    {
      "type": "training",
      "description": "Training step 1351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:55",
      "total_flops_so_far": 8032310486433792.0,
      "budget_used_percent": 8.032310486433792
    },
    {
      "type": "training",
      "description": "Training step 1352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:55",
      "total_flops_so_far": 8038251544485888.0,
      "budget_used_percent": 8.038251544485888
    },
    {
      "type": "training",
      "description": "Training step 1353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:55",
      "total_flops_so_far": 8044192602537984.0,
      "budget_used_percent": 8.044192602537983
    },
    {
      "type": "training",
      "description": "Training step 1354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:56",
      "total_flops_so_far": 8050133660590080.0,
      "budget_used_percent": 8.05013366059008
    },
    {
      "type": "training",
      "description": "Training step 1355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:56",
      "total_flops_so_far": 8056074718642176.0,
      "budget_used_percent": 8.056074718642176
    },
    {
      "type": "training",
      "description": "Training step 1356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:56",
      "total_flops_so_far": 8062015776694272.0,
      "budget_used_percent": 8.062015776694272
    },
    {
      "type": "training",
      "description": "Training step 1357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:57",
      "total_flops_so_far": 8067956834746368.0,
      "budget_used_percent": 8.067956834746367
    },
    {
      "type": "training",
      "description": "Training step 1358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:57",
      "total_flops_so_far": 8073897892798464.0,
      "budget_used_percent": 8.073897892798463
    },
    {
      "type": "training",
      "description": "Training step 1359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:57",
      "total_flops_so_far": 8079838950850560.0,
      "budget_used_percent": 8.07983895085056
    },
    {
      "type": "training",
      "description": "Training step 1360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:58",
      "total_flops_so_far": 8085780008902656.0,
      "budget_used_percent": 8.085780008902656
    },
    {
      "type": "training",
      "description": "Training step 1361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:58",
      "total_flops_so_far": 8091721066954752.0,
      "budget_used_percent": 8.091721066954753
    },
    {
      "type": "training",
      "description": "Training step 1362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:58",
      "total_flops_so_far": 8097662125006848.0,
      "budget_used_percent": 8.097662125006847
    },
    {
      "type": "training",
      "description": "Training step 1363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:59",
      "total_flops_so_far": 8103603183058944.0,
      "budget_used_percent": 8.103603183058944
    },
    {
      "type": "training",
      "description": "Training step 1364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:59",
      "total_flops_so_far": 8109544241111040.0,
      "budget_used_percent": 8.10954424111104
    },
    {
      "type": "training",
      "description": "Training step 1365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:29:59",
      "total_flops_so_far": 8115485299163136.0,
      "budget_used_percent": 8.115485299163137
    },
    {
      "type": "training",
      "description": "Training step 1366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:00",
      "total_flops_so_far": 8121426357215232.0,
      "budget_used_percent": 8.121426357215231
    },
    {
      "type": "training",
      "description": "Training step 1367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:00",
      "total_flops_so_far": 8127367415267328.0,
      "budget_used_percent": 8.127367415267328
    },
    {
      "type": "training",
      "description": "Training step 1368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:00",
      "total_flops_so_far": 8133308473319424.0,
      "budget_used_percent": 8.133308473319424
    },
    {
      "type": "training",
      "description": "Training step 1369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:01",
      "total_flops_so_far": 8139249531371520.0,
      "budget_used_percent": 8.13924953137152
    },
    {
      "type": "training",
      "description": "Training step 1370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:01",
      "total_flops_so_far": 8145190589423616.0,
      "budget_used_percent": 8.145190589423617
    },
    {
      "type": "training",
      "description": "Training step 1371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:01",
      "total_flops_so_far": 8151131647475712.0,
      "budget_used_percent": 8.151131647475712
    },
    {
      "type": "training",
      "description": "Training step 1372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:02",
      "total_flops_so_far": 8157072705527808.0,
      "budget_used_percent": 8.157072705527808
    },
    {
      "type": "training",
      "description": "Training step 1373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:02",
      "total_flops_so_far": 8163013763579904.0,
      "budget_used_percent": 8.163013763579904
    },
    {
      "type": "training",
      "description": "Training step 1374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:02",
      "total_flops_so_far": 8168954821632000.0,
      "budget_used_percent": 8.168954821632001
    },
    {
      "type": "training",
      "description": "Training step 1375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:03",
      "total_flops_so_far": 8174895879684096.0,
      "budget_used_percent": 8.174895879684097
    },
    {
      "type": "training",
      "description": "Training step 1376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:03",
      "total_flops_so_far": 8180836937736192.0,
      "budget_used_percent": 8.180836937736192
    },
    {
      "type": "training",
      "description": "Training step 1377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:03",
      "total_flops_so_far": 8186777995788288.0,
      "budget_used_percent": 8.186777995788288
    },
    {
      "type": "training",
      "description": "Training step 1378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:04",
      "total_flops_so_far": 8192719053840384.0,
      "budget_used_percent": 8.192719053840385
    },
    {
      "type": "training",
      "description": "Training step 1379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:04",
      "total_flops_so_far": 8198660111892480.0,
      "budget_used_percent": 8.198660111892481
    },
    {
      "type": "training",
      "description": "Training step 1380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:04",
      "total_flops_so_far": 8204601169944576.0,
      "budget_used_percent": 8.204601169944576
    },
    {
      "type": "training",
      "description": "Training step 1381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:05",
      "total_flops_so_far": 8210542227996672.0,
      "budget_used_percent": 8.21054222799667
    },
    {
      "type": "training",
      "description": "Training step 1382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:05",
      "total_flops_so_far": 8216483286048768.0,
      "budget_used_percent": 8.216483286048767
    },
    {
      "type": "training",
      "description": "Training step 1383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:05",
      "total_flops_so_far": 8222424344100864.0,
      "budget_used_percent": 8.222424344100864
    },
    {
      "type": "training",
      "description": "Training step 1384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:06",
      "total_flops_so_far": 8228365402152960.0,
      "budget_used_percent": 8.22836540215296
    },
    {
      "type": "training",
      "description": "Training step 1385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:06",
      "total_flops_so_far": 8234306460205056.0,
      "budget_used_percent": 8.234306460205056
    },
    {
      "type": "training",
      "description": "Training step 1386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:06",
      "total_flops_so_far": 8240247518257152.0,
      "budget_used_percent": 8.240247518257151
    },
    {
      "type": "training",
      "description": "Training step 1387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:07",
      "total_flops_so_far": 8246188576309248.0,
      "budget_used_percent": 8.246188576309248
    },
    {
      "type": "training",
      "description": "Training step 1388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:07",
      "total_flops_so_far": 8252129634361344.0,
      "budget_used_percent": 8.252129634361344
    },
    {
      "type": "training",
      "description": "Training step 1389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:07",
      "total_flops_so_far": 8258070692413440.0,
      "budget_used_percent": 8.25807069241344
    },
    {
      "type": "training",
      "description": "Training step 1390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:08",
      "total_flops_so_far": 8264011750465536.0,
      "budget_used_percent": 8.264011750465535
    },
    {
      "type": "training",
      "description": "Training step 1391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:08",
      "total_flops_so_far": 8269952808517632.0,
      "budget_used_percent": 8.269952808517631
    },
    {
      "type": "training",
      "description": "Training step 1392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:08",
      "total_flops_so_far": 8275893866569728.0,
      "budget_used_percent": 8.275893866569728
    },
    {
      "type": "training",
      "description": "Training step 1393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:09",
      "total_flops_so_far": 8281834924621824.0,
      "budget_used_percent": 8.281834924621824
    },
    {
      "type": "training",
      "description": "Training step 1394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:09",
      "total_flops_so_far": 8287775982673920.0,
      "budget_used_percent": 8.28777598267392
    },
    {
      "type": "training",
      "description": "Training step 1395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:09",
      "total_flops_so_far": 8293717040726016.0,
      "budget_used_percent": 8.293717040726015
    },
    {
      "type": "training",
      "description": "Training step 1396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:10",
      "total_flops_so_far": 8299658098778112.0,
      "budget_used_percent": 8.299658098778112
    },
    {
      "type": "training",
      "description": "Training step 1397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:10",
      "total_flops_so_far": 8305599156830208.0,
      "budget_used_percent": 8.305599156830208
    },
    {
      "type": "training",
      "description": "Training step 1398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:10",
      "total_flops_so_far": 8311540214882304.0,
      "budget_used_percent": 8.311540214882305
    },
    {
      "type": "training",
      "description": "Training step 1399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:11",
      "total_flops_so_far": 8317481272934400.0,
      "budget_used_percent": 8.3174812729344
    },
    {
      "type": "training",
      "description": "Training step 1400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:11",
      "total_flops_so_far": 8323422330986496.0,
      "budget_used_percent": 8.323422330986496
    },
    {
      "type": "training",
      "description": "Training step 1401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:11",
      "total_flops_so_far": 8329363389038592.0,
      "budget_used_percent": 8.329363389038592
    },
    {
      "type": "training",
      "description": "Training step 1402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:12",
      "total_flops_so_far": 8335304447090688.0,
      "budget_used_percent": 8.335304447090689
    },
    {
      "type": "training",
      "description": "Training step 1403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:12",
      "total_flops_so_far": 8341245505142784.0,
      "budget_used_percent": 8.341245505142785
    },
    {
      "type": "training",
      "description": "Training step 1404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:12",
      "total_flops_so_far": 8347186563194880.0,
      "budget_used_percent": 8.34718656319488
    },
    {
      "type": "training",
      "description": "Training step 1405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:13",
      "total_flops_so_far": 8353127621246976.0,
      "budget_used_percent": 8.353127621246976
    },
    {
      "type": "training",
      "description": "Training step 1406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:13",
      "total_flops_so_far": 8359068679299072.0,
      "budget_used_percent": 8.359068679299073
    },
    {
      "type": "training",
      "description": "Training step 1407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:13",
      "total_flops_so_far": 8365009737351168.0,
      "budget_used_percent": 8.36500973735117
    },
    {
      "type": "training",
      "description": "Training step 1408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:14",
      "total_flops_so_far": 8370950795403264.0,
      "budget_used_percent": 8.370950795403264
    },
    {
      "type": "training",
      "description": "Training step 1409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:14",
      "total_flops_so_far": 8376891853455360.0,
      "budget_used_percent": 8.376891853455358
    },
    {
      "type": "training",
      "description": "Training step 1410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:14",
      "total_flops_so_far": 8382832911507456.0,
      "budget_used_percent": 8.382832911507455
    },
    {
      "type": "training",
      "description": "Training step 1411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:15",
      "total_flops_so_far": 8388773969559552.0,
      "budget_used_percent": 8.388773969559551
    },
    {
      "type": "training",
      "description": "Training step 1412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:15",
      "total_flops_so_far": 8394715027611648.0,
      "budget_used_percent": 8.394715027611648
    },
    {
      "type": "training",
      "description": "Training step 1413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:15",
      "total_flops_so_far": 8400656085663744.0,
      "budget_used_percent": 8.400656085663744
    },
    {
      "type": "training",
      "description": "Training step 1414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:16",
      "total_flops_so_far": 8406597143715840.0,
      "budget_used_percent": 8.406597143715839
    },
    {
      "type": "training",
      "description": "Training step 1415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:16",
      "total_flops_so_far": 8412538201767936.0,
      "budget_used_percent": 8.412538201767935
    },
    {
      "type": "training",
      "description": "Training step 1416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:16",
      "total_flops_so_far": 8418479259820032.0,
      "budget_used_percent": 8.418479259820032
    },
    {
      "type": "training",
      "description": "Training step 1417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:17",
      "total_flops_so_far": 8424420317872128.0,
      "budget_used_percent": 8.424420317872128
    },
    {
      "type": "training",
      "description": "Training step 1418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:17",
      "total_flops_so_far": 8430361375924224.0,
      "budget_used_percent": 8.430361375924225
    },
    {
      "type": "training",
      "description": "Training step 1419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:17",
      "total_flops_so_far": 8436302433976320.0,
      "budget_used_percent": 8.43630243397632
    },
    {
      "type": "training",
      "description": "Training step 1420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:18",
      "total_flops_so_far": 8442243492028416.0,
      "budget_used_percent": 8.442243492028416
    },
    {
      "type": "training",
      "description": "Training step 1421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:18",
      "total_flops_so_far": 8448184550080512.0,
      "budget_used_percent": 8.448184550080512
    },
    {
      "type": "training",
      "description": "Training step 1422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:18",
      "total_flops_so_far": 8454125608132608.0,
      "budget_used_percent": 8.454125608132609
    },
    {
      "type": "training",
      "description": "Training step 1423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:19",
      "total_flops_so_far": 8460066666184704.0,
      "budget_used_percent": 8.460066666184703
    },
    {
      "type": "training",
      "description": "Training step 1424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:19",
      "total_flops_so_far": 8466007724236800.0,
      "budget_used_percent": 8.4660077242368
    },
    {
      "type": "training",
      "description": "Training step 1425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:19",
      "total_flops_so_far": 8471948782288896.0,
      "budget_used_percent": 8.471948782288896
    },
    {
      "type": "training",
      "description": "Training step 1426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:20",
      "total_flops_so_far": 8477889840340992.0,
      "budget_used_percent": 8.477889840340993
    },
    {
      "type": "training",
      "description": "Training step 1427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:20",
      "total_flops_so_far": 8483830898393088.0,
      "budget_used_percent": 8.483830898393089
    },
    {
      "type": "training",
      "description": "Training step 1428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:20",
      "total_flops_so_far": 8489771956445184.0,
      "budget_used_percent": 8.489771956445184
    },
    {
      "type": "training",
      "description": "Training step 1429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:21",
      "total_flops_so_far": 8495713014497280.0,
      "budget_used_percent": 8.49571301449728
    },
    {
      "type": "training",
      "description": "Training step 1430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:21",
      "total_flops_so_far": 8501654072549376.0,
      "budget_used_percent": 8.501654072549377
    },
    {
      "type": "training",
      "description": "Training step 1431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:21",
      "total_flops_so_far": 8507595130601472.0,
      "budget_used_percent": 8.507595130601473
    },
    {
      "type": "training",
      "description": "Training step 1432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:22",
      "total_flops_so_far": 8513536188653568.0,
      "budget_used_percent": 8.513536188653568
    },
    {
      "type": "training",
      "description": "Training step 1433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:22",
      "total_flops_so_far": 8519477246705664.0,
      "budget_used_percent": 8.519477246705664
    },
    {
      "type": "training",
      "description": "Training step 1434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:22",
      "total_flops_so_far": 8525418304757760.0,
      "budget_used_percent": 8.52541830475776
    },
    {
      "type": "training",
      "description": "Training step 1435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:23",
      "total_flops_so_far": 8531359362809856.0,
      "budget_used_percent": 8.531359362809857
    },
    {
      "type": "training",
      "description": "Training step 1436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:23",
      "total_flops_so_far": 8537300420861952.0,
      "budget_used_percent": 8.537300420861953
    },
    {
      "type": "training",
      "description": "Training step 1437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:23",
      "total_flops_so_far": 8543241478914048.0,
      "budget_used_percent": 8.543241478914048
    },
    {
      "type": "training",
      "description": "Training step 1438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:24",
      "total_flops_so_far": 8549182536966144.0,
      "budget_used_percent": 8.549182536966143
    },
    {
      "type": "training",
      "description": "Training step 1439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:24",
      "total_flops_so_far": 8555123595018240.0,
      "budget_used_percent": 8.55512359501824
    },
    {
      "type": "training",
      "description": "Training step 1440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:24",
      "total_flops_so_far": 8561064653070336.0,
      "budget_used_percent": 8.561064653070336
    },
    {
      "type": "training",
      "description": "Training step 1441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:25",
      "total_flops_so_far": 8567005711122432.0,
      "budget_used_percent": 8.567005711122432
    },
    {
      "type": "training",
      "description": "Training step 1442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:25",
      "total_flops_so_far": 8572946769174528.0,
      "budget_used_percent": 8.572946769174528
    },
    {
      "type": "training",
      "description": "Training step 1443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:25",
      "total_flops_so_far": 8578887827226624.0,
      "budget_used_percent": 8.578887827226623
    },
    {
      "type": "training",
      "description": "Training step 1444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:26",
      "total_flops_so_far": 8584828885278720.0,
      "budget_used_percent": 8.58482888527872
    },
    {
      "type": "training",
      "description": "Training step 1445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:26",
      "total_flops_so_far": 8590769943330816.0,
      "budget_used_percent": 8.590769943330816
    },
    {
      "type": "training",
      "description": "Training step 1446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:26",
      "total_flops_so_far": 8596711001382912.0,
      "budget_used_percent": 8.596711001382912
    },
    {
      "type": "training",
      "description": "Training step 1447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:27",
      "total_flops_so_far": 8602652059435008.0,
      "budget_used_percent": 8.602652059435007
    },
    {
      "type": "training",
      "description": "Training step 1448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:27",
      "total_flops_so_far": 8608593117487104.0,
      "budget_used_percent": 8.608593117487104
    },
    {
      "type": "training",
      "description": "Training step 1449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:27",
      "total_flops_so_far": 8614534175539200.0,
      "budget_used_percent": 8.6145341755392
    },
    {
      "type": "training",
      "description": "Training step 1450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:28",
      "total_flops_so_far": 8620475233591296.0,
      "budget_used_percent": 8.620475233591296
    },
    {
      "type": "training",
      "description": "Training step 1451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:28",
      "total_flops_so_far": 8626416291643392.0,
      "budget_used_percent": 8.626416291643393
    },
    {
      "type": "training",
      "description": "Training step 1452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:28",
      "total_flops_so_far": 8632357349695488.0,
      "budget_used_percent": 8.632357349695488
    },
    {
      "type": "training",
      "description": "Training step 1453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:29",
      "total_flops_so_far": 8638298407747584.0,
      "budget_used_percent": 8.638298407747584
    },
    {
      "type": "training",
      "description": "Training step 1454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:29",
      "total_flops_so_far": 8644239465799680.0,
      "budget_used_percent": 8.64423946579968
    },
    {
      "type": "training",
      "description": "Training step 1455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:29",
      "total_flops_so_far": 8650180523851776.0,
      "budget_used_percent": 8.650180523851777
    },
    {
      "type": "training",
      "description": "Training step 1456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:30",
      "total_flops_so_far": 8656121581903872.0,
      "budget_used_percent": 8.656121581903871
    },
    {
      "type": "training",
      "description": "Training step 1457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:30",
      "total_flops_so_far": 8662062639955968.0,
      "budget_used_percent": 8.662062639955968
    },
    {
      "type": "training",
      "description": "Training step 1458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:30",
      "total_flops_so_far": 8668003698008064.0,
      "budget_used_percent": 8.668003698008064
    },
    {
      "type": "training",
      "description": "Training step 1459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:31",
      "total_flops_so_far": 8673944756060160.0,
      "budget_used_percent": 8.67394475606016
    },
    {
      "type": "training",
      "description": "Training step 1460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:31",
      "total_flops_so_far": 8679885814112256.0,
      "budget_used_percent": 8.679885814112257
    },
    {
      "type": "training",
      "description": "Training step 1461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:31",
      "total_flops_so_far": 8685826872164352.0,
      "budget_used_percent": 8.685826872164352
    },
    {
      "type": "training",
      "description": "Training step 1462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:32",
      "total_flops_so_far": 8691767930216448.0,
      "budget_used_percent": 8.691767930216448
    },
    {
      "type": "training",
      "description": "Training step 1463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:32",
      "total_flops_so_far": 8697708988268544.0,
      "budget_used_percent": 8.697708988268545
    },
    {
      "type": "training",
      "description": "Training step 1464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:32",
      "total_flops_so_far": 8703650046320640.0,
      "budget_used_percent": 8.703650046320641
    },
    {
      "type": "training",
      "description": "Training step 1465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:33",
      "total_flops_so_far": 8709591104372736.0,
      "budget_used_percent": 8.709591104372736
    },
    {
      "type": "training",
      "description": "Training step 1466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:33",
      "total_flops_so_far": 8715532162424832.0,
      "budget_used_percent": 8.71553216242483
    },
    {
      "type": "training",
      "description": "Training step 1467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:33",
      "total_flops_so_far": 8721473220476928.0,
      "budget_used_percent": 8.721473220476927
    },
    {
      "type": "training",
      "description": "Training step 1468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:34",
      "total_flops_so_far": 8727414278529024.0,
      "budget_used_percent": 8.727414278529023
    },
    {
      "type": "training",
      "description": "Training step 1469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:34",
      "total_flops_so_far": 8733355336581120.0,
      "budget_used_percent": 8.73335533658112
    },
    {
      "type": "training",
      "description": "Training step 1470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:34",
      "total_flops_so_far": 8739296394633216.0,
      "budget_used_percent": 8.739296394633216
    },
    {
      "type": "training",
      "description": "Training step 1471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:35",
      "total_flops_so_far": 8745237452685312.0,
      "budget_used_percent": 8.745237452685311
    },
    {
      "type": "training",
      "description": "Training step 1472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:35",
      "total_flops_so_far": 8751178510737408.0,
      "budget_used_percent": 8.751178510737407
    },
    {
      "type": "training",
      "description": "Training step 1473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:35",
      "total_flops_so_far": 8757119568789504.0,
      "budget_used_percent": 8.757119568789504
    },
    {
      "type": "training",
      "description": "Training step 1474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:36",
      "total_flops_so_far": 8763060626841600.0,
      "budget_used_percent": 8.7630606268416
    },
    {
      "type": "training",
      "description": "Training step 1475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:36",
      "total_flops_so_far": 8769001684893696.0,
      "budget_used_percent": 8.769001684893695
    },
    {
      "type": "training",
      "description": "Training step 1476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:36",
      "total_flops_so_far": 8774942742945792.0,
      "budget_used_percent": 8.774942742945791
    },
    {
      "type": "training",
      "description": "Training step 1477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:37",
      "total_flops_so_far": 8780883800997888.0,
      "budget_used_percent": 8.780883800997888
    },
    {
      "type": "training",
      "description": "Training step 1478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:37",
      "total_flops_so_far": 8786824859049984.0,
      "budget_used_percent": 8.786824859049984
    },
    {
      "type": "training",
      "description": "Training step 1479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:38",
      "total_flops_so_far": 8792765917102080.0,
      "budget_used_percent": 8.79276591710208
    },
    {
      "type": "training",
      "description": "Training step 1480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:38",
      "total_flops_so_far": 8798706975154176.0,
      "budget_used_percent": 8.798706975154175
    },
    {
      "type": "training",
      "description": "Training step 1481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:38",
      "total_flops_so_far": 8804648033206272.0,
      "budget_used_percent": 8.804648033206272
    },
    {
      "type": "training",
      "description": "Training step 1482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:39",
      "total_flops_so_far": 8810589091258368.0,
      "budget_used_percent": 8.810589091258368
    },
    {
      "type": "training",
      "description": "Training step 1483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:39",
      "total_flops_so_far": 8816530149310464.0,
      "budget_used_percent": 8.816530149310465
    },
    {
      "type": "training",
      "description": "Training step 1484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:39",
      "total_flops_so_far": 8822471207362560.0,
      "budget_used_percent": 8.822471207362561
    },
    {
      "type": "training",
      "description": "Training step 1485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:40",
      "total_flops_so_far": 8828412265414656.0,
      "budget_used_percent": 8.828412265414656
    },
    {
      "type": "training",
      "description": "Training step 1486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:40",
      "total_flops_so_far": 8834353323466752.0,
      "budget_used_percent": 8.834353323466752
    },
    {
      "type": "training",
      "description": "Training step 1487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:40",
      "total_flops_so_far": 8840294381518848.0,
      "budget_used_percent": 8.840294381518849
    },
    {
      "type": "training",
      "description": "Training step 1488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:41",
      "total_flops_so_far": 8846235439570944.0,
      "budget_used_percent": 8.846235439570945
    },
    {
      "type": "training",
      "description": "Training step 1489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:41",
      "total_flops_so_far": 8852176497623040.0,
      "budget_used_percent": 8.85217649762304
    },
    {
      "type": "training",
      "description": "Training step 1490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:41",
      "total_flops_so_far": 8858117555675136.0,
      "budget_used_percent": 8.858117555675136
    },
    {
      "type": "training",
      "description": "Training step 1491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:42",
      "total_flops_so_far": 8864058613727232.0,
      "budget_used_percent": 8.864058613727233
    },
    {
      "type": "training",
      "description": "Training step 1492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:42",
      "total_flops_so_far": 8869999671779328.0,
      "budget_used_percent": 8.869999671779329
    },
    {
      "type": "training",
      "description": "Training step 1493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:42",
      "total_flops_so_far": 8875940729831424.0,
      "budget_used_percent": 8.875940729831425
    },
    {
      "type": "training",
      "description": "Training step 1494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:43",
      "total_flops_so_far": 8881881787883520.0,
      "budget_used_percent": 8.88188178788352
    },
    {
      "type": "training",
      "description": "Training step 1495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:43",
      "total_flops_so_far": 8887822845935616.0,
      "budget_used_percent": 8.887822845935615
    },
    {
      "type": "training",
      "description": "Training step 1496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:43",
      "total_flops_so_far": 8893763903987712.0,
      "budget_used_percent": 8.893763903987711
    },
    {
      "type": "training",
      "description": "Training step 1497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:44",
      "total_flops_so_far": 8899704962039808.0,
      "budget_used_percent": 8.899704962039808
    },
    {
      "type": "training",
      "description": "Training step 1498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:44",
      "total_flops_so_far": 8905646020091904.0,
      "budget_used_percent": 8.905646020091904
    },
    {
      "type": "training",
      "description": "Training step 1499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:30:44",
      "total_flops_so_far": 8911587078144000.0,
      "budget_used_percent": 8.911587078143999
    },
    {
      "type": "training",
      "description": "Training step 1500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:08",
      "total_flops_so_far": 8917528136196096.0,
      "budget_used_percent": 8.917528136196095
    },
    {
      "type": "training",
      "description": "Training step 1501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:08",
      "total_flops_so_far": 8923469194248192.0,
      "budget_used_percent": 8.923469194248192
    },
    {
      "type": "training",
      "description": "Training step 1502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:08",
      "total_flops_so_far": 8929410252300288.0,
      "budget_used_percent": 8.929410252300288
    },
    {
      "type": "training",
      "description": "Training step 1503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:09",
      "total_flops_so_far": 8935351310352384.0,
      "budget_used_percent": 8.935351310352385
    },
    {
      "type": "training",
      "description": "Training step 1504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:09",
      "total_flops_so_far": 8941292368404480.0,
      "budget_used_percent": 8.94129236840448
    },
    {
      "type": "training",
      "description": "Training step 1505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:09",
      "total_flops_so_far": 8947233426456576.0,
      "budget_used_percent": 8.947233426456576
    },
    {
      "type": "training",
      "description": "Training step 1506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:10",
      "total_flops_so_far": 8953174484508672.0,
      "budget_used_percent": 8.953174484508672
    },
    {
      "type": "training",
      "description": "Training step 1507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:10",
      "total_flops_so_far": 8959115542560768.0,
      "budget_used_percent": 8.959115542560768
    },
    {
      "type": "training",
      "description": "Training step 1508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:10",
      "total_flops_so_far": 8965056600612864.0,
      "budget_used_percent": 8.965056600612865
    },
    {
      "type": "training",
      "description": "Training step 1509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:11",
      "total_flops_so_far": 8970997658664960.0,
      "budget_used_percent": 8.97099765866496
    },
    {
      "type": "training",
      "description": "Training step 1510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:11",
      "total_flops_so_far": 8976938716717056.0,
      "budget_used_percent": 8.976938716717056
    },
    {
      "type": "training",
      "description": "Training step 1511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:11",
      "total_flops_so_far": 8982879774769152.0,
      "budget_used_percent": 8.982879774769152
    },
    {
      "type": "training",
      "description": "Training step 1512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:12",
      "total_flops_so_far": 8988820832821248.0,
      "budget_used_percent": 8.988820832821249
    },
    {
      "type": "training",
      "description": "Training step 1513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:12",
      "total_flops_so_far": 8994761890873344.0,
      "budget_used_percent": 8.994761890873344
    },
    {
      "type": "training",
      "description": "Training step 1514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:12",
      "total_flops_so_far": 9000702948925440.0,
      "budget_used_percent": 9.00070294892544
    },
    {
      "type": "training",
      "description": "Training step 1515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:13",
      "total_flops_so_far": 9006644006977536.0,
      "budget_used_percent": 9.006644006977536
    },
    {
      "type": "training",
      "description": "Training step 1516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:13",
      "total_flops_so_far": 9012585065029632.0,
      "budget_used_percent": 9.012585065029633
    },
    {
      "type": "training",
      "description": "Training step 1517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:13",
      "total_flops_so_far": 9018526123081728.0,
      "budget_used_percent": 9.01852612308173
    },
    {
      "type": "training",
      "description": "Training step 1518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:14",
      "total_flops_so_far": 9024467181133824.0,
      "budget_used_percent": 9.024467181133824
    },
    {
      "type": "training",
      "description": "Training step 1519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:14",
      "total_flops_so_far": 9030408239185920.0,
      "budget_used_percent": 9.03040823918592
    },
    {
      "type": "training",
      "description": "Training step 1520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:14",
      "total_flops_so_far": 9036349297238016.0,
      "budget_used_percent": 9.036349297238017
    },
    {
      "type": "training",
      "description": "Training step 1521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:15",
      "total_flops_so_far": 9042290355290112.0,
      "budget_used_percent": 9.042290355290113
    },
    {
      "type": "training",
      "description": "Training step 1522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:15",
      "total_flops_so_far": 9048231413342208.0,
      "budget_used_percent": 9.048231413342208
    },
    {
      "type": "training",
      "description": "Training step 1523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:15",
      "total_flops_so_far": 9054172471394304.0,
      "budget_used_percent": 9.054172471394303
    },
    {
      "type": "training",
      "description": "Training step 1524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:16",
      "total_flops_so_far": 9060113529446400.0,
      "budget_used_percent": 9.060113529446399
    },
    {
      "type": "training",
      "description": "Training step 1525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:16",
      "total_flops_so_far": 9066054587498496.0,
      "budget_used_percent": 9.066054587498495
    },
    {
      "type": "training",
      "description": "Training step 1526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:17",
      "total_flops_so_far": 9071995645550592.0,
      "budget_used_percent": 9.071995645550592
    },
    {
      "type": "training",
      "description": "Training step 1527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:17",
      "total_flops_so_far": 9077936703602688.0,
      "budget_used_percent": 9.077936703602688
    },
    {
      "type": "training",
      "description": "Training step 1528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:17",
      "total_flops_so_far": 9083877761654784.0,
      "budget_used_percent": 9.083877761654783
    },
    {
      "type": "training",
      "description": "Training step 1529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:18",
      "total_flops_so_far": 9089818819706880.0,
      "budget_used_percent": 9.08981881970688
    },
    {
      "type": "training",
      "description": "Training step 1530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:18",
      "total_flops_so_far": 9095759877758976.0,
      "budget_used_percent": 9.095759877758976
    },
    {
      "type": "training",
      "description": "Training step 1531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:18",
      "total_flops_so_far": 9101700935811072.0,
      "budget_used_percent": 9.101700935811072
    },
    {
      "type": "training",
      "description": "Training step 1532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:19",
      "total_flops_so_far": 9107641993863168.0,
      "budget_used_percent": 9.107641993863167
    },
    {
      "type": "training",
      "description": "Training step 1533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:19",
      "total_flops_so_far": 9113583051915264.0,
      "budget_used_percent": 9.113583051915263
    },
    {
      "type": "training",
      "description": "Training step 1534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:19",
      "total_flops_so_far": 9119524109967360.0,
      "budget_used_percent": 9.11952410996736
    },
    {
      "type": "training",
      "description": "Training step 1535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:20",
      "total_flops_so_far": 9125465168019456.0,
      "budget_used_percent": 9.125465168019456
    },
    {
      "type": "training",
      "description": "Training step 1536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:20",
      "total_flops_so_far": 9131406226071552.0,
      "budget_used_percent": 9.131406226071553
    },
    {
      "type": "training",
      "description": "Training step 1537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:20",
      "total_flops_so_far": 9137347284123648.0,
      "budget_used_percent": 9.137347284123647
    },
    {
      "type": "training",
      "description": "Training step 1538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:21",
      "total_flops_so_far": 9143288342175744.0,
      "budget_used_percent": 9.143288342175744
    },
    {
      "type": "training",
      "description": "Training step 1539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:21",
      "total_flops_so_far": 9149229400227840.0,
      "budget_used_percent": 9.14922940022784
    },
    {
      "type": "training",
      "description": "Training step 1540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:21",
      "total_flops_so_far": 9155170458279936.0,
      "budget_used_percent": 9.155170458279937
    },
    {
      "type": "training",
      "description": "Training step 1541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:22",
      "total_flops_so_far": 9161111516332032.0,
      "budget_used_percent": 9.161111516332031
    },
    {
      "type": "training",
      "description": "Training step 1542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:22",
      "total_flops_so_far": 9167052574384128.0,
      "budget_used_percent": 9.167052574384128
    },
    {
      "type": "training",
      "description": "Training step 1543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:22",
      "total_flops_so_far": 9172993632436224.0,
      "budget_used_percent": 9.172993632436224
    },
    {
      "type": "training",
      "description": "Training step 1544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:23",
      "total_flops_so_far": 9178934690488320.0,
      "budget_used_percent": 9.17893469048832
    },
    {
      "type": "training",
      "description": "Training step 1545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:23",
      "total_flops_so_far": 9184875748540416.0,
      "budget_used_percent": 9.184875748540417
    },
    {
      "type": "training",
      "description": "Training step 1546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:23",
      "total_flops_so_far": 9190816806592512.0,
      "budget_used_percent": 9.190816806592512
    },
    {
      "type": "training",
      "description": "Training step 1547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:24",
      "total_flops_so_far": 9196757864644608.0,
      "budget_used_percent": 9.196757864644608
    },
    {
      "type": "training",
      "description": "Training step 1548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:24",
      "total_flops_so_far": 9202698922696704.0,
      "budget_used_percent": 9.202698922696705
    },
    {
      "type": "training",
      "description": "Training step 1549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:25",
      "total_flops_so_far": 9208639980748800.0,
      "budget_used_percent": 9.208639980748801
    },
    {
      "type": "training",
      "description": "Training step 1550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:25",
      "total_flops_so_far": 9214581038800896.0,
      "budget_used_percent": 9.214581038800898
    },
    {
      "type": "training",
      "description": "Training step 1551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:25",
      "total_flops_so_far": 9220522096852992.0,
      "budget_used_percent": 9.220522096852992
    },
    {
      "type": "training",
      "description": "Training step 1552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:26",
      "total_flops_so_far": 9226463154905088.0,
      "budget_used_percent": 9.226463154905087
    },
    {
      "type": "training",
      "description": "Training step 1553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:26",
      "total_flops_so_far": 9232404212957184.0,
      "budget_used_percent": 9.232404212957183
    },
    {
      "type": "training",
      "description": "Training step 1554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:26",
      "total_flops_so_far": 9238345271009280.0,
      "budget_used_percent": 9.23834527100928
    },
    {
      "type": "training",
      "description": "Training step 1555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:27",
      "total_flops_so_far": 9244286329061376.0,
      "budget_used_percent": 9.244286329061376
    },
    {
      "type": "training",
      "description": "Training step 1556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:27",
      "total_flops_so_far": 9250227387113472.0,
      "budget_used_percent": 9.25022738711347
    },
    {
      "type": "training",
      "description": "Training step 1557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:27",
      "total_flops_so_far": 9256168445165568.0,
      "budget_used_percent": 9.256168445165567
    },
    {
      "type": "training",
      "description": "Training step 1558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:28",
      "total_flops_so_far": 9262109503217664.0,
      "budget_used_percent": 9.262109503217664
    },
    {
      "type": "training",
      "description": "Training step 1559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:28",
      "total_flops_so_far": 9268050561269760.0,
      "budget_used_percent": 9.26805056126976
    },
    {
      "type": "training",
      "description": "Training step 1560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:28",
      "total_flops_so_far": 9273991619321856.0,
      "budget_used_percent": 9.273991619321857
    },
    {
      "type": "training",
      "description": "Training step 1561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:29",
      "total_flops_so_far": 9279932677373952.0,
      "budget_used_percent": 9.279932677373951
    },
    {
      "type": "training",
      "description": "Training step 1562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:29",
      "total_flops_so_far": 9285873735426048.0,
      "budget_used_percent": 9.285873735426048
    },
    {
      "type": "training",
      "description": "Training step 1563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:29",
      "total_flops_so_far": 9291814793478144.0,
      "budget_used_percent": 9.291814793478144
    },
    {
      "type": "training",
      "description": "Training step 1564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:30",
      "total_flops_so_far": 9297755851530240.0,
      "budget_used_percent": 9.29775585153024
    },
    {
      "type": "training",
      "description": "Training step 1565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:30",
      "total_flops_so_far": 9303696909582336.0,
      "budget_used_percent": 9.303696909582335
    },
    {
      "type": "training",
      "description": "Training step 1566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:30",
      "total_flops_so_far": 9309637967634432.0,
      "budget_used_percent": 9.309637967634432
    },
    {
      "type": "training",
      "description": "Training step 1567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:31",
      "total_flops_so_far": 9315579025686528.0,
      "budget_used_percent": 9.315579025686528
    },
    {
      "type": "training",
      "description": "Training step 1568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:31",
      "total_flops_so_far": 9321520083738624.0,
      "budget_used_percent": 9.321520083738625
    },
    {
      "type": "training",
      "description": "Training step 1569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:31",
      "total_flops_so_far": 9327461141790720.0,
      "budget_used_percent": 9.327461141790721
    },
    {
      "type": "training",
      "description": "Training step 1570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:32",
      "total_flops_so_far": 9333402199842816.0,
      "budget_used_percent": 9.333402199842816
    },
    {
      "type": "training",
      "description": "Training step 1571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:32",
      "total_flops_so_far": 9339343257894912.0,
      "budget_used_percent": 9.339343257894912
    },
    {
      "type": "training",
      "description": "Training step 1572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:32",
      "total_flops_so_far": 9345284315947008.0,
      "budget_used_percent": 9.345284315947008
    },
    {
      "type": "training",
      "description": "Training step 1573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:33",
      "total_flops_so_far": 9351225373999104.0,
      "budget_used_percent": 9.351225373999105
    },
    {
      "type": "training",
      "description": "Training step 1574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:33",
      "total_flops_so_far": 9357166432051200.0,
      "budget_used_percent": 9.357166432051201
    },
    {
      "type": "training",
      "description": "Training step 1575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:33",
      "total_flops_so_far": 9363107490103296.0,
      "budget_used_percent": 9.363107490103296
    },
    {
      "type": "training",
      "description": "Training step 1576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:34",
      "total_flops_so_far": 9369048548155392.0,
      "budget_used_percent": 9.369048548155392
    },
    {
      "type": "training",
      "description": "Training step 1577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:34",
      "total_flops_so_far": 9374989606207488.0,
      "budget_used_percent": 9.374989606207489
    },
    {
      "type": "training",
      "description": "Training step 1578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:34",
      "total_flops_so_far": 9380930664259584.0,
      "budget_used_percent": 9.380930664259585
    },
    {
      "type": "training",
      "description": "Training step 1579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:35",
      "total_flops_so_far": 9386871722311680.0,
      "budget_used_percent": 9.38687172231168
    },
    {
      "type": "training",
      "description": "Training step 1580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:35",
      "total_flops_so_far": 9392812780363776.0,
      "budget_used_percent": 9.392812780363775
    },
    {
      "type": "training",
      "description": "Training step 1581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:35",
      "total_flops_so_far": 9398753838415872.0,
      "budget_used_percent": 9.398753838415871
    },
    {
      "type": "training",
      "description": "Training step 1582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:36",
      "total_flops_so_far": 9404694896467968.0,
      "budget_used_percent": 9.404694896467968
    },
    {
      "type": "training",
      "description": "Training step 1583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:36",
      "total_flops_so_far": 9410635954520064.0,
      "budget_used_percent": 9.410635954520064
    },
    {
      "type": "training",
      "description": "Training step 1584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:36",
      "total_flops_so_far": 9416577012572160.0,
      "budget_used_percent": 9.41657701257216
    },
    {
      "type": "training",
      "description": "Training step 1585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:37",
      "total_flops_so_far": 9422518070624256.0,
      "budget_used_percent": 9.422518070624255
    },
    {
      "type": "training",
      "description": "Training step 1586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:37",
      "total_flops_so_far": 9428459128676352.0,
      "budget_used_percent": 9.428459128676351
    },
    {
      "type": "training",
      "description": "Training step 1587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:38",
      "total_flops_so_far": 9434400186728448.0,
      "budget_used_percent": 9.434400186728448
    },
    {
      "type": "training",
      "description": "Training step 1588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:38",
      "total_flops_so_far": 9440341244780544.0,
      "budget_used_percent": 9.440341244780544
    },
    {
      "type": "training",
      "description": "Training step 1589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:38",
      "total_flops_so_far": 9446282302832640.0,
      "budget_used_percent": 9.446282302832639
    },
    {
      "type": "training",
      "description": "Training step 1590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:39",
      "total_flops_so_far": 9452223360884736.0,
      "budget_used_percent": 9.452223360884735
    },
    {
      "type": "training",
      "description": "Training step 1591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:39",
      "total_flops_so_far": 9458164418936832.0,
      "budget_used_percent": 9.458164418936832
    },
    {
      "type": "training",
      "description": "Training step 1592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:39",
      "total_flops_so_far": 9464105476988928.0,
      "budget_used_percent": 9.464105476988928
    },
    {
      "type": "training",
      "description": "Training step 1593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:40",
      "total_flops_so_far": 9470046535041024.0,
      "budget_used_percent": 9.470046535041025
    },
    {
      "type": "training",
      "description": "Training step 1594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:40",
      "total_flops_so_far": 9475987593093120.0,
      "budget_used_percent": 9.47598759309312
    },
    {
      "type": "training",
      "description": "Training step 1595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:40",
      "total_flops_so_far": 9481928651145216.0,
      "budget_used_percent": 9.481928651145216
    },
    {
      "type": "training",
      "description": "Training step 1596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:41",
      "total_flops_so_far": 9487869709197312.0,
      "budget_used_percent": 9.487869709197312
    },
    {
      "type": "training",
      "description": "Training step 1597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:41",
      "total_flops_so_far": 9493810767249408.0,
      "budget_used_percent": 9.493810767249409
    },
    {
      "type": "training",
      "description": "Training step 1598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:41",
      "total_flops_so_far": 9499751825301504.0,
      "budget_used_percent": 9.499751825301503
    },
    {
      "type": "training",
      "description": "Training step 1599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:42",
      "total_flops_so_far": 9505692883353600.0,
      "budget_used_percent": 9.5056928833536
    },
    {
      "type": "training",
      "description": "Training step 1600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:42",
      "total_flops_so_far": 9511633941405696.0,
      "budget_used_percent": 9.511633941405696
    },
    {
      "type": "training",
      "description": "Training step 1601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:42",
      "total_flops_so_far": 9517574999457792.0,
      "budget_used_percent": 9.517574999457793
    },
    {
      "type": "training",
      "description": "Training step 1602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:43",
      "total_flops_so_far": 9523516057509888.0,
      "budget_used_percent": 9.52351605750989
    },
    {
      "type": "training",
      "description": "Training step 1603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:43",
      "total_flops_so_far": 9529457115561984.0,
      "budget_used_percent": 9.529457115561984
    },
    {
      "type": "training",
      "description": "Training step 1604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:43",
      "total_flops_so_far": 9535398173614080.0,
      "budget_used_percent": 9.53539817361408
    },
    {
      "type": "training",
      "description": "Training step 1605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:44",
      "total_flops_so_far": 9541339231666176.0,
      "budget_used_percent": 9.541339231666177
    },
    {
      "type": "training",
      "description": "Training step 1606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:44",
      "total_flops_so_far": 9547280289718272.0,
      "budget_used_percent": 9.547280289718273
    },
    {
      "type": "training",
      "description": "Training step 1607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:44",
      "total_flops_so_far": 9553221347770368.0,
      "budget_used_percent": 9.553221347770368
    },
    {
      "type": "training",
      "description": "Training step 1608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:45",
      "total_flops_so_far": 9559162405822464.0,
      "budget_used_percent": 9.559162405822462
    },
    {
      "type": "training",
      "description": "Training step 1609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:45",
      "total_flops_so_far": 9565103463874560.0,
      "budget_used_percent": 9.565103463874559
    },
    {
      "type": "training",
      "description": "Training step 1610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:45",
      "total_flops_so_far": 9571044521926656.0,
      "budget_used_percent": 9.571044521926655
    },
    {
      "type": "training",
      "description": "Training step 1611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:46",
      "total_flops_so_far": 9576985579978752.0,
      "budget_used_percent": 9.576985579978752
    },
    {
      "type": "training",
      "description": "Training step 1612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:46",
      "total_flops_so_far": 9582926638030848.0,
      "budget_used_percent": 9.582926638030848
    },
    {
      "type": "training",
      "description": "Training step 1613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:46",
      "total_flops_so_far": 9588867696082944.0,
      "budget_used_percent": 9.588867696082943
    },
    {
      "type": "training",
      "description": "Training step 1614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:47",
      "total_flops_so_far": 9594808754135040.0,
      "budget_used_percent": 9.59480875413504
    },
    {
      "type": "training",
      "description": "Training step 1615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:47",
      "total_flops_so_far": 9600749812187136.0,
      "budget_used_percent": 9.600749812187136
    },
    {
      "type": "training",
      "description": "Training step 1616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:47",
      "total_flops_so_far": 9606690870239232.0,
      "budget_used_percent": 9.606690870239232
    },
    {
      "type": "training",
      "description": "Training step 1617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:48",
      "total_flops_so_far": 9612631928291328.0,
      "budget_used_percent": 9.612631928291329
    },
    {
      "type": "training",
      "description": "Training step 1618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:48",
      "total_flops_so_far": 9618572986343424.0,
      "budget_used_percent": 9.618572986343423
    },
    {
      "type": "training",
      "description": "Training step 1619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:48",
      "total_flops_so_far": 9624514044395520.0,
      "budget_used_percent": 9.62451404439552
    },
    {
      "type": "training",
      "description": "Training step 1620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:49",
      "total_flops_so_far": 9630455102447616.0,
      "budget_used_percent": 9.630455102447616
    },
    {
      "type": "training",
      "description": "Training step 1621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:49",
      "total_flops_so_far": 9636396160499712.0,
      "budget_used_percent": 9.636396160499713
    },
    {
      "type": "training",
      "description": "Training step 1622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:49",
      "total_flops_so_far": 9642337218551808.0,
      "budget_used_percent": 9.642337218551807
    },
    {
      "type": "training",
      "description": "Training step 1623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:50",
      "total_flops_so_far": 9648278276603904.0,
      "budget_used_percent": 9.648278276603904
    },
    {
      "type": "training",
      "description": "Training step 1624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:50",
      "total_flops_so_far": 9654219334656000.0,
      "budget_used_percent": 9.654219334656
    },
    {
      "type": "training",
      "description": "Training step 1625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:50",
      "total_flops_so_far": 9660160392708096.0,
      "budget_used_percent": 9.660160392708097
    },
    {
      "type": "training",
      "description": "Training step 1626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:51",
      "total_flops_so_far": 9666101450760192.0,
      "budget_used_percent": 9.666101450760193
    },
    {
      "type": "training",
      "description": "Training step 1627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:51",
      "total_flops_so_far": 9672042508812288.0,
      "budget_used_percent": 9.672042508812288
    },
    {
      "type": "training",
      "description": "Training step 1628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:51",
      "total_flops_so_far": 9677983566864384.0,
      "budget_used_percent": 9.677983566864384
    },
    {
      "type": "training",
      "description": "Training step 1629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:52",
      "total_flops_so_far": 9683924624916480.0,
      "budget_used_percent": 9.68392462491648
    },
    {
      "type": "training",
      "description": "Training step 1630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:52",
      "total_flops_so_far": 9689865682968576.0,
      "budget_used_percent": 9.689865682968577
    },
    {
      "type": "training",
      "description": "Training step 1631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:53",
      "total_flops_so_far": 9695806741020672.0,
      "budget_used_percent": 9.695806741020672
    },
    {
      "type": "training",
      "description": "Training step 1632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:53",
      "total_flops_so_far": 9701747799072768.0,
      "budget_used_percent": 9.701747799072768
    },
    {
      "type": "training",
      "description": "Training step 1633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:53",
      "total_flops_so_far": 9707688857124864.0,
      "budget_used_percent": 9.707688857124865
    },
    {
      "type": "training",
      "description": "Training step 1634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:54",
      "total_flops_so_far": 9713629915176960.0,
      "budget_used_percent": 9.713629915176961
    },
    {
      "type": "training",
      "description": "Training step 1635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:54",
      "total_flops_so_far": 9719570973229056.0,
      "budget_used_percent": 9.719570973229057
    },
    {
      "type": "training",
      "description": "Training step 1636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:54",
      "total_flops_so_far": 9725512031281152.0,
      "budget_used_percent": 9.725512031281152
    },
    {
      "type": "training",
      "description": "Training step 1637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:55",
      "total_flops_so_far": 9731453089333248.0,
      "budget_used_percent": 9.731453089333247
    },
    {
      "type": "training",
      "description": "Training step 1638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:55",
      "total_flops_so_far": 9737394147385344.0,
      "budget_used_percent": 9.737394147385343
    },
    {
      "type": "training",
      "description": "Training step 1639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:55",
      "total_flops_so_far": 9743335205437440.0,
      "budget_used_percent": 9.74333520543744
    },
    {
      "type": "training",
      "description": "Training step 1640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:56",
      "total_flops_so_far": 9749276263489536.0,
      "budget_used_percent": 9.749276263489536
    },
    {
      "type": "training",
      "description": "Training step 1641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:56",
      "total_flops_so_far": 9755217321541632.0,
      "budget_used_percent": 9.755217321541632
    },
    {
      "type": "training",
      "description": "Training step 1642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:56",
      "total_flops_so_far": 9761158379593728.0,
      "budget_used_percent": 9.761158379593727
    },
    {
      "type": "training",
      "description": "Training step 1643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:57",
      "total_flops_so_far": 9767099437645824.0,
      "budget_used_percent": 9.767099437645824
    },
    {
      "type": "training",
      "description": "Training step 1644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:57",
      "total_flops_so_far": 9773040495697920.0,
      "budget_used_percent": 9.77304049569792
    },
    {
      "type": "training",
      "description": "Training step 1645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:57",
      "total_flops_so_far": 9778981553750016.0,
      "budget_used_percent": 9.778981553750016
    },
    {
      "type": "training",
      "description": "Training step 1646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:58",
      "total_flops_so_far": 9784922611802112.0,
      "budget_used_percent": 9.784922611802111
    },
    {
      "type": "training",
      "description": "Training step 1647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:58",
      "total_flops_so_far": 9790863669854208.0,
      "budget_used_percent": 9.790863669854208
    },
    {
      "type": "training",
      "description": "Training step 1648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:58",
      "total_flops_so_far": 9796804727906304.0,
      "budget_used_percent": 9.796804727906304
    },
    {
      "type": "training",
      "description": "Training step 1649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:59",
      "total_flops_so_far": 9802745785958400.0,
      "budget_used_percent": 9.8027457859584
    },
    {
      "type": "training",
      "description": "Training step 1650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:59",
      "total_flops_so_far": 9808686844010496.0,
      "budget_used_percent": 9.808686844010497
    },
    {
      "type": "training",
      "description": "Training step 1651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:33:59",
      "total_flops_so_far": 9814627902062592.0,
      "budget_used_percent": 9.814627902062592
    },
    {
      "type": "training",
      "description": "Training step 1652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:00",
      "total_flops_so_far": 9820568960114688.0,
      "budget_used_percent": 9.820568960114688
    },
    {
      "type": "training",
      "description": "Training step 1653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:00",
      "total_flops_so_far": 9826510018166784.0,
      "budget_used_percent": 9.826510018166784
    },
    {
      "type": "training",
      "description": "Training step 1654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:00",
      "total_flops_so_far": 9832451076218880.0,
      "budget_used_percent": 9.83245107621888
    },
    {
      "type": "training",
      "description": "Training step 1655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:01",
      "total_flops_so_far": 9838392134270976.0,
      "budget_used_percent": 9.838392134270975
    },
    {
      "type": "training",
      "description": "Training step 1656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:01",
      "total_flops_so_far": 9844333192323072.0,
      "budget_used_percent": 9.844333192323072
    },
    {
      "type": "training",
      "description": "Training step 1657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:01",
      "total_flops_so_far": 9850274250375168.0,
      "budget_used_percent": 9.850274250375168
    },
    {
      "type": "training",
      "description": "Training step 1658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:02",
      "total_flops_so_far": 9856215308427264.0,
      "budget_used_percent": 9.856215308427265
    },
    {
      "type": "training",
      "description": "Training step 1659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:02",
      "total_flops_so_far": 9862156366479360.0,
      "budget_used_percent": 9.862156366479361
    },
    {
      "type": "training",
      "description": "Training step 1660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:02",
      "total_flops_so_far": 9868097424531456.0,
      "budget_used_percent": 9.868097424531456
    },
    {
      "type": "training",
      "description": "Training step 1661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:03",
      "total_flops_so_far": 9874038482583552.0,
      "budget_used_percent": 9.874038482583552
    },
    {
      "type": "training",
      "description": "Training step 1662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:03",
      "total_flops_so_far": 9879979540635648.0,
      "budget_used_percent": 9.879979540635649
    },
    {
      "type": "training",
      "description": "Training step 1663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:03",
      "total_flops_so_far": 9885920598687744.0,
      "budget_used_percent": 9.885920598687745
    },
    {
      "type": "training",
      "description": "Training step 1664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:04",
      "total_flops_so_far": 9891861656739840.0,
      "budget_used_percent": 9.89186165673984
    },
    {
      "type": "training",
      "description": "Training step 1665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:04",
      "total_flops_so_far": 9897802714791936.0,
      "budget_used_percent": 9.897802714791935
    },
    {
      "type": "training",
      "description": "Training step 1666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:05",
      "total_flops_so_far": 9903743772844032.0,
      "budget_used_percent": 9.903743772844031
    },
    {
      "type": "training",
      "description": "Training step 1667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:05",
      "total_flops_so_far": 9909684830896128.0,
      "budget_used_percent": 9.909684830896127
    },
    {
      "type": "training",
      "description": "Training step 1668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:05",
      "total_flops_so_far": 9915625888948224.0,
      "budget_used_percent": 9.915625888948224
    },
    {
      "type": "training",
      "description": "Training step 1669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:06",
      "total_flops_so_far": 9921566947000320.0,
      "budget_used_percent": 9.92156694700032
    },
    {
      "type": "training",
      "description": "Training step 1670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:06",
      "total_flops_so_far": 9927508005052416.0,
      "budget_used_percent": 9.927508005052415
    },
    {
      "type": "training",
      "description": "Training step 1671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:06",
      "total_flops_so_far": 9933449063104512.0,
      "budget_used_percent": 9.933449063104511
    },
    {
      "type": "training",
      "description": "Training step 1672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:07",
      "total_flops_so_far": 9939390121156608.0,
      "budget_used_percent": 9.939390121156608
    },
    {
      "type": "training",
      "description": "Training step 1673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:07",
      "total_flops_so_far": 9945331179208704.0,
      "budget_used_percent": 9.945331179208704
    },
    {
      "type": "training",
      "description": "Training step 1674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:07",
      "total_flops_so_far": 9951272237260800.0,
      "budget_used_percent": 9.951272237260799
    },
    {
      "type": "training",
      "description": "Training step 1675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:08",
      "total_flops_so_far": 9957213295312896.0,
      "budget_used_percent": 9.957213295312895
    },
    {
      "type": "training",
      "description": "Training step 1676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:08",
      "total_flops_so_far": 9963154353364992.0,
      "budget_used_percent": 9.963154353364992
    },
    {
      "type": "training",
      "description": "Training step 1677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:08",
      "total_flops_so_far": 9969095411417088.0,
      "budget_used_percent": 9.969095411417088
    },
    {
      "type": "training",
      "description": "Training step 1678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:09",
      "total_flops_so_far": 9975036469469184.0,
      "budget_used_percent": 9.975036469469185
    },
    {
      "type": "training",
      "description": "Training step 1679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:09",
      "total_flops_so_far": 9980977527521280.0,
      "budget_used_percent": 9.98097752752128
    },
    {
      "type": "training",
      "description": "Training step 1680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:09",
      "total_flops_so_far": 9986918585573376.0,
      "budget_used_percent": 9.986918585573376
    },
    {
      "type": "training",
      "description": "Training step 1681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:10",
      "total_flops_so_far": 9992859643625472.0,
      "budget_used_percent": 9.992859643625472
    },
    {
      "type": "training",
      "description": "Training step 1682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:10",
      "total_flops_so_far": 9998800701677568.0,
      "budget_used_percent": 9.998800701677569
    },
    {
      "type": "training",
      "description": "Training step 1683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:10",
      "total_flops_so_far": 1.0004741759729664e+16,
      "budget_used_percent": 10.004741759729665
    },
    {
      "type": "training",
      "description": "Training step 1684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:11",
      "total_flops_so_far": 1.001068281778176e+16,
      "budget_used_percent": 10.01068281778176
    },
    {
      "type": "training",
      "description": "Training step 1685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:11",
      "total_flops_so_far": 1.0016623875833856e+16,
      "budget_used_percent": 10.016623875833856
    },
    {
      "type": "training",
      "description": "Training step 1686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:11",
      "total_flops_so_far": 1.0022564933885952e+16,
      "budget_used_percent": 10.022564933885953
    },
    {
      "type": "training",
      "description": "Training step 1687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:12",
      "total_flops_so_far": 1.0028505991938048e+16,
      "budget_used_percent": 10.028505991938049
    },
    {
      "type": "training",
      "description": "Training step 1688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:12",
      "total_flops_so_far": 1.0034447049990144e+16,
      "budget_used_percent": 10.034447049990144
    },
    {
      "type": "training",
      "description": "Training step 1689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:12",
      "total_flops_so_far": 1.004038810804224e+16,
      "budget_used_percent": 10.04038810804224
    },
    {
      "type": "training",
      "description": "Training step 1690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:13",
      "total_flops_so_far": 1.0046329166094336e+16,
      "budget_used_percent": 10.046329166094337
    },
    {
      "type": "training",
      "description": "Training step 1691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:13",
      "total_flops_so_far": 1.0052270224146432e+16,
      "budget_used_percent": 10.052270224146433
    },
    {
      "type": "training",
      "description": "Training step 1692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:13",
      "total_flops_so_far": 1.0058211282198528e+16,
      "budget_used_percent": 10.05821128219853
    },
    {
      "type": "training",
      "description": "Training step 1693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:14",
      "total_flops_so_far": 1.0064152340250624e+16,
      "budget_used_percent": 10.064152340250624
    },
    {
      "type": "training",
      "description": "Training step 1694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:14",
      "total_flops_so_far": 1.007009339830272e+16,
      "budget_used_percent": 10.070093398302719
    },
    {
      "type": "training",
      "description": "Training step 1695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:14",
      "total_flops_so_far": 1.0076034456354816e+16,
      "budget_used_percent": 10.076034456354815
    },
    {
      "type": "training",
      "description": "Training step 1696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:15",
      "total_flops_so_far": 1.0081975514406912e+16,
      "budget_used_percent": 10.081975514406912
    },
    {
      "type": "training",
      "description": "Training step 1697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:15",
      "total_flops_so_far": 1.0087916572459008e+16,
      "budget_used_percent": 10.087916572459008
    },
    {
      "type": "training",
      "description": "Training step 1698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:16",
      "total_flops_so_far": 1.0093857630511104e+16,
      "budget_used_percent": 10.093857630511103
    },
    {
      "type": "training",
      "description": "Training step 1699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:16",
      "total_flops_so_far": 1.00997986885632e+16,
      "budget_used_percent": 10.0997986885632
    },
    {
      "type": "training",
      "description": "Training step 1700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:16",
      "total_flops_so_far": 1.0105739746615296e+16,
      "budget_used_percent": 10.105739746615296
    },
    {
      "type": "training",
      "description": "Training step 1701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:17",
      "total_flops_so_far": 1.0111680804667392e+16,
      "budget_used_percent": 10.111680804667392
    },
    {
      "type": "training",
      "description": "Training step 1702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:17",
      "total_flops_so_far": 1.0117621862719488e+16,
      "budget_used_percent": 10.117621862719488
    },
    {
      "type": "training",
      "description": "Training step 1703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:17",
      "total_flops_so_far": 1.0123562920771584e+16,
      "budget_used_percent": 10.123562920771583
    },
    {
      "type": "training",
      "description": "Training step 1704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:18",
      "total_flops_so_far": 1.012950397882368e+16,
      "budget_used_percent": 10.12950397882368
    },
    {
      "type": "training",
      "description": "Training step 1705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:18",
      "total_flops_so_far": 1.0135445036875776e+16,
      "budget_used_percent": 10.135445036875776
    },
    {
      "type": "training",
      "description": "Training step 1706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:18",
      "total_flops_so_far": 1.0141386094927872e+16,
      "budget_used_percent": 10.141386094927872
    },
    {
      "type": "training",
      "description": "Training step 1707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:19",
      "total_flops_so_far": 1.0147327152979968e+16,
      "budget_used_percent": 10.147327152979969
    },
    {
      "type": "training",
      "description": "Training step 1708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:19",
      "total_flops_so_far": 1.0153268211032064e+16,
      "budget_used_percent": 10.153268211032064
    },
    {
      "type": "training",
      "description": "Training step 1709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:19",
      "total_flops_so_far": 1.015920926908416e+16,
      "budget_used_percent": 10.15920926908416
    },
    {
      "type": "training",
      "description": "Training step 1710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:20",
      "total_flops_so_far": 1.0165150327136256e+16,
      "budget_used_percent": 10.165150327136256
    },
    {
      "type": "training",
      "description": "Training step 1711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:20",
      "total_flops_so_far": 1.0171091385188352e+16,
      "budget_used_percent": 10.171091385188353
    },
    {
      "type": "training",
      "description": "Training step 1712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:20",
      "total_flops_so_far": 1.0177032443240448e+16,
      "budget_used_percent": 10.177032443240448
    },
    {
      "type": "training",
      "description": "Training step 1713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:21",
      "total_flops_so_far": 1.0182973501292544e+16,
      "budget_used_percent": 10.182973501292544
    },
    {
      "type": "training",
      "description": "Training step 1714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:21",
      "total_flops_so_far": 1.018891455934464e+16,
      "budget_used_percent": 10.18891455934464
    },
    {
      "type": "training",
      "description": "Training step 1715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:21",
      "total_flops_so_far": 1.0194855617396736e+16,
      "budget_used_percent": 10.194855617396737
    },
    {
      "type": "training",
      "description": "Training step 1716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:22",
      "total_flops_so_far": 1.0200796675448832e+16,
      "budget_used_percent": 10.200796675448833
    },
    {
      "type": "training",
      "description": "Training step 1717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:22",
      "total_flops_so_far": 1.0206737733500928e+16,
      "budget_used_percent": 10.206737733500928
    },
    {
      "type": "training",
      "description": "Training step 1718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:22",
      "total_flops_so_far": 1.0212678791553024e+16,
      "budget_used_percent": 10.212678791553024
    },
    {
      "type": "training",
      "description": "Training step 1719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:23",
      "total_flops_so_far": 1.021861984960512e+16,
      "budget_used_percent": 10.21861984960512
    },
    {
      "type": "training",
      "description": "Training step 1720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:23",
      "total_flops_so_far": 1.0224560907657216e+16,
      "budget_used_percent": 10.224560907657217
    },
    {
      "type": "training",
      "description": "Training step 1721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:23",
      "total_flops_so_far": 1.0230501965709312e+16,
      "budget_used_percent": 10.230501965709312
    },
    {
      "type": "training",
      "description": "Training step 1722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:24",
      "total_flops_so_far": 1.0236443023761408e+16,
      "budget_used_percent": 10.236443023761407
    },
    {
      "type": "training",
      "description": "Training step 1723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:24",
      "total_flops_so_far": 1.0242384081813504e+16,
      "budget_used_percent": 10.242384081813503
    },
    {
      "type": "training",
      "description": "Training step 1724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:24",
      "total_flops_so_far": 1.02483251398656e+16,
      "budget_used_percent": 10.2483251398656
    },
    {
      "type": "training",
      "description": "Training step 1725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:25",
      "total_flops_so_far": 1.0254266197917696e+16,
      "budget_used_percent": 10.254266197917696
    },
    {
      "type": "training",
      "description": "Training step 1726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:25",
      "total_flops_so_far": 1.0260207255969792e+16,
      "budget_used_percent": 10.260207255969792
    },
    {
      "type": "training",
      "description": "Training step 1727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:25",
      "total_flops_so_far": 1.0266148314021888e+16,
      "budget_used_percent": 10.266148314021887
    },
    {
      "type": "training",
      "description": "Training step 1728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:26",
      "total_flops_so_far": 1.0272089372073984e+16,
      "budget_used_percent": 10.272089372073983
    },
    {
      "type": "training",
      "description": "Training step 1729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:26",
      "total_flops_so_far": 1.027803043012608e+16,
      "budget_used_percent": 10.27803043012608
    },
    {
      "type": "training",
      "description": "Training step 1730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:27",
      "total_flops_so_far": 1.0283971488178176e+16,
      "budget_used_percent": 10.283971488178176
    },
    {
      "type": "training",
      "description": "Training step 1731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:27",
      "total_flops_so_far": 1.0289912546230272e+16,
      "budget_used_percent": 10.289912546230271
    },
    {
      "type": "training",
      "description": "Training step 1732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:27",
      "total_flops_so_far": 1.0295853604282368e+16,
      "budget_used_percent": 10.295853604282367
    },
    {
      "type": "training",
      "description": "Training step 1733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:28",
      "total_flops_so_far": 1.0301794662334464e+16,
      "budget_used_percent": 10.301794662334464
    },
    {
      "type": "training",
      "description": "Training step 1734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:28",
      "total_flops_so_far": 1.030773572038656e+16,
      "budget_used_percent": 10.30773572038656
    },
    {
      "type": "training",
      "description": "Training step 1735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:28",
      "total_flops_so_far": 1.0313676778438656e+16,
      "budget_used_percent": 10.313676778438657
    },
    {
      "type": "training",
      "description": "Training step 1736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:29",
      "total_flops_so_far": 1.0319617836490752e+16,
      "budget_used_percent": 10.319617836490751
    },
    {
      "type": "training",
      "description": "Training step 1737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:29",
      "total_flops_so_far": 1.0325558894542848e+16,
      "budget_used_percent": 10.325558894542848
    },
    {
      "type": "training",
      "description": "Training step 1738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:29",
      "total_flops_so_far": 1.0331499952594944e+16,
      "budget_used_percent": 10.331499952594944
    },
    {
      "type": "training",
      "description": "Training step 1739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:30",
      "total_flops_so_far": 1.033744101064704e+16,
      "budget_used_percent": 10.33744101064704
    },
    {
      "type": "training",
      "description": "Training step 1740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:30",
      "total_flops_so_far": 1.0343382068699136e+16,
      "budget_used_percent": 10.343382068699135
    },
    {
      "type": "training",
      "description": "Training step 1741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:30",
      "total_flops_so_far": 1.0349323126751232e+16,
      "budget_used_percent": 10.349323126751232
    },
    {
      "type": "training",
      "description": "Training step 1742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:31",
      "total_flops_so_far": 1.0355264184803328e+16,
      "budget_used_percent": 10.355264184803328
    },
    {
      "type": "training",
      "description": "Training step 1743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:31",
      "total_flops_so_far": 1.0361205242855424e+16,
      "budget_used_percent": 10.361205242855425
    },
    {
      "type": "training",
      "description": "Training step 1744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:31",
      "total_flops_so_far": 1.036714630090752e+16,
      "budget_used_percent": 10.367146300907521
    },
    {
      "type": "training",
      "description": "Training step 1745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:32",
      "total_flops_so_far": 1.0373087358959616e+16,
      "budget_used_percent": 10.373087358959616
    },
    {
      "type": "training",
      "description": "Training step 1746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:32",
      "total_flops_so_far": 1.0379028417011712e+16,
      "budget_used_percent": 10.379028417011712
    },
    {
      "type": "training",
      "description": "Training step 1747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:32",
      "total_flops_so_far": 1.0384969475063808e+16,
      "budget_used_percent": 10.384969475063809
    },
    {
      "type": "training",
      "description": "Training step 1748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:33",
      "total_flops_so_far": 1.0390910533115904e+16,
      "budget_used_percent": 10.390910533115905
    },
    {
      "type": "training",
      "description": "Training step 1749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:33",
      "total_flops_so_far": 1.0396851591168e+16,
      "budget_used_percent": 10.396851591168002
    },
    {
      "type": "training",
      "description": "Training step 1750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:33",
      "total_flops_so_far": 1.0402792649220096e+16,
      "budget_used_percent": 10.402792649220096
    },
    {
      "type": "training",
      "description": "Training step 1751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:34",
      "total_flops_so_far": 1.0408733707272192e+16,
      "budget_used_percent": 10.40873370727219
    },
    {
      "type": "training",
      "description": "Training step 1752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:34",
      "total_flops_so_far": 1.0414674765324288e+16,
      "budget_used_percent": 10.414674765324287
    },
    {
      "type": "training",
      "description": "Training step 1753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:34",
      "total_flops_so_far": 1.0420615823376384e+16,
      "budget_used_percent": 10.420615823376384
    },
    {
      "type": "training",
      "description": "Training step 1754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:35",
      "total_flops_so_far": 1.042655688142848e+16,
      "budget_used_percent": 10.42655688142848
    },
    {
      "type": "training",
      "description": "Training step 1755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:35",
      "total_flops_so_far": 1.0432497939480576e+16,
      "budget_used_percent": 10.432497939480575
    },
    {
      "type": "training",
      "description": "Training step 1756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:35",
      "total_flops_so_far": 1.0438438997532672e+16,
      "budget_used_percent": 10.438438997532671
    },
    {
      "type": "training",
      "description": "Training step 1757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:36",
      "total_flops_so_far": 1.0444380055584768e+16,
      "budget_used_percent": 10.444380055584768
    },
    {
      "type": "training",
      "description": "Training step 1758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:36",
      "total_flops_so_far": 1.0450321113636864e+16,
      "budget_used_percent": 10.450321113636864
    },
    {
      "type": "training",
      "description": "Training step 1759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:36",
      "total_flops_so_far": 1.045626217168896e+16,
      "budget_used_percent": 10.45626217168896
    },
    {
      "type": "training",
      "description": "Training step 1760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:37",
      "total_flops_so_far": 1.0462203229741056e+16,
      "budget_used_percent": 10.462203229741055
    },
    {
      "type": "training",
      "description": "Training step 1761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:37",
      "total_flops_so_far": 1.0468144287793152e+16,
      "budget_used_percent": 10.468144287793152
    },
    {
      "type": "training",
      "description": "Training step 1762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:37",
      "total_flops_so_far": 1.0474085345845248e+16,
      "budget_used_percent": 10.474085345845248
    },
    {
      "type": "training",
      "description": "Training step 1763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:38",
      "total_flops_so_far": 1.0480026403897344e+16,
      "budget_used_percent": 10.480026403897345
    },
    {
      "type": "training",
      "description": "Training step 1764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:38",
      "total_flops_so_far": 1.048596746194944e+16,
      "budget_used_percent": 10.48596746194944
    },
    {
      "type": "training",
      "description": "Training step 1765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:38",
      "total_flops_so_far": 1.0491908520001536e+16,
      "budget_used_percent": 10.491908520001536
    },
    {
      "type": "training",
      "description": "Training step 1766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:39",
      "total_flops_so_far": 1.0497849578053632e+16,
      "budget_used_percent": 10.497849578053632
    },
    {
      "type": "training",
      "description": "Training step 1767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:39",
      "total_flops_so_far": 1.0503790636105728e+16,
      "budget_used_percent": 10.503790636105728
    },
    {
      "type": "training",
      "description": "Training step 1768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:39",
      "total_flops_so_far": 1.0509731694157824e+16,
      "budget_used_percent": 10.509731694157825
    },
    {
      "type": "training",
      "description": "Training step 1769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:40",
      "total_flops_so_far": 1.051567275220992e+16,
      "budget_used_percent": 10.51567275220992
    },
    {
      "type": "training",
      "description": "Training step 1770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:40",
      "total_flops_so_far": 1.0521613810262016e+16,
      "budget_used_percent": 10.521613810262016
    },
    {
      "type": "training",
      "description": "Training step 1771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:41",
      "total_flops_so_far": 1.0527554868314112e+16,
      "budget_used_percent": 10.527554868314112
    },
    {
      "type": "training",
      "description": "Training step 1772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:41",
      "total_flops_so_far": 1.0533495926366208e+16,
      "budget_used_percent": 10.533495926366209
    },
    {
      "type": "training",
      "description": "Training step 1773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:41",
      "total_flops_so_far": 1.0539436984418304e+16,
      "budget_used_percent": 10.539436984418305
    },
    {
      "type": "training",
      "description": "Training step 1774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:42",
      "total_flops_so_far": 1.05453780424704e+16,
      "budget_used_percent": 10.5453780424704
    },
    {
      "type": "training",
      "description": "Training step 1775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:42",
      "total_flops_so_far": 1.0551319100522496e+16,
      "budget_used_percent": 10.551319100522496
    },
    {
      "type": "training",
      "description": "Training step 1776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:42",
      "total_flops_so_far": 1.0557260158574592e+16,
      "budget_used_percent": 10.557260158574593
    },
    {
      "type": "training",
      "description": "Training step 1777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:43",
      "total_flops_so_far": 1.0563201216626688e+16,
      "budget_used_percent": 10.56320121662669
    },
    {
      "type": "training",
      "description": "Training step 1778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:43",
      "total_flops_so_far": 1.0569142274678784e+16,
      "budget_used_percent": 10.569142274678784
    },
    {
      "type": "training",
      "description": "Training step 1779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:43",
      "total_flops_so_far": 1.057508333273088e+16,
      "budget_used_percent": 10.575083332730879
    },
    {
      "type": "training",
      "description": "Training step 1780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:44",
      "total_flops_so_far": 1.0581024390782976e+16,
      "budget_used_percent": 10.581024390782975
    },
    {
      "type": "training",
      "description": "Training step 1781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:44",
      "total_flops_so_far": 1.0586965448835072e+16,
      "budget_used_percent": 10.586965448835072
    },
    {
      "type": "training",
      "description": "Training step 1782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:44",
      "total_flops_so_far": 1.0592906506887168e+16,
      "budget_used_percent": 10.592906506887168
    },
    {
      "type": "training",
      "description": "Training step 1783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:45",
      "total_flops_so_far": 1.0598847564939264e+16,
      "budget_used_percent": 10.598847564939264
    },
    {
      "type": "training",
      "description": "Training step 1784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:45",
      "total_flops_so_far": 1.060478862299136e+16,
      "budget_used_percent": 10.604788622991359
    },
    {
      "type": "training",
      "description": "Training step 1785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:45",
      "total_flops_so_far": 1.0610729681043456e+16,
      "budget_used_percent": 10.610729681043455
    },
    {
      "type": "training",
      "description": "Training step 1786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:46",
      "total_flops_so_far": 1.0616670739095552e+16,
      "budget_used_percent": 10.616670739095552
    },
    {
      "type": "training",
      "description": "Training step 1787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:46",
      "total_flops_so_far": 1.0622611797147648e+16,
      "budget_used_percent": 10.622611797147648
    },
    {
      "type": "training",
      "description": "Training step 1788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:46",
      "total_flops_so_far": 1.0628552855199744e+16,
      "budget_used_percent": 10.628552855199743
    },
    {
      "type": "training",
      "description": "Training step 1789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:47",
      "total_flops_so_far": 1.063449391325184e+16,
      "budget_used_percent": 10.63449391325184
    },
    {
      "type": "training",
      "description": "Training step 1790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:47",
      "total_flops_so_far": 1.0640434971303936e+16,
      "budget_used_percent": 10.640434971303936
    },
    {
      "type": "training",
      "description": "Training step 1791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:47",
      "total_flops_so_far": 1.0646376029356032e+16,
      "budget_used_percent": 10.646376029356032
    },
    {
      "type": "training",
      "description": "Training step 1792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:48",
      "total_flops_so_far": 1.0652317087408128e+16,
      "budget_used_percent": 10.652317087408129
    },
    {
      "type": "training",
      "description": "Training step 1793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:48",
      "total_flops_so_far": 1.0658258145460224e+16,
      "budget_used_percent": 10.658258145460223
    },
    {
      "type": "training",
      "description": "Training step 1794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:48",
      "total_flops_so_far": 1.066419920351232e+16,
      "budget_used_percent": 10.66419920351232
    },
    {
      "type": "training",
      "description": "Training step 1795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:49",
      "total_flops_so_far": 1.0670140261564416e+16,
      "budget_used_percent": 10.670140261564416
    },
    {
      "type": "training",
      "description": "Training step 1796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:49",
      "total_flops_so_far": 1.0676081319616512e+16,
      "budget_used_percent": 10.676081319616513
    },
    {
      "type": "training",
      "description": "Training step 1797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:50",
      "total_flops_so_far": 1.0682022377668608e+16,
      "budget_used_percent": 10.682022377668607
    },
    {
      "type": "training",
      "description": "Training step 1798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:50",
      "total_flops_so_far": 1.0687963435720704e+16,
      "budget_used_percent": 10.687963435720704
    },
    {
      "type": "training",
      "description": "Training step 1799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:50",
      "total_flops_so_far": 1.06939044937728e+16,
      "budget_used_percent": 10.6939044937728
    },
    {
      "type": "training",
      "description": "Training step 1800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:51",
      "total_flops_so_far": 1.0699845551824896e+16,
      "budget_used_percent": 10.699845551824897
    },
    {
      "type": "training",
      "description": "Training step 1801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:51",
      "total_flops_so_far": 1.0705786609876992e+16,
      "budget_used_percent": 10.705786609876993
    },
    {
      "type": "training",
      "description": "Training step 1802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:51",
      "total_flops_so_far": 1.0711727667929088e+16,
      "budget_used_percent": 10.711727667929088
    },
    {
      "type": "training",
      "description": "Training step 1803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:52",
      "total_flops_so_far": 1.0717668725981184e+16,
      "budget_used_percent": 10.717668725981184
    },
    {
      "type": "training",
      "description": "Training step 1804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:52",
      "total_flops_so_far": 1.072360978403328e+16,
      "budget_used_percent": 10.72360978403328
    },
    {
      "type": "training",
      "description": "Training step 1805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:52",
      "total_flops_so_far": 1.0729550842085376e+16,
      "budget_used_percent": 10.729550842085377
    },
    {
      "type": "training",
      "description": "Training step 1806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:53",
      "total_flops_so_far": 1.0735491900137472e+16,
      "budget_used_percent": 10.735491900137472
    },
    {
      "type": "training",
      "description": "Training step 1807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:53",
      "total_flops_so_far": 1.0741432958189568e+16,
      "budget_used_percent": 10.741432958189566
    },
    {
      "type": "training",
      "description": "Training step 1808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:53",
      "total_flops_so_far": 1.0747374016241664e+16,
      "budget_used_percent": 10.747374016241663
    },
    {
      "type": "training",
      "description": "Training step 1809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:54",
      "total_flops_so_far": 1.075331507429376e+16,
      "budget_used_percent": 10.75331507429376
    },
    {
      "type": "training",
      "description": "Training step 1810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:54",
      "total_flops_so_far": 1.0759256132345856e+16,
      "budget_used_percent": 10.759256132345856
    },
    {
      "type": "training",
      "description": "Training step 1811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:54",
      "total_flops_so_far": 1.0765197190397952e+16,
      "budget_used_percent": 10.765197190397952
    },
    {
      "type": "training",
      "description": "Training step 1812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:55",
      "total_flops_so_far": 1.0771138248450048e+16,
      "budget_used_percent": 10.771138248450047
    },
    {
      "type": "training",
      "description": "Training step 1813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:55",
      "total_flops_so_far": 1.0777079306502144e+16,
      "budget_used_percent": 10.777079306502143
    },
    {
      "type": "training",
      "description": "Training step 1814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:55",
      "total_flops_so_far": 1.078302036455424e+16,
      "budget_used_percent": 10.78302036455424
    },
    {
      "type": "training",
      "description": "Training step 1815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:56",
      "total_flops_so_far": 1.0788961422606336e+16,
      "budget_used_percent": 10.788961422606336
    },
    {
      "type": "training",
      "description": "Training step 1816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:56",
      "total_flops_so_far": 1.0794902480658432e+16,
      "budget_used_percent": 10.794902480658433
    },
    {
      "type": "training",
      "description": "Training step 1817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:56",
      "total_flops_so_far": 1.0800843538710528e+16,
      "budget_used_percent": 10.800843538710527
    },
    {
      "type": "training",
      "description": "Training step 1818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:57",
      "total_flops_so_far": 1.0806784596762624e+16,
      "budget_used_percent": 10.806784596762624
    },
    {
      "type": "training",
      "description": "Training step 1819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:57",
      "total_flops_so_far": 1.081272565481472e+16,
      "budget_used_percent": 10.81272565481472
    },
    {
      "type": "training",
      "description": "Training step 1820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:57",
      "total_flops_so_far": 1.0818666712866816e+16,
      "budget_used_percent": 10.818666712866817
    },
    {
      "type": "training",
      "description": "Training step 1821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:58",
      "total_flops_so_far": 1.0824607770918912e+16,
      "budget_used_percent": 10.824607770918911
    },
    {
      "type": "training",
      "description": "Training step 1822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:58",
      "total_flops_so_far": 1.0830548828971008e+16,
      "budget_used_percent": 10.830548828971008
    },
    {
      "type": "training",
      "description": "Training step 1823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:59",
      "total_flops_so_far": 1.0836489887023104e+16,
      "budget_used_percent": 10.836489887023104
    },
    {
      "type": "training",
      "description": "Training step 1824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:59",
      "total_flops_so_far": 1.08424309450752e+16,
      "budget_used_percent": 10.8424309450752
    },
    {
      "type": "training",
      "description": "Training step 1825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:34:59",
      "total_flops_so_far": 1.0848372003127296e+16,
      "budget_used_percent": 10.848372003127297
    },
    {
      "type": "training",
      "description": "Training step 1826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:00",
      "total_flops_so_far": 1.0854313061179392e+16,
      "budget_used_percent": 10.854313061179392
    },
    {
      "type": "training",
      "description": "Training step 1827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:00",
      "total_flops_so_far": 1.0860254119231488e+16,
      "budget_used_percent": 10.860254119231488
    },
    {
      "type": "training",
      "description": "Training step 1828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:00",
      "total_flops_so_far": 1.0866195177283584e+16,
      "budget_used_percent": 10.866195177283585
    },
    {
      "type": "training",
      "description": "Training step 1829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:01",
      "total_flops_so_far": 1.087213623533568e+16,
      "budget_used_percent": 10.872136235335681
    },
    {
      "type": "training",
      "description": "Training step 1830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:01",
      "total_flops_so_far": 1.0878077293387776e+16,
      "budget_used_percent": 10.878077293387776
    },
    {
      "type": "training",
      "description": "Training step 1831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:01",
      "total_flops_so_far": 1.0884018351439872e+16,
      "budget_used_percent": 10.884018351439872
    },
    {
      "type": "training",
      "description": "Training step 1832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:02",
      "total_flops_so_far": 1.0889959409491968e+16,
      "budget_used_percent": 10.889959409491969
    },
    {
      "type": "training",
      "description": "Training step 1833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:02",
      "total_flops_so_far": 1.0895900467544064e+16,
      "budget_used_percent": 10.895900467544065
    },
    {
      "type": "training",
      "description": "Training step 1834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:02",
      "total_flops_so_far": 1.090184152559616e+16,
      "budget_used_percent": 10.901841525596161
    },
    {
      "type": "training",
      "description": "Training step 1835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:03",
      "total_flops_so_far": 1.0907782583648256e+16,
      "budget_used_percent": 10.907782583648256
    },
    {
      "type": "training",
      "description": "Training step 1836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:03",
      "total_flops_so_far": 1.0913723641700352e+16,
      "budget_used_percent": 10.91372364170035
    },
    {
      "type": "training",
      "description": "Training step 1837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:03",
      "total_flops_so_far": 1.0919664699752448e+16,
      "budget_used_percent": 10.919664699752447
    },
    {
      "type": "training",
      "description": "Training step 1838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:04",
      "total_flops_so_far": 1.0925605757804544e+16,
      "budget_used_percent": 10.925605757804544
    },
    {
      "type": "training",
      "description": "Training step 1839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:04",
      "total_flops_so_far": 1.093154681585664e+16,
      "budget_used_percent": 10.93154681585664
    },
    {
      "type": "training",
      "description": "Training step 1840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:04",
      "total_flops_so_far": 1.0937487873908736e+16,
      "budget_used_percent": 10.937487873908736
    },
    {
      "type": "training",
      "description": "Training step 1841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:05",
      "total_flops_so_far": 1.0943428931960832e+16,
      "budget_used_percent": 10.943428931960831
    },
    {
      "type": "training",
      "description": "Training step 1842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:05",
      "total_flops_so_far": 1.0949369990012928e+16,
      "budget_used_percent": 10.949369990012928
    },
    {
      "type": "training",
      "description": "Training step 1843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:05",
      "total_flops_so_far": 1.0955311048065024e+16,
      "budget_used_percent": 10.955311048065024
    },
    {
      "type": "training",
      "description": "Training step 1844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:06",
      "total_flops_so_far": 1.096125210611712e+16,
      "budget_used_percent": 10.96125210611712
    },
    {
      "type": "training",
      "description": "Training step 1845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:06",
      "total_flops_so_far": 1.0967193164169216e+16,
      "budget_used_percent": 10.967193164169215
    },
    {
      "type": "training",
      "description": "Training step 1846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:06",
      "total_flops_so_far": 1.0973134222221312e+16,
      "budget_used_percent": 10.973134222221312
    },
    {
      "type": "training",
      "description": "Training step 1847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:07",
      "total_flops_so_far": 1.0979075280273408e+16,
      "budget_used_percent": 10.979075280273408
    },
    {
      "type": "training",
      "description": "Training step 1848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:07",
      "total_flops_so_far": 1.0985016338325504e+16,
      "budget_used_percent": 10.985016338325504
    },
    {
      "type": "training",
      "description": "Training step 1849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:08",
      "total_flops_so_far": 1.09909573963776e+16,
      "budget_used_percent": 10.9909573963776
    },
    {
      "type": "training",
      "description": "Training step 1850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:08",
      "total_flops_so_far": 1.0996898454429696e+16,
      "budget_used_percent": 10.996898454429695
    },
    {
      "type": "training",
      "description": "Training step 1851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:08",
      "total_flops_so_far": 1.1002839512481792e+16,
      "budget_used_percent": 11.002839512481792
    },
    {
      "type": "training",
      "description": "Training step 1852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:09",
      "total_flops_so_far": 1.1008780570533888e+16,
      "budget_used_percent": 11.008780570533888
    },
    {
      "type": "training",
      "description": "Training step 1853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:09",
      "total_flops_so_far": 1.1014721628585984e+16,
      "budget_used_percent": 11.014721628585985
    },
    {
      "type": "training",
      "description": "Training step 1854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:09",
      "total_flops_so_far": 1.102066268663808e+16,
      "budget_used_percent": 11.02066268663808
    },
    {
      "type": "training",
      "description": "Training step 1855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:10",
      "total_flops_so_far": 1.1026603744690176e+16,
      "budget_used_percent": 11.026603744690176
    },
    {
      "type": "training",
      "description": "Training step 1856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:10",
      "total_flops_so_far": 1.1032544802742272e+16,
      "budget_used_percent": 11.032544802742272
    },
    {
      "type": "training",
      "description": "Training step 1857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:10",
      "total_flops_so_far": 1.1038485860794368e+16,
      "budget_used_percent": 11.038485860794369
    },
    {
      "type": "training",
      "description": "Training step 1858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:11",
      "total_flops_so_far": 1.1044426918846464e+16,
      "budget_used_percent": 11.044426918846465
    },
    {
      "type": "training",
      "description": "Training step 1859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:11",
      "total_flops_so_far": 1.105036797689856e+16,
      "budget_used_percent": 11.05036797689856
    },
    {
      "type": "training",
      "description": "Training step 1860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:11",
      "total_flops_so_far": 1.1056309034950656e+16,
      "budget_used_percent": 11.056309034950656
    },
    {
      "type": "training",
      "description": "Training step 1861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:12",
      "total_flops_so_far": 1.1062250093002752e+16,
      "budget_used_percent": 11.062250093002753
    },
    {
      "type": "training",
      "description": "Training step 1862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:12",
      "total_flops_so_far": 1.1068191151054848e+16,
      "budget_used_percent": 11.06819115105485
    },
    {
      "type": "training",
      "description": "Training step 1863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:12",
      "total_flops_so_far": 1.1074132209106944e+16,
      "budget_used_percent": 11.074132209106944
    },
    {
      "type": "training",
      "description": "Training step 1864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:13",
      "total_flops_so_far": 1.108007326715904e+16,
      "budget_used_percent": 11.080073267159039
    },
    {
      "type": "training",
      "description": "Training step 1865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:13",
      "total_flops_so_far": 1.1086014325211136e+16,
      "budget_used_percent": 11.086014325211135
    },
    {
      "type": "training",
      "description": "Training step 1866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:13",
      "total_flops_so_far": 1.1091955383263232e+16,
      "budget_used_percent": 11.091955383263231
    },
    {
      "type": "training",
      "description": "Training step 1867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:14",
      "total_flops_so_far": 1.1097896441315328e+16,
      "budget_used_percent": 11.097896441315328
    },
    {
      "type": "training",
      "description": "Training step 1868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:14",
      "total_flops_so_far": 1.1103837499367424e+16,
      "budget_used_percent": 11.103837499367424
    },
    {
      "type": "training",
      "description": "Training step 1869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:14",
      "total_flops_so_far": 1.110977855741952e+16,
      "budget_used_percent": 11.109778557419519
    },
    {
      "type": "training",
      "description": "Training step 1870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:15",
      "total_flops_so_far": 1.1115719615471616e+16,
      "budget_used_percent": 11.115719615471615
    },
    {
      "type": "training",
      "description": "Training step 1871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:15",
      "total_flops_so_far": 1.1121660673523712e+16,
      "budget_used_percent": 11.121660673523712
    },
    {
      "type": "training",
      "description": "Training step 1872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:16",
      "total_flops_so_far": 1.1127601731575808e+16,
      "budget_used_percent": 11.127601731575808
    },
    {
      "type": "training",
      "description": "Training step 1873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:16",
      "total_flops_so_far": 1.1133542789627904e+16,
      "budget_used_percent": 11.133542789627903
    },
    {
      "type": "training",
      "description": "Training step 1874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:16",
      "total_flops_so_far": 1.113948384768e+16,
      "budget_used_percent": 11.13948384768
    },
    {
      "type": "training",
      "description": "Training step 1875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:17",
      "total_flops_so_far": 1.1145424905732096e+16,
      "budget_used_percent": 11.145424905732096
    },
    {
      "type": "training",
      "description": "Training step 1876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:17",
      "total_flops_so_far": 1.1151365963784192e+16,
      "budget_used_percent": 11.151365963784192
    },
    {
      "type": "training",
      "description": "Training step 1877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:17",
      "total_flops_so_far": 1.1157307021836288e+16,
      "budget_used_percent": 11.157307021836289
    },
    {
      "type": "training",
      "description": "Training step 1878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:18",
      "total_flops_so_far": 1.1163248079888384e+16,
      "budget_used_percent": 11.163248079888383
    },
    {
      "type": "training",
      "description": "Training step 1879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:18",
      "total_flops_so_far": 1.116918913794048e+16,
      "budget_used_percent": 11.16918913794048
    },
    {
      "type": "training",
      "description": "Training step 1880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:18",
      "total_flops_so_far": 1.1175130195992576e+16,
      "budget_used_percent": 11.175130195992576
    },
    {
      "type": "training",
      "description": "Training step 1881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:19",
      "total_flops_so_far": 1.1181071254044672e+16,
      "budget_used_percent": 11.181071254044673
    },
    {
      "type": "training",
      "description": "Training step 1882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:19",
      "total_flops_so_far": 1.1187012312096768e+16,
      "budget_used_percent": 11.187012312096769
    },
    {
      "type": "training",
      "description": "Training step 1883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:19",
      "total_flops_so_far": 1.1192953370148864e+16,
      "budget_used_percent": 11.192953370148864
    },
    {
      "type": "training",
      "description": "Training step 1884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:20",
      "total_flops_so_far": 1.119889442820096e+16,
      "budget_used_percent": 11.19889442820096
    },
    {
      "type": "training",
      "description": "Training step 1885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:20",
      "total_flops_so_far": 1.1204835486253056e+16,
      "budget_used_percent": 11.204835486253057
    },
    {
      "type": "training",
      "description": "Training step 1886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:20",
      "total_flops_so_far": 1.1210776544305152e+16,
      "budget_used_percent": 11.210776544305153
    },
    {
      "type": "training",
      "description": "Training step 1887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:21",
      "total_flops_so_far": 1.1216717602357248e+16,
      "budget_used_percent": 11.216717602357248
    },
    {
      "type": "training",
      "description": "Training step 1888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:21",
      "total_flops_so_far": 1.1222658660409344e+16,
      "budget_used_percent": 11.222658660409344
    },
    {
      "type": "training",
      "description": "Training step 1889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:21",
      "total_flops_so_far": 1.122859971846144e+16,
      "budget_used_percent": 11.22859971846144
    },
    {
      "type": "training",
      "description": "Training step 1890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:22",
      "total_flops_so_far": 1.1234540776513536e+16,
      "budget_used_percent": 11.234540776513537
    },
    {
      "type": "training",
      "description": "Training step 1891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:22",
      "total_flops_so_far": 1.1240481834565632e+16,
      "budget_used_percent": 11.240481834565633
    },
    {
      "type": "training",
      "description": "Training step 1892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:23",
      "total_flops_so_far": 1.1246422892617728e+16,
      "budget_used_percent": 11.246422892617728
    },
    {
      "type": "training",
      "description": "Training step 1893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:23",
      "total_flops_so_far": 1.1252363950669824e+16,
      "budget_used_percent": 11.252363950669823
    },
    {
      "type": "training",
      "description": "Training step 1894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:23",
      "total_flops_so_far": 1.125830500872192e+16,
      "budget_used_percent": 11.25830500872192
    },
    {
      "type": "training",
      "description": "Training step 1895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:24",
      "total_flops_so_far": 1.1264246066774016e+16,
      "budget_used_percent": 11.264246066774016
    },
    {
      "type": "training",
      "description": "Training step 1896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:24",
      "total_flops_so_far": 1.1270187124826112e+16,
      "budget_used_percent": 11.270187124826112
    },
    {
      "type": "training",
      "description": "Training step 1897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:25",
      "total_flops_so_far": 1.1276128182878208e+16,
      "budget_used_percent": 11.276128182878207
    },
    {
      "type": "training",
      "description": "Training step 1898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:25",
      "total_flops_so_far": 1.1282069240930304e+16,
      "budget_used_percent": 11.282069240930303
    },
    {
      "type": "training",
      "description": "Training step 1899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:25",
      "total_flops_so_far": 1.12880102989824e+16,
      "budget_used_percent": 11.2880102989824
    },
    {
      "type": "training",
      "description": "Training step 1900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:26",
      "total_flops_so_far": 1.1293951357034496e+16,
      "budget_used_percent": 11.293951357034496
    },
    {
      "type": "training",
      "description": "Training step 1901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:26",
      "total_flops_so_far": 1.1299892415086592e+16,
      "budget_used_percent": 11.299892415086592
    },
    {
      "type": "training",
      "description": "Training step 1902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:26",
      "total_flops_so_far": 1.1305833473138688e+16,
      "budget_used_percent": 11.305833473138687
    },
    {
      "type": "training",
      "description": "Training step 1903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:27",
      "total_flops_so_far": 1.1311774531190784e+16,
      "budget_used_percent": 11.311774531190784
    },
    {
      "type": "training",
      "description": "Training step 1904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:27",
      "total_flops_so_far": 1.131771558924288e+16,
      "budget_used_percent": 11.31771558924288
    },
    {
      "type": "training",
      "description": "Training step 1905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:27",
      "total_flops_so_far": 1.1323656647294976e+16,
      "budget_used_percent": 11.323656647294976
    },
    {
      "type": "training",
      "description": "Training step 1906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:28",
      "total_flops_so_far": 1.1329597705347072e+16,
      "budget_used_percent": 11.329597705347073
    },
    {
      "type": "training",
      "description": "Training step 1907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:28",
      "total_flops_so_far": 1.1335538763399168e+16,
      "budget_used_percent": 11.335538763399168
    },
    {
      "type": "training",
      "description": "Training step 1908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:28",
      "total_flops_so_far": 1.1341479821451264e+16,
      "budget_used_percent": 11.341479821451264
    },
    {
      "type": "training",
      "description": "Training step 1909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:29",
      "total_flops_so_far": 1.134742087950336e+16,
      "budget_used_percent": 11.34742087950336
    },
    {
      "type": "training",
      "description": "Training step 1910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:29",
      "total_flops_so_far": 1.1353361937555456e+16,
      "budget_used_percent": 11.353361937555457
    },
    {
      "type": "training",
      "description": "Training step 1911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:29",
      "total_flops_so_far": 1.1359302995607552e+16,
      "budget_used_percent": 11.359302995607552
    },
    {
      "type": "training",
      "description": "Training step 1912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:30",
      "total_flops_so_far": 1.1365244053659648e+16,
      "budget_used_percent": 11.365244053659648
    },
    {
      "type": "training",
      "description": "Training step 1913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:30",
      "total_flops_so_far": 1.1371185111711744e+16,
      "budget_used_percent": 11.371185111711744
    },
    {
      "type": "training",
      "description": "Training step 1914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:30",
      "total_flops_so_far": 1.137712616976384e+16,
      "budget_used_percent": 11.37712616976384
    },
    {
      "type": "training",
      "description": "Training step 1915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:31",
      "total_flops_so_far": 1.1383067227815936e+16,
      "budget_used_percent": 11.383067227815937
    },
    {
      "type": "training",
      "description": "Training step 1916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:31",
      "total_flops_so_far": 1.1389008285868032e+16,
      "budget_used_percent": 11.389008285868032
    },
    {
      "type": "training",
      "description": "Training step 1917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:31",
      "total_flops_so_far": 1.1394949343920128e+16,
      "budget_used_percent": 11.394949343920128
    },
    {
      "type": "training",
      "description": "Training step 1918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:32",
      "total_flops_so_far": 1.1400890401972224e+16,
      "budget_used_percent": 11.400890401972225
    },
    {
      "type": "training",
      "description": "Training step 1919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:32",
      "total_flops_so_far": 1.140683146002432e+16,
      "budget_used_percent": 11.406831460024321
    },
    {
      "type": "training",
      "description": "Training step 1920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:33",
      "total_flops_so_far": 1.1412772518076416e+16,
      "budget_used_percent": 11.412772518076416
    },
    {
      "type": "training",
      "description": "Training step 1921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:33",
      "total_flops_so_far": 1.1418713576128512e+16,
      "budget_used_percent": 11.41871357612851
    },
    {
      "type": "training",
      "description": "Training step 1922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:33",
      "total_flops_so_far": 1.1424654634180608e+16,
      "budget_used_percent": 11.424654634180607
    },
    {
      "type": "training",
      "description": "Training step 1923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:34",
      "total_flops_so_far": 1.1430595692232704e+16,
      "budget_used_percent": 11.430595692232703
    },
    {
      "type": "training",
      "description": "Training step 1924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:34",
      "total_flops_so_far": 1.14365367502848e+16,
      "budget_used_percent": 11.4365367502848
    },
    {
      "type": "training",
      "description": "Training step 1925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:34",
      "total_flops_so_far": 1.1442477808336896e+16,
      "budget_used_percent": 11.442477808336896
    },
    {
      "type": "training",
      "description": "Training step 1926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:35",
      "total_flops_so_far": 1.1448418866388992e+16,
      "budget_used_percent": 11.448418866388991
    },
    {
      "type": "training",
      "description": "Training step 1927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:35",
      "total_flops_so_far": 1.1454359924441088e+16,
      "budget_used_percent": 11.454359924441087
    },
    {
      "type": "training",
      "description": "Training step 1928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:35",
      "total_flops_so_far": 1.1460300982493184e+16,
      "budget_used_percent": 11.460300982493184
    },
    {
      "type": "training",
      "description": "Training step 1929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:36",
      "total_flops_so_far": 1.146624204054528e+16,
      "budget_used_percent": 11.46624204054528
    },
    {
      "type": "training",
      "description": "Training step 1930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:36",
      "total_flops_so_far": 1.1472183098597376e+16,
      "budget_used_percent": 11.472183098597375
    },
    {
      "type": "training",
      "description": "Training step 1931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:36",
      "total_flops_so_far": 1.1478124156649472e+16,
      "budget_used_percent": 11.478124156649471
    },
    {
      "type": "training",
      "description": "Training step 1932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:37",
      "total_flops_so_far": 1.1484065214701568e+16,
      "budget_used_percent": 11.484065214701568
    },
    {
      "type": "training",
      "description": "Training step 1933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:37",
      "total_flops_so_far": 1.1490006272753664e+16,
      "budget_used_percent": 11.490006272753664
    },
    {
      "type": "training",
      "description": "Training step 1934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:37",
      "total_flops_so_far": 1.149594733080576e+16,
      "budget_used_percent": 11.49594733080576
    },
    {
      "type": "training",
      "description": "Training step 1935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:38",
      "total_flops_so_far": 1.1501888388857856e+16,
      "budget_used_percent": 11.501888388857855
    },
    {
      "type": "training",
      "description": "Training step 1936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:38",
      "total_flops_so_far": 1.1507829446909952e+16,
      "budget_used_percent": 11.507829446909952
    },
    {
      "type": "training",
      "description": "Training step 1937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:38",
      "total_flops_so_far": 1.1513770504962048e+16,
      "budget_used_percent": 11.513770504962048
    },
    {
      "type": "training",
      "description": "Training step 1938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:39",
      "total_flops_so_far": 1.1519711563014144e+16,
      "budget_used_percent": 11.519711563014145
    },
    {
      "type": "training",
      "description": "Training step 1939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:39",
      "total_flops_so_far": 1.152565262106624e+16,
      "budget_used_percent": 11.52565262106624
    },
    {
      "type": "training",
      "description": "Training step 1940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:40",
      "total_flops_so_far": 1.1531593679118336e+16,
      "budget_used_percent": 11.531593679118336
    },
    {
      "type": "training",
      "description": "Training step 1941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:40",
      "total_flops_so_far": 1.1537534737170432e+16,
      "budget_used_percent": 11.537534737170432
    },
    {
      "type": "training",
      "description": "Training step 1942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:40",
      "total_flops_so_far": 1.1543475795222528e+16,
      "budget_used_percent": 11.543475795222529
    },
    {
      "type": "training",
      "description": "Training step 1943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:41",
      "total_flops_so_far": 1.1549416853274624e+16,
      "budget_used_percent": 11.549416853274625
    },
    {
      "type": "training",
      "description": "Training step 1944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:41",
      "total_flops_so_far": 1.155535791132672e+16,
      "budget_used_percent": 11.55535791132672
    },
    {
      "type": "training",
      "description": "Training step 1945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:41",
      "total_flops_so_far": 1.1561298969378816e+16,
      "budget_used_percent": 11.561298969378816
    },
    {
      "type": "training",
      "description": "Training step 1946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:42",
      "total_flops_so_far": 1.1567240027430912e+16,
      "budget_used_percent": 11.567240027430913
    },
    {
      "type": "training",
      "description": "Training step 1947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:42",
      "total_flops_so_far": 1.1573181085483008e+16,
      "budget_used_percent": 11.573181085483009
    },
    {
      "type": "training",
      "description": "Training step 1948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:42",
      "total_flops_so_far": 1.1579122143535104e+16,
      "budget_used_percent": 11.579122143535105
    },
    {
      "type": "training",
      "description": "Training step 1949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:43",
      "total_flops_so_far": 1.15850632015872e+16,
      "budget_used_percent": 11.5850632015872
    },
    {
      "type": "training",
      "description": "Training step 1950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:43",
      "total_flops_so_far": 1.1591004259639296e+16,
      "budget_used_percent": 11.591004259639295
    },
    {
      "type": "training",
      "description": "Training step 1951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:43",
      "total_flops_so_far": 1.1596945317691392e+16,
      "budget_used_percent": 11.596945317691391
    },
    {
      "type": "training",
      "description": "Training step 1952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:44",
      "total_flops_so_far": 1.1602886375743488e+16,
      "budget_used_percent": 11.602886375743488
    },
    {
      "type": "training",
      "description": "Training step 1953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:44",
      "total_flops_so_far": 1.1608827433795584e+16,
      "budget_used_percent": 11.608827433795584
    },
    {
      "type": "training",
      "description": "Training step 1954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:44",
      "total_flops_so_far": 1.161476849184768e+16,
      "budget_used_percent": 11.614768491847679
    },
    {
      "type": "training",
      "description": "Training step 1955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:45",
      "total_flops_so_far": 1.1620709549899776e+16,
      "budget_used_percent": 11.620709549899775
    },
    {
      "type": "training",
      "description": "Training step 1956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:45",
      "total_flops_so_far": 1.1626650607951872e+16,
      "budget_used_percent": 11.626650607951872
    },
    {
      "type": "training",
      "description": "Training step 1957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:45",
      "total_flops_so_far": 1.1632591666003968e+16,
      "budget_used_percent": 11.632591666003968
    },
    {
      "type": "training",
      "description": "Training step 1958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:46",
      "total_flops_so_far": 1.1638532724056064e+16,
      "budget_used_percent": 11.638532724056065
    },
    {
      "type": "training",
      "description": "Training step 1959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:46",
      "total_flops_so_far": 1.164447378210816e+16,
      "budget_used_percent": 11.64447378210816
    },
    {
      "type": "training",
      "description": "Training step 1960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:47",
      "total_flops_so_far": 1.1650414840160256e+16,
      "budget_used_percent": 11.650414840160256
    },
    {
      "type": "training",
      "description": "Training step 1961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:47",
      "total_flops_so_far": 1.1656355898212352e+16,
      "budget_used_percent": 11.656355898212352
    },
    {
      "type": "training",
      "description": "Training step 1962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:47",
      "total_flops_so_far": 1.1662296956264448e+16,
      "budget_used_percent": 11.662296956264449
    },
    {
      "type": "training",
      "description": "Training step 1963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:48",
      "total_flops_so_far": 1.1668238014316544e+16,
      "budget_used_percent": 11.668238014316543
    },
    {
      "type": "training",
      "description": "Training step 1964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:48",
      "total_flops_so_far": 1.167417907236864e+16,
      "budget_used_percent": 11.67417907236864
    },
    {
      "type": "training",
      "description": "Training step 1965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:48",
      "total_flops_so_far": 1.1680120130420736e+16,
      "budget_used_percent": 11.680120130420736
    },
    {
      "type": "training",
      "description": "Training step 1966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:49",
      "total_flops_so_far": 1.1686061188472832e+16,
      "budget_used_percent": 11.686061188472832
    },
    {
      "type": "training",
      "description": "Training step 1967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:49",
      "total_flops_so_far": 1.1692002246524928e+16,
      "budget_used_percent": 11.692002246524929
    },
    {
      "type": "training",
      "description": "Training step 1968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:49",
      "total_flops_so_far": 1.1697943304577024e+16,
      "budget_used_percent": 11.697943304577024
    },
    {
      "type": "training",
      "description": "Training step 1969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:50",
      "total_flops_so_far": 1.170388436262912e+16,
      "budget_used_percent": 11.70388436262912
    },
    {
      "type": "training",
      "description": "Training step 1970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:50",
      "total_flops_so_far": 1.1709825420681216e+16,
      "budget_used_percent": 11.709825420681216
    },
    {
      "type": "training",
      "description": "Training step 1971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:50",
      "total_flops_so_far": 1.1715766478733312e+16,
      "budget_used_percent": 11.715766478733313
    },
    {
      "type": "training",
      "description": "Training step 1972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:51",
      "total_flops_so_far": 1.1721707536785408e+16,
      "budget_used_percent": 11.72170753678541
    },
    {
      "type": "training",
      "description": "Training step 1973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:51",
      "total_flops_so_far": 1.1727648594837504e+16,
      "budget_used_percent": 11.727648594837504
    },
    {
      "type": "training",
      "description": "Training step 1974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:51",
      "total_flops_so_far": 1.17335896528896e+16,
      "budget_used_percent": 11.7335896528896
    },
    {
      "type": "training",
      "description": "Training step 1975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:52",
      "total_flops_so_far": 1.1739530710941696e+16,
      "budget_used_percent": 11.739530710941697
    },
    {
      "type": "training",
      "description": "Training step 1976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:52",
      "total_flops_so_far": 1.1745471768993792e+16,
      "budget_used_percent": 11.745471768993793
    },
    {
      "type": "training",
      "description": "Training step 1977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:52",
      "total_flops_so_far": 1.1751412827045888e+16,
      "budget_used_percent": 11.751412827045888
    },
    {
      "type": "training",
      "description": "Training step 1978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:53",
      "total_flops_so_far": 1.1757353885097984e+16,
      "budget_used_percent": 11.757353885097983
    },
    {
      "type": "training",
      "description": "Training step 1979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:53",
      "total_flops_so_far": 1.176329494315008e+16,
      "budget_used_percent": 11.763294943150079
    },
    {
      "type": "training",
      "description": "Training step 1980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:54",
      "total_flops_so_far": 1.1769236001202176e+16,
      "budget_used_percent": 11.769236001202176
    },
    {
      "type": "training",
      "description": "Training step 1981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:54",
      "total_flops_so_far": 1.1775177059254272e+16,
      "budget_used_percent": 11.775177059254272
    },
    {
      "type": "training",
      "description": "Training step 1982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:54",
      "total_flops_so_far": 1.1781118117306368e+16,
      "budget_used_percent": 11.781118117306368
    },
    {
      "type": "training",
      "description": "Training step 1983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:55",
      "total_flops_so_far": 1.1787059175358464e+16,
      "budget_used_percent": 11.787059175358463
    },
    {
      "type": "training",
      "description": "Training step 1984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:55",
      "total_flops_so_far": 1.179300023341056e+16,
      "budget_used_percent": 11.79300023341056
    },
    {
      "type": "training",
      "description": "Training step 1985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:55",
      "total_flops_so_far": 1.1798941291462656e+16,
      "budget_used_percent": 11.798941291462656
    },
    {
      "type": "training",
      "description": "Training step 1986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:56",
      "total_flops_so_far": 1.1804882349514752e+16,
      "budget_used_percent": 11.804882349514752
    },
    {
      "type": "training",
      "description": "Training step 1987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:56",
      "total_flops_so_far": 1.1810823407566848e+16,
      "budget_used_percent": 11.810823407566847
    },
    {
      "type": "training",
      "description": "Training step 1988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:56",
      "total_flops_so_far": 1.1816764465618944e+16,
      "budget_used_percent": 11.816764465618943
    },
    {
      "type": "training",
      "description": "Training step 1989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:57",
      "total_flops_so_far": 1.182270552367104e+16,
      "budget_used_percent": 11.82270552367104
    },
    {
      "type": "training",
      "description": "Training step 1990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:57",
      "total_flops_so_far": 1.1828646581723136e+16,
      "budget_used_percent": 11.828646581723136
    },
    {
      "type": "training",
      "description": "Training step 1991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:57",
      "total_flops_so_far": 1.1834587639775232e+16,
      "budget_used_percent": 11.834587639775233
    },
    {
      "type": "training",
      "description": "Training step 1992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:58",
      "total_flops_so_far": 1.1840528697827328e+16,
      "budget_used_percent": 11.840528697827327
    },
    {
      "type": "training",
      "description": "Training step 1993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:58",
      "total_flops_so_far": 1.1846469755879424e+16,
      "budget_used_percent": 11.846469755879424
    },
    {
      "type": "training",
      "description": "Training step 1994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:58",
      "total_flops_so_far": 1.185241081393152e+16,
      "budget_used_percent": 11.85241081393152
    },
    {
      "type": "training",
      "description": "Training step 1995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:59",
      "total_flops_so_far": 1.1858351871983616e+16,
      "budget_used_percent": 11.858351871983617
    },
    {
      "type": "training",
      "description": "Training step 1996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:59",
      "total_flops_so_far": 1.1864292930035712e+16,
      "budget_used_percent": 11.864292930035711
    },
    {
      "type": "training",
      "description": "Training step 1997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:35:59",
      "total_flops_so_far": 1.1870233988087808e+16,
      "budget_used_percent": 11.870233988087808
    },
    {
      "type": "training",
      "description": "Training step 1998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:36:00",
      "total_flops_so_far": 1.1876175046139904e+16,
      "budget_used_percent": 11.876175046139904
    },
    {
      "type": "training",
      "description": "Training step 1999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:36:00",
      "total_flops_so_far": 1.1882116104192e+16,
      "budget_used_percent": 11.882116104192
    },
    {
      "type": "training",
      "description": "Training step 2000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:25",
      "total_flops_so_far": 1.1888057162244096e+16,
      "budget_used_percent": 11.888057162244097
    },
    {
      "type": "training",
      "description": "Training step 2001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:25",
      "total_flops_so_far": 1.1893998220296192e+16,
      "budget_used_percent": 11.893998220296192
    },
    {
      "type": "training",
      "description": "Training step 2002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:26",
      "total_flops_so_far": 1.1899939278348288e+16,
      "budget_used_percent": 11.899939278348288
    },
    {
      "type": "training",
      "description": "Training step 2003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:26",
      "total_flops_so_far": 1.1905880336400384e+16,
      "budget_used_percent": 11.905880336400385
    },
    {
      "type": "training",
      "description": "Training step 2004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:26",
      "total_flops_so_far": 1.191182139445248e+16,
      "budget_used_percent": 11.911821394452481
    },
    {
      "type": "training",
      "description": "Training step 2005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:27",
      "total_flops_so_far": 1.1917762452504576e+16,
      "budget_used_percent": 11.917762452504576
    },
    {
      "type": "training",
      "description": "Training step 2006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:27",
      "total_flops_so_far": 1.1923703510556672e+16,
      "budget_used_percent": 11.92370351055667
    },
    {
      "type": "training",
      "description": "Training step 2007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:28",
      "total_flops_so_far": 1.1929644568608768e+16,
      "budget_used_percent": 11.929644568608767
    },
    {
      "type": "training",
      "description": "Training step 2008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:28",
      "total_flops_so_far": 1.1935585626660864e+16,
      "budget_used_percent": 11.935585626660863
    },
    {
      "type": "training",
      "description": "Training step 2009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:28",
      "total_flops_so_far": 1.194152668471296e+16,
      "budget_used_percent": 11.94152668471296
    },
    {
      "type": "training",
      "description": "Training step 2010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:29",
      "total_flops_so_far": 1.1947467742765056e+16,
      "budget_used_percent": 11.947467742765056
    },
    {
      "type": "training",
      "description": "Training step 2011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:29",
      "total_flops_so_far": 1.1953408800817152e+16,
      "budget_used_percent": 11.95340880081715
    },
    {
      "type": "training",
      "description": "Training step 2012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:29",
      "total_flops_so_far": 1.1959349858869248e+16,
      "budget_used_percent": 11.959349858869247
    },
    {
      "type": "training",
      "description": "Training step 2013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:30",
      "total_flops_so_far": 1.1965290916921344e+16,
      "budget_used_percent": 11.965290916921344
    },
    {
      "type": "training",
      "description": "Training step 2014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:30",
      "total_flops_so_far": 1.197123197497344e+16,
      "budget_used_percent": 11.97123197497344
    },
    {
      "type": "training",
      "description": "Training step 2015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:30",
      "total_flops_so_far": 1.1977173033025536e+16,
      "budget_used_percent": 11.977173033025537
    },
    {
      "type": "training",
      "description": "Training step 2016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:31",
      "total_flops_so_far": 1.1983114091077632e+16,
      "budget_used_percent": 11.983114091077631
    },
    {
      "type": "training",
      "description": "Training step 2017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:31",
      "total_flops_so_far": 1.1989055149129728e+16,
      "budget_used_percent": 11.989055149129728
    },
    {
      "type": "training",
      "description": "Training step 2018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:31",
      "total_flops_so_far": 1.1994996207181824e+16,
      "budget_used_percent": 11.994996207181824
    },
    {
      "type": "training",
      "description": "Training step 2019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:32",
      "total_flops_so_far": 1.200093726523392e+16,
      "budget_used_percent": 12.00093726523392
    },
    {
      "type": "training",
      "description": "Training step 2020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:32",
      "total_flops_so_far": 1.2006878323286016e+16,
      "budget_used_percent": 12.006878323286015
    },
    {
      "type": "training",
      "description": "Training step 2021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:32",
      "total_flops_so_far": 1.2012819381338112e+16,
      "budget_used_percent": 12.012819381338112
    },
    {
      "type": "training",
      "description": "Training step 2022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:33",
      "total_flops_so_far": 1.2018760439390208e+16,
      "budget_used_percent": 12.018760439390208
    },
    {
      "type": "training",
      "description": "Training step 2023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:33",
      "total_flops_so_far": 1.2024701497442304e+16,
      "budget_used_percent": 12.024701497442305
    },
    {
      "type": "training",
      "description": "Training step 2024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:33",
      "total_flops_so_far": 1.20306425554944e+16,
      "budget_used_percent": 12.030642555494401
    },
    {
      "type": "training",
      "description": "Training step 2025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:34",
      "total_flops_so_far": 1.2036583613546496e+16,
      "budget_used_percent": 12.036583613546496
    },
    {
      "type": "training",
      "description": "Training step 2026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:34",
      "total_flops_so_far": 1.2042524671598592e+16,
      "budget_used_percent": 12.042524671598592
    },
    {
      "type": "training",
      "description": "Training step 2027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:35",
      "total_flops_so_far": 1.2048465729650688e+16,
      "budget_used_percent": 12.048465729650689
    },
    {
      "type": "training",
      "description": "Training step 2028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:35",
      "total_flops_so_far": 1.2054406787702784e+16,
      "budget_used_percent": 12.054406787702785
    },
    {
      "type": "training",
      "description": "Training step 2029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:35",
      "total_flops_so_far": 1.206034784575488e+16,
      "budget_used_percent": 12.06034784575488
    },
    {
      "type": "training",
      "description": "Training step 2030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:36",
      "total_flops_so_far": 1.2066288903806976e+16,
      "budget_used_percent": 12.066288903806976
    },
    {
      "type": "training",
      "description": "Training step 2031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:36",
      "total_flops_so_far": 1.2072229961859072e+16,
      "budget_used_percent": 12.072229961859072
    },
    {
      "type": "training",
      "description": "Training step 2032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:36",
      "total_flops_so_far": 1.2078171019911168e+16,
      "budget_used_percent": 12.078171019911169
    },
    {
      "type": "training",
      "description": "Training step 2033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:37",
      "total_flops_so_far": 1.2084112077963264e+16,
      "budget_used_percent": 12.084112077963265
    },
    {
      "type": "training",
      "description": "Training step 2034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:37",
      "total_flops_so_far": 1.209005313601536e+16,
      "budget_used_percent": 12.09005313601536
    },
    {
      "type": "training",
      "description": "Training step 2035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:37",
      "total_flops_so_far": 1.2095994194067456e+16,
      "budget_used_percent": 12.095994194067455
    },
    {
      "type": "training",
      "description": "Training step 2036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:38",
      "total_flops_so_far": 1.2101935252119552e+16,
      "budget_used_percent": 12.101935252119551
    },
    {
      "type": "training",
      "description": "Training step 2037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:38",
      "total_flops_so_far": 1.2107876310171648e+16,
      "budget_used_percent": 12.107876310171648
    },
    {
      "type": "training",
      "description": "Training step 2038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:38",
      "total_flops_so_far": 1.2113817368223744e+16,
      "budget_used_percent": 12.113817368223744
    },
    {
      "type": "training",
      "description": "Training step 2039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:39",
      "total_flops_so_far": 1.211975842627584e+16,
      "budget_used_percent": 12.11975842627584
    },
    {
      "type": "training",
      "description": "Training step 2040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:39",
      "total_flops_so_far": 1.2125699484327936e+16,
      "budget_used_percent": 12.125699484327935
    },
    {
      "type": "training",
      "description": "Training step 2041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:39",
      "total_flops_so_far": 1.2131640542380032e+16,
      "budget_used_percent": 12.131640542380032
    },
    {
      "type": "training",
      "description": "Training step 2042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:40",
      "total_flops_so_far": 1.2137581600432128e+16,
      "budget_used_percent": 12.137581600432128
    },
    {
      "type": "training",
      "description": "Training step 2043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:40",
      "total_flops_so_far": 1.2143522658484224e+16,
      "budget_used_percent": 12.143522658484224
    },
    {
      "type": "training",
      "description": "Training step 2044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:41",
      "total_flops_so_far": 1.214946371653632e+16,
      "budget_used_percent": 12.149463716536319
    },
    {
      "type": "training",
      "description": "Training step 2045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:41",
      "total_flops_so_far": 1.2155404774588416e+16,
      "budget_used_percent": 12.155404774588416
    },
    {
      "type": "training",
      "description": "Training step 2046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:41",
      "total_flops_so_far": 1.2161345832640512e+16,
      "budget_used_percent": 12.161345832640512
    },
    {
      "type": "training",
      "description": "Training step 2047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:42",
      "total_flops_so_far": 1.2167286890692608e+16,
      "budget_used_percent": 12.167286890692608
    },
    {
      "type": "training",
      "description": "Training step 2048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:42",
      "total_flops_so_far": 1.2173227948744704e+16,
      "budget_used_percent": 12.173227948744705
    },
    {
      "type": "training",
      "description": "Training step 2049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:42",
      "total_flops_so_far": 1.21791690067968e+16,
      "budget_used_percent": 12.1791690067968
    },
    {
      "type": "training",
      "description": "Training step 2050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:43",
      "total_flops_so_far": 1.2185110064848896e+16,
      "budget_used_percent": 12.185110064848896
    },
    {
      "type": "training",
      "description": "Training step 2051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:43",
      "total_flops_so_far": 1.2191051122900992e+16,
      "budget_used_percent": 12.191051122900992
    },
    {
      "type": "training",
      "description": "Training step 2052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:43",
      "total_flops_so_far": 1.2196992180953088e+16,
      "budget_used_percent": 12.196992180953089
    },
    {
      "type": "training",
      "description": "Training step 2053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:44",
      "total_flops_so_far": 1.2202933239005184e+16,
      "budget_used_percent": 12.202933239005183
    },
    {
      "type": "training",
      "description": "Training step 2054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:44",
      "total_flops_so_far": 1.220887429705728e+16,
      "budget_used_percent": 12.20887429705728
    },
    {
      "type": "training",
      "description": "Training step 2055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:44",
      "total_flops_so_far": 1.2214815355109376e+16,
      "budget_used_percent": 12.214815355109376
    },
    {
      "type": "training",
      "description": "Training step 2056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:45",
      "total_flops_so_far": 1.2220756413161472e+16,
      "budget_used_percent": 12.220756413161473
    },
    {
      "type": "training",
      "description": "Training step 2057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:45",
      "total_flops_so_far": 1.2226697471213568e+16,
      "budget_used_percent": 12.22669747121357
    },
    {
      "type": "training",
      "description": "Training step 2058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:45",
      "total_flops_so_far": 1.2232638529265664e+16,
      "budget_used_percent": 12.232638529265664
    },
    {
      "type": "training",
      "description": "Training step 2059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:46",
      "total_flops_so_far": 1.223857958731776e+16,
      "budget_used_percent": 12.23857958731776
    },
    {
      "type": "training",
      "description": "Training step 2060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:46",
      "total_flops_so_far": 1.2244520645369856e+16,
      "budget_used_percent": 12.244520645369857
    },
    {
      "type": "training",
      "description": "Training step 2061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:46",
      "total_flops_so_far": 1.2250461703421952e+16,
      "budget_used_percent": 12.250461703421953
    },
    {
      "type": "training",
      "description": "Training step 2062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:47",
      "total_flops_so_far": 1.2256402761474048e+16,
      "budget_used_percent": 12.256402761474048
    },
    {
      "type": "training",
      "description": "Training step 2063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:47",
      "total_flops_so_far": 1.2262343819526144e+16,
      "budget_used_percent": 12.262343819526142
    },
    {
      "type": "training",
      "description": "Training step 2064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:48",
      "total_flops_so_far": 1.226828487757824e+16,
      "budget_used_percent": 12.268284877578239
    },
    {
      "type": "training",
      "description": "Training step 2065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:48",
      "total_flops_so_far": 1.2274225935630336e+16,
      "budget_used_percent": 12.274225935630335
    },
    {
      "type": "training",
      "description": "Training step 2066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:48",
      "total_flops_so_far": 1.2280166993682432e+16,
      "budget_used_percent": 12.280166993682432
    },
    {
      "type": "training",
      "description": "Training step 2067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:49",
      "total_flops_so_far": 1.2286108051734528e+16,
      "budget_used_percent": 12.286108051734528
    },
    {
      "type": "training",
      "description": "Training step 2068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:49",
      "total_flops_so_far": 1.2292049109786624e+16,
      "budget_used_percent": 12.292049109786623
    },
    {
      "type": "training",
      "description": "Training step 2069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:49",
      "total_flops_so_far": 1.229799016783872e+16,
      "budget_used_percent": 12.29799016783872
    },
    {
      "type": "training",
      "description": "Training step 2070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:50",
      "total_flops_so_far": 1.2303931225890816e+16,
      "budget_used_percent": 12.303931225890816
    },
    {
      "type": "training",
      "description": "Training step 2071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:50",
      "total_flops_so_far": 1.2309872283942912e+16,
      "budget_used_percent": 12.309872283942912
    },
    {
      "type": "training",
      "description": "Training step 2072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:50",
      "total_flops_so_far": 1.2315813341995008e+16,
      "budget_used_percent": 12.315813341995007
    },
    {
      "type": "training",
      "description": "Training step 2073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:51",
      "total_flops_so_far": 1.2321754400047104e+16,
      "budget_used_percent": 12.321754400047103
    },
    {
      "type": "training",
      "description": "Training step 2074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:51",
      "total_flops_so_far": 1.23276954580992e+16,
      "budget_used_percent": 12.3276954580992
    },
    {
      "type": "training",
      "description": "Training step 2075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:51",
      "total_flops_so_far": 1.2333636516151296e+16,
      "budget_used_percent": 12.333636516151296
    },
    {
      "type": "training",
      "description": "Training step 2076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:52",
      "total_flops_so_far": 1.2339577574203392e+16,
      "budget_used_percent": 12.339577574203393
    },
    {
      "type": "training",
      "description": "Training step 2077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:52",
      "total_flops_so_far": 1.2345518632255488e+16,
      "budget_used_percent": 12.345518632255487
    },
    {
      "type": "training",
      "description": "Training step 2078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:52",
      "total_flops_so_far": 1.2351459690307584e+16,
      "budget_used_percent": 12.351459690307584
    },
    {
      "type": "training",
      "description": "Training step 2079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:53",
      "total_flops_so_far": 1.235740074835968e+16,
      "budget_used_percent": 12.35740074835968
    },
    {
      "type": "training",
      "description": "Training step 2080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:53",
      "total_flops_so_far": 1.2363341806411776e+16,
      "budget_used_percent": 12.363341806411777
    },
    {
      "type": "training",
      "description": "Training step 2081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:54",
      "total_flops_so_far": 1.2369282864463872e+16,
      "budget_used_percent": 12.369282864463873
    },
    {
      "type": "training",
      "description": "Training step 2082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:54",
      "total_flops_so_far": 1.2375223922515968e+16,
      "budget_used_percent": 12.375223922515968
    },
    {
      "type": "training",
      "description": "Training step 2083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:54",
      "total_flops_so_far": 1.2381164980568064e+16,
      "budget_used_percent": 12.381164980568064
    },
    {
      "type": "training",
      "description": "Training step 2084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:55",
      "total_flops_so_far": 1.238710603862016e+16,
      "budget_used_percent": 12.38710603862016
    },
    {
      "type": "training",
      "description": "Training step 2085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:55",
      "total_flops_so_far": 1.2393047096672256e+16,
      "budget_used_percent": 12.393047096672257
    },
    {
      "type": "training",
      "description": "Training step 2086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:55",
      "total_flops_so_far": 1.2398988154724352e+16,
      "budget_used_percent": 12.398988154724352
    },
    {
      "type": "training",
      "description": "Training step 2087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:56",
      "total_flops_so_far": 1.2404929212776448e+16,
      "budget_used_percent": 12.404929212776448
    },
    {
      "type": "training",
      "description": "Training step 2088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:56",
      "total_flops_so_far": 1.2410870270828544e+16,
      "budget_used_percent": 12.410870270828545
    },
    {
      "type": "training",
      "description": "Training step 2089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:56",
      "total_flops_so_far": 1.241681132888064e+16,
      "budget_used_percent": 12.416811328880641
    },
    {
      "type": "training",
      "description": "Training step 2090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:57",
      "total_flops_so_far": 1.2422752386932736e+16,
      "budget_used_percent": 12.422752386932737
    },
    {
      "type": "training",
      "description": "Training step 2091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:57",
      "total_flops_so_far": 1.2428693444984832e+16,
      "budget_used_percent": 12.428693444984832
    },
    {
      "type": "training",
      "description": "Training step 2092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:57",
      "total_flops_so_far": 1.2434634503036928e+16,
      "budget_used_percent": 12.434634503036927
    },
    {
      "type": "training",
      "description": "Training step 2093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:58",
      "total_flops_so_far": 1.2440575561089024e+16,
      "budget_used_percent": 12.440575561089023
    },
    {
      "type": "training",
      "description": "Training step 2094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:58",
      "total_flops_so_far": 1.244651661914112e+16,
      "budget_used_percent": 12.44651661914112
    },
    {
      "type": "training",
      "description": "Training step 2095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:58",
      "total_flops_so_far": 1.2452457677193216e+16,
      "budget_used_percent": 12.452457677193216
    },
    {
      "type": "training",
      "description": "Training step 2096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:38:59",
      "total_flops_so_far": 1.2458398735245312e+16,
      "budget_used_percent": 12.45839873524531
    },
    {
      "type": "training",
      "description": "Training step 2097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:00",
      "total_flops_so_far": 1.2464339793297408e+16,
      "budget_used_percent": 12.464339793297407
    },
    {
      "type": "training",
      "description": "Training step 2098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:00",
      "total_flops_so_far": 1.2470280851349504e+16,
      "budget_used_percent": 12.470280851349504
    },
    {
      "type": "training",
      "description": "Training step 2099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:00",
      "total_flops_so_far": 1.24762219094016e+16,
      "budget_used_percent": 12.4762219094016
    },
    {
      "type": "training",
      "description": "Training step 2100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:01",
      "total_flops_so_far": 1.2482162967453696e+16,
      "budget_used_percent": 12.482162967453696
    },
    {
      "type": "training",
      "description": "Training step 2101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:01",
      "total_flops_so_far": 1.2488104025505792e+16,
      "budget_used_percent": 12.488104025505791
    },
    {
      "type": "training",
      "description": "Training step 2102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:01",
      "total_flops_so_far": 1.2494045083557888e+16,
      "budget_used_percent": 12.494045083557888
    },
    {
      "type": "training",
      "description": "Training step 2103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:02",
      "total_flops_so_far": 1.2499986141609984e+16,
      "budget_used_percent": 12.499986141609984
    },
    {
      "type": "training",
      "description": "Training step 2104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:02",
      "total_flops_so_far": 1.250592719966208e+16,
      "budget_used_percent": 12.505927199662082
    },
    {
      "type": "training",
      "description": "Training step 2105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:02",
      "total_flops_so_far": 1.2511868257714176e+16,
      "budget_used_percent": 12.511868257714177
    },
    {
      "type": "training",
      "description": "Training step 2106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:03",
      "total_flops_so_far": 1.2517809315766272e+16,
      "budget_used_percent": 12.517809315766272
    },
    {
      "type": "training",
      "description": "Training step 2107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:03",
      "total_flops_so_far": 1.2523750373818368e+16,
      "budget_used_percent": 12.523750373818368
    },
    {
      "type": "training",
      "description": "Training step 2108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:03",
      "total_flops_so_far": 1.2529691431870464e+16,
      "budget_used_percent": 12.529691431870463
    },
    {
      "type": "training",
      "description": "Training step 2109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:04",
      "total_flops_so_far": 1.253563248992256e+16,
      "budget_used_percent": 12.53563248992256
    },
    {
      "type": "training",
      "description": "Training step 2110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:04",
      "total_flops_so_far": 1.2541573547974656e+16,
      "budget_used_percent": 12.541573547974656
    },
    {
      "type": "training",
      "description": "Training step 2111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:04",
      "total_flops_so_far": 1.2547514606026752e+16,
      "budget_used_percent": 12.547514606026752
    },
    {
      "type": "training",
      "description": "Training step 2112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:05",
      "total_flops_so_far": 1.2553455664078848e+16,
      "budget_used_percent": 12.553455664078847
    },
    {
      "type": "training",
      "description": "Training step 2113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:05",
      "total_flops_so_far": 1.2559396722130944e+16,
      "budget_used_percent": 12.559396722130945
    },
    {
      "type": "training",
      "description": "Training step 2114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:05",
      "total_flops_so_far": 1.256533778018304e+16,
      "budget_used_percent": 12.56533778018304
    },
    {
      "type": "training",
      "description": "Training step 2115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:06",
      "total_flops_so_far": 1.2571278838235136e+16,
      "budget_used_percent": 12.571278838235136
    },
    {
      "type": "training",
      "description": "Training step 2116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:06",
      "total_flops_so_far": 1.2577219896287232e+16,
      "budget_used_percent": 12.57721989628723
    },
    {
      "type": "training",
      "description": "Training step 2117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:06",
      "total_flops_so_far": 1.2583160954339328e+16,
      "budget_used_percent": 12.583160954339329
    },
    {
      "type": "training",
      "description": "Training step 2118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:07",
      "total_flops_so_far": 1.2589102012391424e+16,
      "budget_used_percent": 12.589102012391423
    },
    {
      "type": "training",
      "description": "Training step 2119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:07",
      "total_flops_so_far": 1.259504307044352e+16,
      "budget_used_percent": 12.59504307044352
    },
    {
      "type": "training",
      "description": "Training step 2120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:08",
      "total_flops_so_far": 1.2600984128495616e+16,
      "budget_used_percent": 12.600984128495615
    },
    {
      "type": "training",
      "description": "Training step 2121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:08",
      "total_flops_so_far": 1.2606925186547712e+16,
      "budget_used_percent": 12.606925186547713
    },
    {
      "type": "training",
      "description": "Training step 2122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:08",
      "total_flops_so_far": 1.2612866244599808e+16,
      "budget_used_percent": 12.612866244599807
    },
    {
      "type": "training",
      "description": "Training step 2123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:09",
      "total_flops_so_far": 1.2618807302651904e+16,
      "budget_used_percent": 12.618807302651906
    },
    {
      "type": "training",
      "description": "Training step 2124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:09",
      "total_flops_so_far": 1.2624748360704e+16,
      "budget_used_percent": 12.624748360704
    },
    {
      "type": "training",
      "description": "Training step 2125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:09",
      "total_flops_so_far": 1.2630689418756096e+16,
      "budget_used_percent": 12.630689418756097
    },
    {
      "type": "training",
      "description": "Training step 2126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:10",
      "total_flops_so_far": 1.2636630476808192e+16,
      "budget_used_percent": 12.636630476808191
    },
    {
      "type": "training",
      "description": "Training step 2127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:10",
      "total_flops_so_far": 1.2642571534860288e+16,
      "budget_used_percent": 12.64257153486029
    },
    {
      "type": "training",
      "description": "Training step 2128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:10",
      "total_flops_so_far": 1.2648512592912384e+16,
      "budget_used_percent": 12.648512592912384
    },
    {
      "type": "training",
      "description": "Training step 2129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:11",
      "total_flops_so_far": 1.265445365096448e+16,
      "budget_used_percent": 12.65445365096448
    },
    {
      "type": "training",
      "description": "Training step 2130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:11",
      "total_flops_so_far": 1.2660394709016576e+16,
      "budget_used_percent": 12.660394709016575
    },
    {
      "type": "training",
      "description": "Training step 2131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:11",
      "total_flops_so_far": 1.2666335767068672e+16,
      "budget_used_percent": 12.666335767068674
    },
    {
      "type": "training",
      "description": "Training step 2132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:12",
      "total_flops_so_far": 1.2672276825120768e+16,
      "budget_used_percent": 12.672276825120768
    },
    {
      "type": "training",
      "description": "Training step 2133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:12",
      "total_flops_so_far": 1.2678217883172864e+16,
      "budget_used_percent": 12.678217883172865
    },
    {
      "type": "training",
      "description": "Training step 2134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:12",
      "total_flops_so_far": 1.268415894122496e+16,
      "budget_used_percent": 12.68415894122496
    },
    {
      "type": "training",
      "description": "Training step 2135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:13",
      "total_flops_so_far": 1.2690099999277056e+16,
      "budget_used_percent": 12.690099999277054
    },
    {
      "type": "training",
      "description": "Training step 2136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:13",
      "total_flops_so_far": 1.2696041057329152e+16,
      "budget_used_percent": 12.696041057329152
    },
    {
      "type": "training",
      "description": "Training step 2137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:14",
      "total_flops_so_far": 1.2701982115381248e+16,
      "budget_used_percent": 12.701982115381247
    },
    {
      "type": "training",
      "description": "Training step 2138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:14",
      "total_flops_so_far": 1.2707923173433344e+16,
      "budget_used_percent": 12.707923173433343
    },
    {
      "type": "training",
      "description": "Training step 2139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:14",
      "total_flops_so_far": 1.271386423148544e+16,
      "budget_used_percent": 12.713864231485438
    },
    {
      "type": "training",
      "description": "Training step 2140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:15",
      "total_flops_so_far": 1.2719805289537536e+16,
      "budget_used_percent": 12.719805289537536
    },
    {
      "type": "training",
      "description": "Training step 2141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:15",
      "total_flops_so_far": 1.2725746347589632e+16,
      "budget_used_percent": 12.72574634758963
    },
    {
      "type": "training",
      "description": "Training step 2142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:15",
      "total_flops_so_far": 1.2731687405641728e+16,
      "budget_used_percent": 12.731687405641729
    },
    {
      "type": "training",
      "description": "Training step 2143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:16",
      "total_flops_so_far": 1.2737628463693824e+16,
      "budget_used_percent": 12.737628463693824
    },
    {
      "type": "training",
      "description": "Training step 2144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:16",
      "total_flops_so_far": 1.274356952174592e+16,
      "budget_used_percent": 12.74356952174592
    },
    {
      "type": "training",
      "description": "Training step 2145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:16",
      "total_flops_so_far": 1.2749510579798016e+16,
      "budget_used_percent": 12.749510579798015
    },
    {
      "type": "training",
      "description": "Training step 2146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:17",
      "total_flops_so_far": 1.2755451637850112e+16,
      "budget_used_percent": 12.755451637850113
    },
    {
      "type": "training",
      "description": "Training step 2147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:17",
      "total_flops_so_far": 1.2761392695902208e+16,
      "budget_used_percent": 12.761392695902208
    },
    {
      "type": "training",
      "description": "Training step 2148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:17",
      "total_flops_so_far": 1.2767333753954304e+16,
      "budget_used_percent": 12.767333753954304
    },
    {
      "type": "training",
      "description": "Training step 2149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:18",
      "total_flops_so_far": 1.27732748120064e+16,
      "budget_used_percent": 12.773274812006399
    },
    {
      "type": "training",
      "description": "Training step 2150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:18",
      "total_flops_so_far": 1.2779215870058496e+16,
      "budget_used_percent": 12.779215870058497
    },
    {
      "type": "training",
      "description": "Training step 2151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:18",
      "total_flops_so_far": 1.2785156928110592e+16,
      "budget_used_percent": 12.785156928110592
    },
    {
      "type": "training",
      "description": "Training step 2152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:19",
      "total_flops_so_far": 1.2791097986162688e+16,
      "budget_used_percent": 12.791097986162688
    },
    {
      "type": "training",
      "description": "Training step 2153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:19",
      "total_flops_so_far": 1.2797039044214784e+16,
      "budget_used_percent": 12.797039044214783
    },
    {
      "type": "training",
      "description": "Training step 2154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:20",
      "total_flops_so_far": 1.280298010226688e+16,
      "budget_used_percent": 12.802980102266881
    },
    {
      "type": "training",
      "description": "Training step 2155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:20",
      "total_flops_so_far": 1.2808921160318976e+16,
      "budget_used_percent": 12.808921160318976
    },
    {
      "type": "training",
      "description": "Training step 2156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:20",
      "total_flops_so_far": 1.2814862218371072e+16,
      "budget_used_percent": 12.814862218371074
    },
    {
      "type": "training",
      "description": "Training step 2157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:21",
      "total_flops_so_far": 1.2820803276423168e+16,
      "budget_used_percent": 12.820803276423169
    },
    {
      "type": "training",
      "description": "Training step 2158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:21",
      "total_flops_so_far": 1.2826744334475264e+16,
      "budget_used_percent": 12.826744334475265
    },
    {
      "type": "training",
      "description": "Training step 2159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:21",
      "total_flops_so_far": 1.283268539252736e+16,
      "budget_used_percent": 12.83268539252736
    },
    {
      "type": "training",
      "description": "Training step 2160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:22",
      "total_flops_so_far": 1.2838626450579456e+16,
      "budget_used_percent": 12.838626450579458
    },
    {
      "type": "training",
      "description": "Training step 2161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:22",
      "total_flops_so_far": 1.2844567508631552e+16,
      "budget_used_percent": 12.844567508631552
    },
    {
      "type": "training",
      "description": "Training step 2162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:22",
      "total_flops_so_far": 1.2850508566683648e+16,
      "budget_used_percent": 12.850508566683649
    },
    {
      "type": "training",
      "description": "Training step 2163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:23",
      "total_flops_so_far": 1.2856449624735744e+16,
      "budget_used_percent": 12.856449624735744
    },
    {
      "type": "training",
      "description": "Training step 2164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:23",
      "total_flops_so_far": 1.286239068278784e+16,
      "budget_used_percent": 12.862390682787838
    },
    {
      "type": "training",
      "description": "Training step 2165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:23",
      "total_flops_so_far": 1.2868331740839936e+16,
      "budget_used_percent": 12.868331740839936
    },
    {
      "type": "training",
      "description": "Training step 2166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:24",
      "total_flops_so_far": 1.2874272798892032e+16,
      "budget_used_percent": 12.874272798892031
    },
    {
      "type": "training",
      "description": "Training step 2167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:24",
      "total_flops_so_far": 1.2880213856944128e+16,
      "budget_used_percent": 12.880213856944128
    },
    {
      "type": "training",
      "description": "Training step 2168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:25",
      "total_flops_so_far": 1.2886154914996224e+16,
      "budget_used_percent": 12.886154914996222
    },
    {
      "type": "training",
      "description": "Training step 2169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:25",
      "total_flops_so_far": 1.289209597304832e+16,
      "budget_used_percent": 12.89209597304832
    },
    {
      "type": "training",
      "description": "Training step 2170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:25",
      "total_flops_so_far": 1.2898037031100416e+16,
      "budget_used_percent": 12.898037031100415
    },
    {
      "type": "training",
      "description": "Training step 2171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:26",
      "total_flops_so_far": 1.2903978089152512e+16,
      "budget_used_percent": 12.903978089152513
    },
    {
      "type": "training",
      "description": "Training step 2172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:26",
      "total_flops_so_far": 1.2909919147204608e+16,
      "budget_used_percent": 12.909919147204608
    },
    {
      "type": "training",
      "description": "Training step 2173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:26",
      "total_flops_so_far": 1.2915860205256704e+16,
      "budget_used_percent": 12.915860205256704
    },
    {
      "type": "training",
      "description": "Training step 2174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:27",
      "total_flops_so_far": 1.29218012633088e+16,
      "budget_used_percent": 12.921801263308799
    },
    {
      "type": "training",
      "description": "Training step 2175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:27",
      "total_flops_so_far": 1.2927742321360896e+16,
      "budget_used_percent": 12.927742321360897
    },
    {
      "type": "training",
      "description": "Training step 2176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:27",
      "total_flops_so_far": 1.2933683379412992e+16,
      "budget_used_percent": 12.933683379412992
    },
    {
      "type": "training",
      "description": "Training step 2177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:28",
      "total_flops_so_far": 1.2939624437465088e+16,
      "budget_used_percent": 12.939624437465088
    },
    {
      "type": "training",
      "description": "Training step 2178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:28",
      "total_flops_so_far": 1.2945565495517184e+16,
      "budget_used_percent": 12.945565495517183
    },
    {
      "type": "training",
      "description": "Training step 2179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:28",
      "total_flops_so_far": 1.295150655356928e+16,
      "budget_used_percent": 12.951506553569281
    },
    {
      "type": "training",
      "description": "Training step 2180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:29",
      "total_flops_so_far": 1.2957447611621376e+16,
      "budget_used_percent": 12.957447611621376
    },
    {
      "type": "training",
      "description": "Training step 2181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:29",
      "total_flops_so_far": 1.2963388669673472e+16,
      "budget_used_percent": 12.963388669673472
    },
    {
      "type": "training",
      "description": "Training step 2182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:29",
      "total_flops_so_far": 1.2969329727725568e+16,
      "budget_used_percent": 12.969329727725567
    },
    {
      "type": "training",
      "description": "Training step 2183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:30",
      "total_flops_so_far": 1.2975270785777664e+16,
      "budget_used_percent": 12.975270785777665
    },
    {
      "type": "training",
      "description": "Training step 2184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:30",
      "total_flops_so_far": 1.298121184382976e+16,
      "budget_used_percent": 12.98121184382976
    },
    {
      "type": "training",
      "description": "Training step 2185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:31",
      "total_flops_so_far": 1.2987152901881856e+16,
      "budget_used_percent": 12.987152901881856
    },
    {
      "type": "training",
      "description": "Training step 2186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:31",
      "total_flops_so_far": 1.2993093959933952e+16,
      "budget_used_percent": 12.993093959933951
    },
    {
      "type": "training",
      "description": "Training step 2187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:31",
      "total_flops_so_far": 1.2999035017986048e+16,
      "budget_used_percent": 12.99903501798605
    },
    {
      "type": "training",
      "description": "Training step 2188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:32",
      "total_flops_so_far": 1.3004976076038144e+16,
      "budget_used_percent": 13.004976076038144
    },
    {
      "type": "training",
      "description": "Training step 2189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:32",
      "total_flops_so_far": 1.301091713409024e+16,
      "budget_used_percent": 13.010917134090242
    },
    {
      "type": "training",
      "description": "Training step 2190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:32",
      "total_flops_so_far": 1.3016858192142336e+16,
      "budget_used_percent": 13.016858192142337
    },
    {
      "type": "training",
      "description": "Training step 2191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:33",
      "total_flops_so_far": 1.3022799250194432e+16,
      "budget_used_percent": 13.022799250194431
    },
    {
      "type": "training",
      "description": "Training step 2192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:33",
      "total_flops_so_far": 1.3028740308246528e+16,
      "budget_used_percent": 13.028740308246528
    },
    {
      "type": "training",
      "description": "Training step 2193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:33",
      "total_flops_so_far": 1.3034681366298624e+16,
      "budget_used_percent": 13.034681366298623
    },
    {
      "type": "training",
      "description": "Training step 2194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:34",
      "total_flops_so_far": 1.304062242435072e+16,
      "budget_used_percent": 13.04062242435072
    },
    {
      "type": "training",
      "description": "Training step 2195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:34",
      "total_flops_so_far": 1.3046563482402816e+16,
      "budget_used_percent": 13.046563482402815
    },
    {
      "type": "training",
      "description": "Training step 2196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:34",
      "total_flops_so_far": 1.3052504540454912e+16,
      "budget_used_percent": 13.052504540454912
    },
    {
      "type": "training",
      "description": "Training step 2197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:35",
      "total_flops_so_far": 1.3058445598507008e+16,
      "budget_used_percent": 13.058445598507006
    },
    {
      "type": "training",
      "description": "Training step 2198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:35",
      "total_flops_so_far": 1.3064386656559104e+16,
      "budget_used_percent": 13.064386656559105
    },
    {
      "type": "training",
      "description": "Training step 2199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:36",
      "total_flops_so_far": 1.30703277146112e+16,
      "budget_used_percent": 13.0703277146112
    },
    {
      "type": "training",
      "description": "Training step 2200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:36",
      "total_flops_so_far": 1.3076268772663296e+16,
      "budget_used_percent": 13.076268772663296
    },
    {
      "type": "training",
      "description": "Training step 2201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:36",
      "total_flops_so_far": 1.3082209830715392e+16,
      "budget_used_percent": 13.08220983071539
    },
    {
      "type": "training",
      "description": "Training step 2202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:37",
      "total_flops_so_far": 1.3088150888767488e+16,
      "budget_used_percent": 13.088150888767489
    },
    {
      "type": "training",
      "description": "Training step 2203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:37",
      "total_flops_so_far": 1.3094091946819584e+16,
      "budget_used_percent": 13.094091946819583
    },
    {
      "type": "training",
      "description": "Training step 2204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:37",
      "total_flops_so_far": 1.310003300487168e+16,
      "budget_used_percent": 13.10003300487168
    },
    {
      "type": "training",
      "description": "Training step 2205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:38",
      "total_flops_so_far": 1.3105974062923776e+16,
      "budget_used_percent": 13.105974062923774
    },
    {
      "type": "training",
      "description": "Training step 2206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:38",
      "total_flops_so_far": 1.3111915120975872e+16,
      "budget_used_percent": 13.111915120975873
    },
    {
      "type": "training",
      "description": "Training step 2207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:38",
      "total_flops_so_far": 1.3117856179027968e+16,
      "budget_used_percent": 13.117856179027967
    },
    {
      "type": "training",
      "description": "Training step 2208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:39",
      "total_flops_so_far": 1.3123797237080064e+16,
      "budget_used_percent": 13.123797237080066
    },
    {
      "type": "training",
      "description": "Training step 2209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:39",
      "total_flops_so_far": 1.312973829513216e+16,
      "budget_used_percent": 13.12973829513216
    },
    {
      "type": "training",
      "description": "Training step 2210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:39",
      "total_flops_so_far": 1.3135679353184256e+16,
      "budget_used_percent": 13.135679353184257
    },
    {
      "type": "training",
      "description": "Training step 2211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:40",
      "total_flops_so_far": 1.3141620411236352e+16,
      "budget_used_percent": 13.141620411236351
    },
    {
      "type": "training",
      "description": "Training step 2212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:40",
      "total_flops_so_far": 1.3147561469288448e+16,
      "budget_used_percent": 13.14756146928845
    },
    {
      "type": "training",
      "description": "Training step 2213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:41",
      "total_flops_so_far": 1.3153502527340544e+16,
      "budget_used_percent": 13.153502527340544
    },
    {
      "type": "training",
      "description": "Training step 2214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:41",
      "total_flops_so_far": 1.315944358539264e+16,
      "budget_used_percent": 13.15944358539264
    },
    {
      "type": "training",
      "description": "Training step 2215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:41",
      "total_flops_so_far": 1.3165384643444736e+16,
      "budget_used_percent": 13.165384643444735
    },
    {
      "type": "training",
      "description": "Training step 2216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:42",
      "total_flops_so_far": 1.3171325701496832e+16,
      "budget_used_percent": 13.171325701496833
    },
    {
      "type": "training",
      "description": "Training step 2217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:42",
      "total_flops_so_far": 1.3177266759548928e+16,
      "budget_used_percent": 13.177266759548928
    },
    {
      "type": "training",
      "description": "Training step 2218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:42",
      "total_flops_so_far": 1.3183207817601024e+16,
      "budget_used_percent": 13.183207817601025
    },
    {
      "type": "training",
      "description": "Training step 2219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:43",
      "total_flops_so_far": 1.318914887565312e+16,
      "budget_used_percent": 13.18914887565312
    },
    {
      "type": "training",
      "description": "Training step 2220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:43",
      "total_flops_so_far": 1.3195089933705216e+16,
      "budget_used_percent": 13.195089933705214
    },
    {
      "type": "training",
      "description": "Training step 2221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:43",
      "total_flops_so_far": 1.3201030991757312e+16,
      "budget_used_percent": 13.201030991757312
    },
    {
      "type": "training",
      "description": "Training step 2222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:44",
      "total_flops_so_far": 1.3206972049809408e+16,
      "budget_used_percent": 13.206972049809407
    },
    {
      "type": "training",
      "description": "Training step 2223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:44",
      "total_flops_so_far": 1.3212913107861504e+16,
      "budget_used_percent": 13.212913107861505
    },
    {
      "type": "training",
      "description": "Training step 2224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:44",
      "total_flops_so_far": 1.32188541659136e+16,
      "budget_used_percent": 13.2188541659136
    },
    {
      "type": "training",
      "description": "Training step 2225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:45",
      "total_flops_so_far": 1.3224795223965696e+16,
      "budget_used_percent": 13.224795223965696
    },
    {
      "type": "training",
      "description": "Training step 2226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:45",
      "total_flops_so_far": 1.3230736282017792e+16,
      "budget_used_percent": 13.23073628201779
    },
    {
      "type": "training",
      "description": "Training step 2227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:46",
      "total_flops_so_far": 1.3236677340069888e+16,
      "budget_used_percent": 13.236677340069889
    },
    {
      "type": "training",
      "description": "Training step 2228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:46",
      "total_flops_so_far": 1.3242618398121984e+16,
      "budget_used_percent": 13.242618398121984
    },
    {
      "type": "training",
      "description": "Training step 2229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:46",
      "total_flops_so_far": 1.324855945617408e+16,
      "budget_used_percent": 13.24855945617408
    },
    {
      "type": "training",
      "description": "Training step 2230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:47",
      "total_flops_so_far": 1.3254500514226176e+16,
      "budget_used_percent": 13.254500514226175
    },
    {
      "type": "training",
      "description": "Training step 2231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:47",
      "total_flops_so_far": 1.3260441572278272e+16,
      "budget_used_percent": 13.260441572278273
    },
    {
      "type": "training",
      "description": "Training step 2232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:47",
      "total_flops_so_far": 1.3266382630330368e+16,
      "budget_used_percent": 13.266382630330368
    },
    {
      "type": "training",
      "description": "Training step 2233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:48",
      "total_flops_so_far": 1.3272323688382464e+16,
      "budget_used_percent": 13.272323688382464
    },
    {
      "type": "training",
      "description": "Training step 2234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:48",
      "total_flops_so_far": 1.327826474643456e+16,
      "budget_used_percent": 13.278264746434559
    },
    {
      "type": "training",
      "description": "Training step 2235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:48",
      "total_flops_so_far": 1.3284205804486656e+16,
      "budget_used_percent": 13.284205804486657
    },
    {
      "type": "training",
      "description": "Training step 2236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:49",
      "total_flops_so_far": 1.3290146862538752e+16,
      "budget_used_percent": 13.290146862538752
    },
    {
      "type": "training",
      "description": "Training step 2237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:49",
      "total_flops_so_far": 1.3296087920590848e+16,
      "budget_used_percent": 13.29608792059085
    },
    {
      "type": "training",
      "description": "Training step 2238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:49",
      "total_flops_so_far": 1.3302028978642944e+16,
      "budget_used_percent": 13.302028978642944
    },
    {
      "type": "training",
      "description": "Training step 2239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:50",
      "total_flops_so_far": 1.330797003669504e+16,
      "budget_used_percent": 13.30797003669504
    },
    {
      "type": "training",
      "description": "Training step 2240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:50",
      "total_flops_so_far": 1.3313911094747136e+16,
      "budget_used_percent": 13.313911094747136
    },
    {
      "type": "training",
      "description": "Training step 2241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:51",
      "total_flops_so_far": 1.3319852152799232e+16,
      "budget_used_percent": 13.319852152799234
    },
    {
      "type": "training",
      "description": "Training step 2242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:51",
      "total_flops_so_far": 1.3325793210851328e+16,
      "budget_used_percent": 13.325793210851328
    },
    {
      "type": "training",
      "description": "Training step 2243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:51",
      "total_flops_so_far": 1.3331734268903424e+16,
      "budget_used_percent": 13.331734268903425
    },
    {
      "type": "training",
      "description": "Training step 2244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:52",
      "total_flops_so_far": 1.333767532695552e+16,
      "budget_used_percent": 13.33767532695552
    },
    {
      "type": "training",
      "description": "Training step 2245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:52",
      "total_flops_so_far": 1.3343616385007616e+16,
      "budget_used_percent": 13.343616385007618
    },
    {
      "type": "training",
      "description": "Training step 2246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:52",
      "total_flops_so_far": 1.3349557443059712e+16,
      "budget_used_percent": 13.349557443059712
    },
    {
      "type": "training",
      "description": "Training step 2247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:53",
      "total_flops_so_far": 1.3355498501111808e+16,
      "budget_used_percent": 13.355498501111809
    },
    {
      "type": "training",
      "description": "Training step 2248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:53",
      "total_flops_so_far": 1.3361439559163904e+16,
      "budget_used_percent": 13.361439559163903
    },
    {
      "type": "training",
      "description": "Training step 2249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:53",
      "total_flops_so_far": 1.3367380617216e+16,
      "budget_used_percent": 13.367380617215998
    },
    {
      "type": "training",
      "description": "Training step 2250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:54",
      "total_flops_so_far": 1.3373321675268096e+16,
      "budget_used_percent": 13.373321675268096
    },
    {
      "type": "training",
      "description": "Training step 2251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:54",
      "total_flops_so_far": 1.3379262733320192e+16,
      "budget_used_percent": 13.379262733320191
    },
    {
      "type": "training",
      "description": "Training step 2252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:54",
      "total_flops_so_far": 1.3385203791372288e+16,
      "budget_used_percent": 13.385203791372287
    },
    {
      "type": "training",
      "description": "Training step 2253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:55",
      "total_flops_so_far": 1.3391144849424384e+16,
      "budget_used_percent": 13.391144849424382
    },
    {
      "type": "training",
      "description": "Training step 2254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:55",
      "total_flops_so_far": 1.339708590747648e+16,
      "budget_used_percent": 13.39708590747648
    },
    {
      "type": "training",
      "description": "Training step 2255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:56",
      "total_flops_so_far": 1.3403026965528576e+16,
      "budget_used_percent": 13.403026965528575
    },
    {
      "type": "training",
      "description": "Training step 2256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:56",
      "total_flops_so_far": 1.3408968023580672e+16,
      "budget_used_percent": 13.408968023580673
    },
    {
      "type": "training",
      "description": "Training step 2257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:56",
      "total_flops_so_far": 1.3414909081632768e+16,
      "budget_used_percent": 13.414909081632768
    },
    {
      "type": "training",
      "description": "Training step 2258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:57",
      "total_flops_so_far": 1.3420850139684864e+16,
      "budget_used_percent": 13.420850139684864
    },
    {
      "type": "training",
      "description": "Training step 2259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:57",
      "total_flops_so_far": 1.342679119773696e+16,
      "budget_used_percent": 13.426791197736959
    },
    {
      "type": "training",
      "description": "Training step 2260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:57",
      "total_flops_so_far": 1.3432732255789056e+16,
      "budget_used_percent": 13.432732255789057
    },
    {
      "type": "training",
      "description": "Training step 2261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:58",
      "total_flops_so_far": 1.3438673313841152e+16,
      "budget_used_percent": 13.438673313841152
    },
    {
      "type": "training",
      "description": "Training step 2262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:58",
      "total_flops_so_far": 1.3444614371893248e+16,
      "budget_used_percent": 13.444614371893248
    },
    {
      "type": "training",
      "description": "Training step 2263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:58",
      "total_flops_so_far": 1.3450555429945344e+16,
      "budget_used_percent": 13.450555429945343
    },
    {
      "type": "training",
      "description": "Training step 2264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:59",
      "total_flops_so_far": 1.345649648799744e+16,
      "budget_used_percent": 13.456496487997441
    },
    {
      "type": "training",
      "description": "Training step 2265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:59",
      "total_flops_so_far": 1.3462437546049536e+16,
      "budget_used_percent": 13.462437546049536
    },
    {
      "type": "training",
      "description": "Training step 2266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:39:59",
      "total_flops_so_far": 1.3468378604101632e+16,
      "budget_used_percent": 13.468378604101632
    },
    {
      "type": "training",
      "description": "Training step 2267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:00",
      "total_flops_so_far": 1.3474319662153728e+16,
      "budget_used_percent": 13.474319662153727
    },
    {
      "type": "training",
      "description": "Training step 2268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:00",
      "total_flops_so_far": 1.3480260720205824e+16,
      "budget_used_percent": 13.480260720205825
    },
    {
      "type": "training",
      "description": "Training step 2269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:01",
      "total_flops_so_far": 1.348620177825792e+16,
      "budget_used_percent": 13.48620177825792
    },
    {
      "type": "training",
      "description": "Training step 2270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:01",
      "total_flops_so_far": 1.3492142836310016e+16,
      "budget_used_percent": 13.492142836310016
    },
    {
      "type": "training",
      "description": "Training step 2271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:01",
      "total_flops_so_far": 1.3498083894362112e+16,
      "budget_used_percent": 13.49808389436211
    },
    {
      "type": "training",
      "description": "Training step 2272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:02",
      "total_flops_so_far": 1.3504024952414208e+16,
      "budget_used_percent": 13.504024952414209
    },
    {
      "type": "training",
      "description": "Training step 2273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:02",
      "total_flops_so_far": 1.3509966010466304e+16,
      "budget_used_percent": 13.509966010466304
    },
    {
      "type": "training",
      "description": "Training step 2274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:02",
      "total_flops_so_far": 1.35159070685184e+16,
      "budget_used_percent": 13.515907068518402
    },
    {
      "type": "training",
      "description": "Training step 2275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:03",
      "total_flops_so_far": 1.3521848126570496e+16,
      "budget_used_percent": 13.521848126570497
    },
    {
      "type": "training",
      "description": "Training step 2276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:03",
      "total_flops_so_far": 1.3527789184622592e+16,
      "budget_used_percent": 13.527789184622591
    },
    {
      "type": "training",
      "description": "Training step 2277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:03",
      "total_flops_so_far": 1.3533730242674688e+16,
      "budget_used_percent": 13.533730242674688
    },
    {
      "type": "training",
      "description": "Training step 2278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:04",
      "total_flops_so_far": 1.3539671300726784e+16,
      "budget_used_percent": 13.539671300726782
    },
    {
      "type": "training",
      "description": "Training step 2279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:04",
      "total_flops_so_far": 1.354561235877888e+16,
      "budget_used_percent": 13.54561235877888
    },
    {
      "type": "training",
      "description": "Training step 2280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:04",
      "total_flops_so_far": 1.3551553416830976e+16,
      "budget_used_percent": 13.551553416830975
    },
    {
      "type": "training",
      "description": "Training step 2281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:05",
      "total_flops_so_far": 1.3557494474883072e+16,
      "budget_used_percent": 13.557494474883072
    },
    {
      "type": "training",
      "description": "Training step 2282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:05",
      "total_flops_so_far": 1.3563435532935168e+16,
      "budget_used_percent": 13.563435532935166
    },
    {
      "type": "training",
      "description": "Training step 2283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:06",
      "total_flops_so_far": 1.3569376590987264e+16,
      "budget_used_percent": 13.569376590987265
    },
    {
      "type": "training",
      "description": "Training step 2284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:06",
      "total_flops_so_far": 1.357531764903936e+16,
      "budget_used_percent": 13.57531764903936
    },
    {
      "type": "training",
      "description": "Training step 2285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:06",
      "total_flops_so_far": 1.3581258707091456e+16,
      "budget_used_percent": 13.581258707091456
    },
    {
      "type": "training",
      "description": "Training step 2286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:07",
      "total_flops_so_far": 1.3587199765143552e+16,
      "budget_used_percent": 13.58719976514355
    },
    {
      "type": "training",
      "description": "Training step 2287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:07",
      "total_flops_so_far": 1.3593140823195648e+16,
      "budget_used_percent": 13.593140823195649
    },
    {
      "type": "training",
      "description": "Training step 2288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:07",
      "total_flops_so_far": 1.3599081881247744e+16,
      "budget_used_percent": 13.599081881247743
    },
    {
      "type": "training",
      "description": "Training step 2289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:08",
      "total_flops_so_far": 1.360502293929984e+16,
      "budget_used_percent": 13.605022939299841
    },
    {
      "type": "training",
      "description": "Training step 2290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:08",
      "total_flops_so_far": 1.3610963997351936e+16,
      "budget_used_percent": 13.610963997351936
    },
    {
      "type": "training",
      "description": "Training step 2291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:08",
      "total_flops_so_far": 1.3616905055404032e+16,
      "budget_used_percent": 13.616905055404033
    },
    {
      "type": "training",
      "description": "Training step 2292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:09",
      "total_flops_so_far": 1.3622846113456128e+16,
      "budget_used_percent": 13.622846113456127
    },
    {
      "type": "training",
      "description": "Training step 2293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:09",
      "total_flops_so_far": 1.3628787171508224e+16,
      "budget_used_percent": 13.628787171508225
    },
    {
      "type": "training",
      "description": "Training step 2294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:09",
      "total_flops_so_far": 1.363472822956032e+16,
      "budget_used_percent": 13.63472822956032
    },
    {
      "type": "training",
      "description": "Training step 2295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:10",
      "total_flops_so_far": 1.3640669287612416e+16,
      "budget_used_percent": 13.640669287612416
    },
    {
      "type": "training",
      "description": "Training step 2296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:10",
      "total_flops_so_far": 1.3646610345664512e+16,
      "budget_used_percent": 13.646610345664511
    },
    {
      "type": "training",
      "description": "Training step 2297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:11",
      "total_flops_so_far": 1.3652551403716608e+16,
      "budget_used_percent": 13.65255140371661
    },
    {
      "type": "training",
      "description": "Training step 2298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:11",
      "total_flops_so_far": 1.3658492461768704e+16,
      "budget_used_percent": 13.658492461768704
    },
    {
      "type": "training",
      "description": "Training step 2299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:11",
      "total_flops_so_far": 1.36644335198208e+16,
      "budget_used_percent": 13.6644335198208
    },
    {
      "type": "training",
      "description": "Training step 2300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:12",
      "total_flops_so_far": 1.3670374577872896e+16,
      "budget_used_percent": 13.670374577872895
    },
    {
      "type": "training",
      "description": "Training step 2301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:12",
      "total_flops_so_far": 1.3676315635924992e+16,
      "budget_used_percent": 13.676315635924993
    },
    {
      "type": "training",
      "description": "Training step 2302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:12",
      "total_flops_so_far": 1.3682256693977088e+16,
      "budget_used_percent": 13.682256693977088
    },
    {
      "type": "training",
      "description": "Training step 2303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:13",
      "total_flops_so_far": 1.3688197752029184e+16,
      "budget_used_percent": 13.688197752029186
    },
    {
      "type": "training",
      "description": "Training step 2304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:13",
      "total_flops_so_far": 1.369413881008128e+16,
      "budget_used_percent": 13.69413881008128
    },
    {
      "type": "training",
      "description": "Training step 2305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:13",
      "total_flops_so_far": 1.3700079868133376e+16,
      "budget_used_percent": 13.700079868133376
    },
    {
      "type": "training",
      "description": "Training step 2306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:14",
      "total_flops_so_far": 1.3706020926185472e+16,
      "budget_used_percent": 13.706020926185472
    },
    {
      "type": "training",
      "description": "Training step 2307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:14",
      "total_flops_so_far": 1.3711961984237568e+16,
      "budget_used_percent": 13.711961984237567
    },
    {
      "type": "training",
      "description": "Training step 2308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:14",
      "total_flops_so_far": 1.3717903042289664e+16,
      "budget_used_percent": 13.717903042289665
    },
    {
      "type": "training",
      "description": "Training step 2309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:15",
      "total_flops_so_far": 1.372384410034176e+16,
      "budget_used_percent": 13.72384410034176
    },
    {
      "type": "training",
      "description": "Training step 2310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:15",
      "total_flops_so_far": 1.3729785158393856e+16,
      "budget_used_percent": 13.729785158393856
    },
    {
      "type": "training",
      "description": "Training step 2311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:16",
      "total_flops_so_far": 1.3735726216445952e+16,
      "budget_used_percent": 13.73572621644595
    },
    {
      "type": "training",
      "description": "Training step 2312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:16",
      "total_flops_so_far": 1.3741667274498048e+16,
      "budget_used_percent": 13.741667274498049
    },
    {
      "type": "training",
      "description": "Training step 2313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:16",
      "total_flops_so_far": 1.3747608332550144e+16,
      "budget_used_percent": 13.747608332550143
    },
    {
      "type": "training",
      "description": "Training step 2314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:17",
      "total_flops_so_far": 1.375354939060224e+16,
      "budget_used_percent": 13.75354939060224
    },
    {
      "type": "training",
      "description": "Training step 2315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:17",
      "total_flops_so_far": 1.3759490448654336e+16,
      "budget_used_percent": 13.759490448654335
    },
    {
      "type": "training",
      "description": "Training step 2316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:17",
      "total_flops_so_far": 1.3765431506706432e+16,
      "budget_used_percent": 13.765431506706433
    },
    {
      "type": "training",
      "description": "Training step 2317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:18",
      "total_flops_so_far": 1.3771372564758528e+16,
      "budget_used_percent": 13.771372564758527
    },
    {
      "type": "training",
      "description": "Training step 2318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:18",
      "total_flops_so_far": 1.3777313622810624e+16,
      "budget_used_percent": 13.777313622810624
    },
    {
      "type": "training",
      "description": "Training step 2319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:18",
      "total_flops_so_far": 1.378325468086272e+16,
      "budget_used_percent": 13.783254680862719
    },
    {
      "type": "training",
      "description": "Training step 2320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:19",
      "total_flops_so_far": 1.3789195738914816e+16,
      "budget_used_percent": 13.789195738914817
    },
    {
      "type": "training",
      "description": "Training step 2321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:19",
      "total_flops_so_far": 1.3795136796966912e+16,
      "budget_used_percent": 13.795136796966911
    },
    {
      "type": "training",
      "description": "Training step 2322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:20",
      "total_flops_so_far": 1.3801077855019008e+16,
      "budget_used_percent": 13.80107785501901
    },
    {
      "type": "training",
      "description": "Training step 2323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:20",
      "total_flops_so_far": 1.3807018913071104e+16,
      "budget_used_percent": 13.807018913071104
    },
    {
      "type": "training",
      "description": "Training step 2324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:20",
      "total_flops_so_far": 1.38129599711232e+16,
      "budget_used_percent": 13.8129599711232
    },
    {
      "type": "training",
      "description": "Training step 2325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:21",
      "total_flops_so_far": 1.3818901029175296e+16,
      "budget_used_percent": 13.818901029175295
    },
    {
      "type": "training",
      "description": "Training step 2326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:21",
      "total_flops_so_far": 1.3824842087227392e+16,
      "budget_used_percent": 13.824842087227394
    },
    {
      "type": "training",
      "description": "Training step 2327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:21",
      "total_flops_so_far": 1.3830783145279488e+16,
      "budget_used_percent": 13.830783145279488
    },
    {
      "type": "training",
      "description": "Training step 2328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:22",
      "total_flops_so_far": 1.3836724203331584e+16,
      "budget_used_percent": 13.836724203331585
    },
    {
      "type": "training",
      "description": "Training step 2329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:22",
      "total_flops_so_far": 1.384266526138368e+16,
      "budget_used_percent": 13.84266526138368
    },
    {
      "type": "training",
      "description": "Training step 2330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:22",
      "total_flops_so_far": 1.3848606319435776e+16,
      "budget_used_percent": 13.848606319435778
    },
    {
      "type": "training",
      "description": "Training step 2331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:23",
      "total_flops_so_far": 1.3854547377487872e+16,
      "budget_used_percent": 13.854547377487872
    },
    {
      "type": "training",
      "description": "Training step 2332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:23",
      "total_flops_so_far": 1.3860488435539968e+16,
      "budget_used_percent": 13.860488435539969
    },
    {
      "type": "training",
      "description": "Training step 2333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:23",
      "total_flops_so_far": 1.3866429493592064e+16,
      "budget_used_percent": 13.866429493592063
    },
    {
      "type": "training",
      "description": "Training step 2334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:24",
      "total_flops_so_far": 1.387237055164416e+16,
      "budget_used_percent": 13.872370551644158
    },
    {
      "type": "training",
      "description": "Training step 2335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:24",
      "total_flops_so_far": 1.3878311609696256e+16,
      "budget_used_percent": 13.878311609696256
    },
    {
      "type": "training",
      "description": "Training step 2336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:25",
      "total_flops_so_far": 1.3884252667748352e+16,
      "budget_used_percent": 13.88425266774835
    },
    {
      "type": "training",
      "description": "Training step 2337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:25",
      "total_flops_so_far": 1.3890193725800448e+16,
      "budget_used_percent": 13.890193725800447
    },
    {
      "type": "training",
      "description": "Training step 2338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:26",
      "total_flops_so_far": 1.3896134783852544e+16,
      "budget_used_percent": 13.896134783852542
    },
    {
      "type": "training",
      "description": "Training step 2339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:26",
      "total_flops_so_far": 1.390207584190464e+16,
      "budget_used_percent": 13.90207584190464
    },
    {
      "type": "training",
      "description": "Training step 2340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:26",
      "total_flops_so_far": 1.3908016899956736e+16,
      "budget_used_percent": 13.908016899956735
    },
    {
      "type": "training",
      "description": "Training step 2341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:27",
      "total_flops_so_far": 1.3913957958008832e+16,
      "budget_used_percent": 13.913957958008833
    },
    {
      "type": "training",
      "description": "Training step 2342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:27",
      "total_flops_so_far": 1.3919899016060928e+16,
      "budget_used_percent": 13.919899016060928
    },
    {
      "type": "training",
      "description": "Training step 2343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:27",
      "total_flops_so_far": 1.3925840074113024e+16,
      "budget_used_percent": 13.925840074113024
    },
    {
      "type": "training",
      "description": "Training step 2344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:28",
      "total_flops_so_far": 1.393178113216512e+16,
      "budget_used_percent": 13.931781132165119
    },
    {
      "type": "training",
      "description": "Training step 2345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:28",
      "total_flops_so_far": 1.3937722190217216e+16,
      "budget_used_percent": 13.937722190217217
    },
    {
      "type": "training",
      "description": "Training step 2346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:28",
      "total_flops_so_far": 1.3943663248269312e+16,
      "budget_used_percent": 13.943663248269312
    },
    {
      "type": "training",
      "description": "Training step 2347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:29",
      "total_flops_so_far": 1.3949604306321408e+16,
      "budget_used_percent": 13.949604306321408
    },
    {
      "type": "training",
      "description": "Training step 2348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:29",
      "total_flops_so_far": 1.3955545364373504e+16,
      "budget_used_percent": 13.955545364373503
    },
    {
      "type": "training",
      "description": "Training step 2349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:30",
      "total_flops_so_far": 1.39614864224256e+16,
      "budget_used_percent": 13.961486422425601
    },
    {
      "type": "training",
      "description": "Training step 2350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:30",
      "total_flops_so_far": 1.3967427480477696e+16,
      "budget_used_percent": 13.967427480477696
    },
    {
      "type": "training",
      "description": "Training step 2351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:30",
      "total_flops_so_far": 1.3973368538529792e+16,
      "budget_used_percent": 13.973368538529792
    },
    {
      "type": "training",
      "description": "Training step 2352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:31",
      "total_flops_so_far": 1.3979309596581888e+16,
      "budget_used_percent": 13.979309596581887
    },
    {
      "type": "training",
      "description": "Training step 2353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:31",
      "total_flops_so_far": 1.3985250654633984e+16,
      "budget_used_percent": 13.985250654633985
    },
    {
      "type": "training",
      "description": "Training step 2354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:31",
      "total_flops_so_far": 1.399119171268608e+16,
      "budget_used_percent": 13.99119171268608
    },
    {
      "type": "training",
      "description": "Training step 2355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:32",
      "total_flops_so_far": 1.3997132770738176e+16,
      "budget_used_percent": 13.997132770738178
    },
    {
      "type": "training",
      "description": "Training step 2356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:32",
      "total_flops_so_far": 1.4003073828790272e+16,
      "budget_used_percent": 14.003073828790273
    },
    {
      "type": "training",
      "description": "Training step 2357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:32",
      "total_flops_so_far": 1.4009014886842368e+16,
      "budget_used_percent": 14.009014886842369
    },
    {
      "type": "training",
      "description": "Training step 2358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:33",
      "total_flops_so_far": 1.4014955944894464e+16,
      "budget_used_percent": 14.014955944894464
    },
    {
      "type": "training",
      "description": "Training step 2359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:33",
      "total_flops_so_far": 1.402089700294656e+16,
      "budget_used_percent": 14.020897002946562
    },
    {
      "type": "training",
      "description": "Training step 2360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:34",
      "total_flops_so_far": 1.4026838060998656e+16,
      "budget_used_percent": 14.026838060998656
    },
    {
      "type": "training",
      "description": "Training step 2361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:34",
      "total_flops_so_far": 1.4032779119050752e+16,
      "budget_used_percent": 14.032779119050753
    },
    {
      "type": "training",
      "description": "Training step 2362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:34",
      "total_flops_so_far": 1.4038720177102848e+16,
      "budget_used_percent": 14.038720177102848
    },
    {
      "type": "training",
      "description": "Training step 2363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:35",
      "total_flops_so_far": 1.4044661235154944e+16,
      "budget_used_percent": 14.044661235154942
    },
    {
      "type": "training",
      "description": "Training step 2364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:35",
      "total_flops_so_far": 1.405060229320704e+16,
      "budget_used_percent": 14.05060229320704
    },
    {
      "type": "training",
      "description": "Training step 2365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:35",
      "total_flops_so_far": 1.4056543351259136e+16,
      "budget_used_percent": 14.056543351259135
    },
    {
      "type": "training",
      "description": "Training step 2366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:36",
      "total_flops_so_far": 1.4062484409311232e+16,
      "budget_used_percent": 14.062484409311232
    },
    {
      "type": "training",
      "description": "Training step 2367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:36",
      "total_flops_so_far": 1.4068425467363328e+16,
      "budget_used_percent": 14.068425467363326
    },
    {
      "type": "training",
      "description": "Training step 2368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:36",
      "total_flops_so_far": 1.4074366525415424e+16,
      "budget_used_percent": 14.074366525415424
    },
    {
      "type": "training",
      "description": "Training step 2369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:37",
      "total_flops_so_far": 1.408030758346752e+16,
      "budget_used_percent": 14.080307583467519
    },
    {
      "type": "training",
      "description": "Training step 2370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:37",
      "total_flops_so_far": 1.4086248641519616e+16,
      "budget_used_percent": 14.086248641519617
    },
    {
      "type": "training",
      "description": "Training step 2371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:37",
      "total_flops_so_far": 1.4092189699571712e+16,
      "budget_used_percent": 14.092189699571712
    },
    {
      "type": "training",
      "description": "Training step 2372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:38",
      "total_flops_so_far": 1.4098130757623808e+16,
      "budget_used_percent": 14.098130757623808
    },
    {
      "type": "training",
      "description": "Training step 2373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:38",
      "total_flops_so_far": 1.4104071815675904e+16,
      "budget_used_percent": 14.104071815675903
    },
    {
      "type": "training",
      "description": "Training step 2374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:39",
      "total_flops_so_far": 1.4110012873728e+16,
      "budget_used_percent": 14.110012873728001
    },
    {
      "type": "training",
      "description": "Training step 2375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:39",
      "total_flops_so_far": 1.4115953931780096e+16,
      "budget_used_percent": 14.115953931780096
    },
    {
      "type": "training",
      "description": "Training step 2376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:39",
      "total_flops_so_far": 1.4121894989832192e+16,
      "budget_used_percent": 14.121894989832192
    },
    {
      "type": "training",
      "description": "Training step 2377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:40",
      "total_flops_so_far": 1.4127836047884288e+16,
      "budget_used_percent": 14.127836047884287
    },
    {
      "type": "training",
      "description": "Training step 2378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:40",
      "total_flops_so_far": 1.4133777105936384e+16,
      "budget_used_percent": 14.133777105936385
    },
    {
      "type": "training",
      "description": "Training step 2379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:40",
      "total_flops_so_far": 1.413971816398848e+16,
      "budget_used_percent": 14.13971816398848
    },
    {
      "type": "training",
      "description": "Training step 2380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:41",
      "total_flops_so_far": 1.4145659222040576e+16,
      "budget_used_percent": 14.145659222040576
    },
    {
      "type": "training",
      "description": "Training step 2381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:41",
      "total_flops_so_far": 1.4151600280092672e+16,
      "budget_used_percent": 14.151600280092671
    },
    {
      "type": "training",
      "description": "Training step 2382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:41",
      "total_flops_so_far": 1.4157541338144768e+16,
      "budget_used_percent": 14.15754133814477
    },
    {
      "type": "training",
      "description": "Training step 2383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:42",
      "total_flops_so_far": 1.4163482396196864e+16,
      "budget_used_percent": 14.163482396196864
    },
    {
      "type": "training",
      "description": "Training step 2384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:42",
      "total_flops_so_far": 1.416942345424896e+16,
      "budget_used_percent": 14.16942345424896
    },
    {
      "type": "training",
      "description": "Training step 2385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:43",
      "total_flops_so_far": 1.4175364512301056e+16,
      "budget_used_percent": 14.175364512301055
    },
    {
      "type": "training",
      "description": "Training step 2386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:43",
      "total_flops_so_far": 1.4181305570353152e+16,
      "budget_used_percent": 14.181305570353153
    },
    {
      "type": "training",
      "description": "Training step 2387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:43",
      "total_flops_so_far": 1.4187246628405248e+16,
      "budget_used_percent": 14.187246628405248
    },
    {
      "type": "training",
      "description": "Training step 2388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:44",
      "total_flops_so_far": 1.4193187686457344e+16,
      "budget_used_percent": 14.193187686457346
    },
    {
      "type": "training",
      "description": "Training step 2389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:44",
      "total_flops_so_far": 1.419912874450944e+16,
      "budget_used_percent": 14.19912874450944
    },
    {
      "type": "training",
      "description": "Training step 2390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:44",
      "total_flops_so_far": 1.4205069802561536e+16,
      "budget_used_percent": 14.205069802561535
    },
    {
      "type": "training",
      "description": "Training step 2391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:45",
      "total_flops_so_far": 1.4211010860613632e+16,
      "budget_used_percent": 14.211010860613632
    },
    {
      "type": "training",
      "description": "Training step 2392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:45",
      "total_flops_so_far": 1.4216951918665728e+16,
      "budget_used_percent": 14.216951918665726
    },
    {
      "type": "training",
      "description": "Training step 2393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:45",
      "total_flops_so_far": 1.4222892976717824e+16,
      "budget_used_percent": 14.222892976717825
    },
    {
      "type": "training",
      "description": "Training step 2394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:46",
      "total_flops_so_far": 1.422883403476992e+16,
      "budget_used_percent": 14.22883403476992
    },
    {
      "type": "training",
      "description": "Training step 2395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:46",
      "total_flops_so_far": 1.4234775092822016e+16,
      "budget_used_percent": 14.234775092822016
    },
    {
      "type": "training",
      "description": "Training step 2396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:46",
      "total_flops_so_far": 1.4240716150874112e+16,
      "budget_used_percent": 14.24071615087411
    },
    {
      "type": "training",
      "description": "Training step 2397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:47",
      "total_flops_so_far": 1.4246657208926208e+16,
      "budget_used_percent": 14.246657208926209
    },
    {
      "type": "training",
      "description": "Training step 2398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:47",
      "total_flops_so_far": 1.4252598266978304e+16,
      "budget_used_percent": 14.252598266978303
    },
    {
      "type": "training",
      "description": "Training step 2399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:48",
      "total_flops_so_far": 1.42585393250304e+16,
      "budget_used_percent": 14.2585393250304
    },
    {
      "type": "training",
      "description": "Training step 2400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:48",
      "total_flops_so_far": 1.4264480383082496e+16,
      "budget_used_percent": 14.264480383082494
    },
    {
      "type": "training",
      "description": "Training step 2401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:48",
      "total_flops_so_far": 1.4270421441134592e+16,
      "budget_used_percent": 14.270421441134593
    },
    {
      "type": "training",
      "description": "Training step 2402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:49",
      "total_flops_so_far": 1.4276362499186688e+16,
      "budget_used_percent": 14.276362499186687
    },
    {
      "type": "training",
      "description": "Training step 2403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:49",
      "total_flops_so_far": 1.4282303557238784e+16,
      "budget_used_percent": 14.282303557238784
    },
    {
      "type": "training",
      "description": "Training step 2404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:49",
      "total_flops_so_far": 1.428824461529088e+16,
      "budget_used_percent": 14.288244615290878
    },
    {
      "type": "training",
      "description": "Training step 2405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:50",
      "total_flops_so_far": 1.4294185673342976e+16,
      "budget_used_percent": 14.294185673342977
    },
    {
      "type": "training",
      "description": "Training step 2406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:50",
      "total_flops_so_far": 1.4300126731395072e+16,
      "budget_used_percent": 14.300126731395071
    },
    {
      "type": "training",
      "description": "Training step 2407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:50",
      "total_flops_so_far": 1.4306067789447168e+16,
      "budget_used_percent": 14.30606778944717
    },
    {
      "type": "training",
      "description": "Training step 2408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:51",
      "total_flops_so_far": 1.4312008847499264e+16,
      "budget_used_percent": 14.312008847499264
    },
    {
      "type": "training",
      "description": "Training step 2409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:51",
      "total_flops_so_far": 1.431794990555136e+16,
      "budget_used_percent": 14.31794990555136
    },
    {
      "type": "training",
      "description": "Training step 2410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:52",
      "total_flops_so_far": 1.4323890963603456e+16,
      "budget_used_percent": 14.323890963603455
    },
    {
      "type": "training",
      "description": "Training step 2411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:52",
      "total_flops_so_far": 1.4329832021655552e+16,
      "budget_used_percent": 14.329832021655553
    },
    {
      "type": "training",
      "description": "Training step 2412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:52",
      "total_flops_so_far": 1.4335773079707648e+16,
      "budget_used_percent": 14.335773079707648
    },
    {
      "type": "training",
      "description": "Training step 2413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:53",
      "total_flops_so_far": 1.4341714137759744e+16,
      "budget_used_percent": 14.341714137759745
    },
    {
      "type": "training",
      "description": "Training step 2414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:53",
      "total_flops_so_far": 1.434765519581184e+16,
      "budget_used_percent": 14.34765519581184
    },
    {
      "type": "training",
      "description": "Training step 2415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:53",
      "total_flops_so_far": 1.4353596253863936e+16,
      "budget_used_percent": 14.353596253863937
    },
    {
      "type": "training",
      "description": "Training step 2416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:54",
      "total_flops_so_far": 1.4359537311916032e+16,
      "budget_used_percent": 14.359537311916032
    },
    {
      "type": "training",
      "description": "Training step 2417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:54",
      "total_flops_so_far": 1.4365478369968128e+16,
      "budget_used_percent": 14.365478369968129
    },
    {
      "type": "training",
      "description": "Training step 2418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:54",
      "total_flops_so_far": 1.4371419428020224e+16,
      "budget_used_percent": 14.371419428020223
    },
    {
      "type": "training",
      "description": "Training step 2419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:55",
      "total_flops_so_far": 1.437736048607232e+16,
      "budget_used_percent": 14.377360486072318
    },
    {
      "type": "training",
      "description": "Training step 2420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:55",
      "total_flops_so_far": 1.4383301544124416e+16,
      "budget_used_percent": 14.383301544124416
    },
    {
      "type": "training",
      "description": "Training step 2421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:56",
      "total_flops_so_far": 1.4389242602176512e+16,
      "budget_used_percent": 14.38924260217651
    },
    {
      "type": "training",
      "description": "Training step 2422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:56",
      "total_flops_so_far": 1.4395183660228608e+16,
      "budget_used_percent": 14.395183660228609
    },
    {
      "type": "training",
      "description": "Training step 2423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:56",
      "total_flops_so_far": 1.4401124718280704e+16,
      "budget_used_percent": 14.401124718280704
    },
    {
      "type": "training",
      "description": "Training step 2424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:57",
      "total_flops_so_far": 1.44070657763328e+16,
      "budget_used_percent": 14.4070657763328
    },
    {
      "type": "training",
      "description": "Training step 2425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:57",
      "total_flops_so_far": 1.4413006834384896e+16,
      "budget_used_percent": 14.413006834384895
    },
    {
      "type": "training",
      "description": "Training step 2426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:57",
      "total_flops_so_far": 1.4418947892436992e+16,
      "budget_used_percent": 14.418947892436993
    },
    {
      "type": "training",
      "description": "Training step 2427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:58",
      "total_flops_so_far": 1.4424888950489088e+16,
      "budget_used_percent": 14.424888950489088
    },
    {
      "type": "training",
      "description": "Training step 2428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:58",
      "total_flops_so_far": 1.4430830008541184e+16,
      "budget_used_percent": 14.430830008541184
    },
    {
      "type": "training",
      "description": "Training step 2429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:58",
      "total_flops_so_far": 1.443677106659328e+16,
      "budget_used_percent": 14.436771066593279
    },
    {
      "type": "training",
      "description": "Training step 2430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:59",
      "total_flops_so_far": 1.4442712124645376e+16,
      "budget_used_percent": 14.442712124645377
    },
    {
      "type": "training",
      "description": "Training step 2431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:59",
      "total_flops_so_far": 1.4448653182697472e+16,
      "budget_used_percent": 14.448653182697472
    },
    {
      "type": "training",
      "description": "Training step 2432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:40:59",
      "total_flops_so_far": 1.4454594240749568e+16,
      "budget_used_percent": 14.454594240749568
    },
    {
      "type": "training",
      "description": "Training step 2433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:00",
      "total_flops_so_far": 1.4460535298801664e+16,
      "budget_used_percent": 14.460535298801663
    },
    {
      "type": "training",
      "description": "Training step 2434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:00",
      "total_flops_so_far": 1.446647635685376e+16,
      "budget_used_percent": 14.46647635685376
    },
    {
      "type": "training",
      "description": "Training step 2435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:01",
      "total_flops_so_far": 1.4472417414905856e+16,
      "budget_used_percent": 14.472417414905856
    },
    {
      "type": "training",
      "description": "Training step 2436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:01",
      "total_flops_so_far": 1.4478358472957952e+16,
      "budget_used_percent": 14.478358472957954
    },
    {
      "type": "training",
      "description": "Training step 2437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:01",
      "total_flops_so_far": 1.4484299531010048e+16,
      "budget_used_percent": 14.484299531010048
    },
    {
      "type": "training",
      "description": "Training step 2438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:02",
      "total_flops_so_far": 1.4490240589062144e+16,
      "budget_used_percent": 14.490240589062145
    },
    {
      "type": "training",
      "description": "Training step 2439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:02",
      "total_flops_so_far": 1.449618164711424e+16,
      "budget_used_percent": 14.49618164711424
    },
    {
      "type": "training",
      "description": "Training step 2440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:02",
      "total_flops_so_far": 1.4502122705166336e+16,
      "budget_used_percent": 14.502122705166338
    },
    {
      "type": "training",
      "description": "Training step 2441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:03",
      "total_flops_so_far": 1.4508063763218432e+16,
      "budget_used_percent": 14.508063763218432
    },
    {
      "type": "training",
      "description": "Training step 2442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:03",
      "total_flops_so_far": 1.4514004821270528e+16,
      "budget_used_percent": 14.514004821270529
    },
    {
      "type": "training",
      "description": "Training step 2443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:03",
      "total_flops_so_far": 1.4519945879322624e+16,
      "budget_used_percent": 14.519945879322623
    },
    {
      "type": "training",
      "description": "Training step 2444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:04",
      "total_flops_so_far": 1.452588693737472e+16,
      "budget_used_percent": 14.525886937374722
    },
    {
      "type": "training",
      "description": "Training step 2445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:04",
      "total_flops_so_far": 1.4531827995426816e+16,
      "budget_used_percent": 14.531827995426816
    },
    {
      "type": "training",
      "description": "Training step 2446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:05",
      "total_flops_so_far": 1.4537769053478912e+16,
      "budget_used_percent": 14.537769053478913
    },
    {
      "type": "training",
      "description": "Training step 2447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:05",
      "total_flops_so_far": 1.4543710111531008e+16,
      "budget_used_percent": 14.543710111531007
    },
    {
      "type": "training",
      "description": "Training step 2448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:05",
      "total_flops_so_far": 1.4549651169583104e+16,
      "budget_used_percent": 14.549651169583102
    },
    {
      "type": "training",
      "description": "Training step 2449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:06",
      "total_flops_so_far": 1.45555922276352e+16,
      "budget_used_percent": 14.5555922276352
    },
    {
      "type": "training",
      "description": "Training step 2450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:06",
      "total_flops_so_far": 1.4561533285687296e+16,
      "budget_used_percent": 14.561533285687295
    },
    {
      "type": "training",
      "description": "Training step 2451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:06",
      "total_flops_so_far": 1.4567474343739392e+16,
      "budget_used_percent": 14.567474343739391
    },
    {
      "type": "training",
      "description": "Training step 2452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:07",
      "total_flops_so_far": 1.4573415401791488e+16,
      "budget_used_percent": 14.573415401791486
    },
    {
      "type": "training",
      "description": "Training step 2453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:07",
      "total_flops_so_far": 1.4579356459843584e+16,
      "budget_used_percent": 14.579356459843584
    },
    {
      "type": "training",
      "description": "Training step 2454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:07",
      "total_flops_so_far": 1.458529751789568e+16,
      "budget_used_percent": 14.585297517895679
    },
    {
      "type": "training",
      "description": "Training step 2455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:08",
      "total_flops_so_far": 1.4591238575947776e+16,
      "budget_used_percent": 14.591238575947777
    },
    {
      "type": "training",
      "description": "Training step 2456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:08",
      "total_flops_so_far": 1.4597179633999872e+16,
      "budget_used_percent": 14.597179633999872
    },
    {
      "type": "training",
      "description": "Training step 2457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:08",
      "total_flops_so_far": 1.4603120692051968e+16,
      "budget_used_percent": 14.603120692051968
    },
    {
      "type": "training",
      "description": "Training step 2458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:09",
      "total_flops_so_far": 1.4609061750104064e+16,
      "budget_used_percent": 14.609061750104063
    },
    {
      "type": "training",
      "description": "Training step 2459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:09",
      "total_flops_so_far": 1.461500280815616e+16,
      "budget_used_percent": 14.615002808156161
    },
    {
      "type": "training",
      "description": "Training step 2460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:09",
      "total_flops_so_far": 1.4620943866208256e+16,
      "budget_used_percent": 14.620943866208256
    },
    {
      "type": "training",
      "description": "Training step 2461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:10",
      "total_flops_so_far": 1.4626884924260352e+16,
      "budget_used_percent": 14.626884924260352
    },
    {
      "type": "training",
      "description": "Training step 2462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:10",
      "total_flops_so_far": 1.4632825982312448e+16,
      "budget_used_percent": 14.632825982312447
    },
    {
      "type": "training",
      "description": "Training step 2463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:11",
      "total_flops_so_far": 1.4638767040364544e+16,
      "budget_used_percent": 14.638767040364545
    },
    {
      "type": "training",
      "description": "Training step 2464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:11",
      "total_flops_so_far": 1.464470809841664e+16,
      "budget_used_percent": 14.64470809841664
    },
    {
      "type": "training",
      "description": "Training step 2465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:11",
      "total_flops_so_far": 1.4650649156468736e+16,
      "budget_used_percent": 14.650649156468736
    },
    {
      "type": "training",
      "description": "Training step 2466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:12",
      "total_flops_so_far": 1.4656590214520832e+16,
      "budget_used_percent": 14.65659021452083
    },
    {
      "type": "training",
      "description": "Training step 2467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:12",
      "total_flops_so_far": 1.4662531272572928e+16,
      "budget_used_percent": 14.662531272572929
    },
    {
      "type": "training",
      "description": "Training step 2468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:12",
      "total_flops_so_far": 1.4668472330625024e+16,
      "budget_used_percent": 14.668472330625024
    },
    {
      "type": "training",
      "description": "Training step 2469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:13",
      "total_flops_so_far": 1.467441338867712e+16,
      "budget_used_percent": 14.67441338867712
    },
    {
      "type": "training",
      "description": "Training step 2470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:13",
      "total_flops_so_far": 1.4680354446729216e+16,
      "budget_used_percent": 14.680354446729215
    },
    {
      "type": "training",
      "description": "Training step 2471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:13",
      "total_flops_so_far": 1.4686295504781312e+16,
      "budget_used_percent": 14.686295504781313
    },
    {
      "type": "training",
      "description": "Training step 2472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:14",
      "total_flops_so_far": 1.4692236562833408e+16,
      "budget_used_percent": 14.692236562833408
    },
    {
      "type": "training",
      "description": "Training step 2473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:14",
      "total_flops_so_far": 1.4698177620885504e+16,
      "budget_used_percent": 14.698177620885506
    },
    {
      "type": "training",
      "description": "Training step 2474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:15",
      "total_flops_so_far": 1.47041186789376e+16,
      "budget_used_percent": 14.7041186789376
    },
    {
      "type": "training",
      "description": "Training step 2475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:15",
      "total_flops_so_far": 1.4710059736989696e+16,
      "budget_used_percent": 14.710059736989697
    },
    {
      "type": "training",
      "description": "Training step 2476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:15",
      "total_flops_so_far": 1.4716000795041792e+16,
      "budget_used_percent": 14.716000795041792
    },
    {
      "type": "training",
      "description": "Training step 2477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:16",
      "total_flops_so_far": 1.4721941853093888e+16,
      "budget_used_percent": 14.721941853093886
    },
    {
      "type": "training",
      "description": "Training step 2478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:16",
      "total_flops_so_far": 1.4727882911145984e+16,
      "budget_used_percent": 14.727882911145985
    },
    {
      "type": "training",
      "description": "Training step 2479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:16",
      "total_flops_so_far": 1.473382396919808e+16,
      "budget_used_percent": 14.73382396919808
    },
    {
      "type": "training",
      "description": "Training step 2480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:17",
      "total_flops_so_far": 1.4739765027250176e+16,
      "budget_used_percent": 14.739765027250176
    },
    {
      "type": "training",
      "description": "Training step 2481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:17",
      "total_flops_so_far": 1.4745706085302272e+16,
      "budget_used_percent": 14.74570608530227
    },
    {
      "type": "training",
      "description": "Training step 2482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:17",
      "total_flops_so_far": 1.4751647143354368e+16,
      "budget_used_percent": 14.751647143354369
    },
    {
      "type": "training",
      "description": "Training step 2483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:18",
      "total_flops_so_far": 1.4757588201406464e+16,
      "budget_used_percent": 14.757588201406463
    },
    {
      "type": "training",
      "description": "Training step 2484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:18",
      "total_flops_so_far": 1.476352925945856e+16,
      "budget_used_percent": 14.76352925945856
    },
    {
      "type": "training",
      "description": "Training step 2485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:18",
      "total_flops_so_far": 1.4769470317510656e+16,
      "budget_used_percent": 14.769470317510654
    },
    {
      "type": "training",
      "description": "Training step 2486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:19",
      "total_flops_so_far": 1.4775411375562752e+16,
      "budget_used_percent": 14.775411375562753
    },
    {
      "type": "training",
      "description": "Training step 2487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:19",
      "total_flops_so_far": 1.4781352433614848e+16,
      "budget_used_percent": 14.781352433614847
    },
    {
      "type": "training",
      "description": "Training step 2488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:20",
      "total_flops_so_far": 1.4787293491666944e+16,
      "budget_used_percent": 14.787293491666945
    },
    {
      "type": "training",
      "description": "Training step 2489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:20",
      "total_flops_so_far": 1.479323454971904e+16,
      "budget_used_percent": 14.79323454971904
    },
    {
      "type": "training",
      "description": "Training step 2490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:20",
      "total_flops_so_far": 1.4799175607771136e+16,
      "budget_used_percent": 14.799175607771136
    },
    {
      "type": "training",
      "description": "Training step 2491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:21",
      "total_flops_so_far": 1.4805116665823232e+16,
      "budget_used_percent": 14.805116665823231
    },
    {
      "type": "training",
      "description": "Training step 2492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:21",
      "total_flops_so_far": 1.4811057723875328e+16,
      "budget_used_percent": 14.81105772387533
    },
    {
      "type": "training",
      "description": "Training step 2493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:22",
      "total_flops_so_far": 1.4816998781927424e+16,
      "budget_used_percent": 14.816998781927424
    },
    {
      "type": "training",
      "description": "Training step 2494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:22",
      "total_flops_so_far": 1.482293983997952e+16,
      "budget_used_percent": 14.82293983997952
    },
    {
      "type": "training",
      "description": "Training step 2495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:22",
      "total_flops_so_far": 1.4828880898031616e+16,
      "budget_used_percent": 14.828880898031615
    },
    {
      "type": "training",
      "description": "Training step 2496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:23",
      "total_flops_so_far": 1.4834821956083712e+16,
      "budget_used_percent": 14.834821956083713
    },
    {
      "type": "training",
      "description": "Training step 2497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:23",
      "total_flops_so_far": 1.4840763014135808e+16,
      "budget_used_percent": 14.840763014135808
    },
    {
      "type": "training",
      "description": "Training step 2498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:23",
      "total_flops_so_far": 1.4846704072187904e+16,
      "budget_used_percent": 14.846704072187904
    },
    {
      "type": "training",
      "description": "Training step 2499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:41:24",
      "total_flops_so_far": 1.485264513024e+16,
      "budget_used_percent": 14.852645130239999
    },
    {
      "type": "training",
      "description": "Training step 2500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:47",
      "total_flops_so_far": 1.4858586188292096e+16,
      "budget_used_percent": 14.858586188292097
    },
    {
      "type": "training",
      "description": "Training step 2501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:48",
      "total_flops_so_far": 1.4864527246344192e+16,
      "budget_used_percent": 14.864527246344192
    },
    {
      "type": "training",
      "description": "Training step 2502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:48",
      "total_flops_so_far": 1.4870468304396288e+16,
      "budget_used_percent": 14.87046830439629
    },
    {
      "type": "training",
      "description": "Training step 2503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:49",
      "total_flops_so_far": 1.4876409362448384e+16,
      "budget_used_percent": 14.876409362448385
    },
    {
      "type": "training",
      "description": "Training step 2504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:49",
      "total_flops_so_far": 1.488235042050048e+16,
      "budget_used_percent": 14.88235042050048
    },
    {
      "type": "training",
      "description": "Training step 2505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:49",
      "total_flops_so_far": 1.4888291478552576e+16,
      "budget_used_percent": 14.888291478552576
    },
    {
      "type": "training",
      "description": "Training step 2506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:50",
      "total_flops_so_far": 1.4894232536604672e+16,
      "budget_used_percent": 14.89423253660467
    },
    {
      "type": "training",
      "description": "Training step 2507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:50",
      "total_flops_so_far": 1.4900173594656768e+16,
      "budget_used_percent": 14.900173594656769
    },
    {
      "type": "training",
      "description": "Training step 2508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:50",
      "total_flops_so_far": 1.4906114652708864e+16,
      "budget_used_percent": 14.906114652708863
    },
    {
      "type": "training",
      "description": "Training step 2509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:51",
      "total_flops_so_far": 1.491205571076096e+16,
      "budget_used_percent": 14.91205571076096
    },
    {
      "type": "training",
      "description": "Training step 2510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:51",
      "total_flops_so_far": 1.4917996768813056e+16,
      "budget_used_percent": 14.917996768813055
    },
    {
      "type": "training",
      "description": "Training step 2511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:52",
      "total_flops_so_far": 1.4923937826865152e+16,
      "budget_used_percent": 14.923937826865153
    },
    {
      "type": "training",
      "description": "Training step 2512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:52",
      "total_flops_so_far": 1.4929878884917248e+16,
      "budget_used_percent": 14.929878884917247
    },
    {
      "type": "training",
      "description": "Training step 2513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:52",
      "total_flops_so_far": 1.4935819942969344e+16,
      "budget_used_percent": 14.935819942969344
    },
    {
      "type": "training",
      "description": "Training step 2514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:53",
      "total_flops_so_far": 1.494176100102144e+16,
      "budget_used_percent": 14.941761001021439
    },
    {
      "type": "training",
      "description": "Training step 2515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:53",
      "total_flops_so_far": 1.4947702059073536e+16,
      "budget_used_percent": 14.947702059073537
    },
    {
      "type": "training",
      "description": "Training step 2516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:53",
      "total_flops_so_far": 1.4953643117125632e+16,
      "budget_used_percent": 14.953643117125631
    },
    {
      "type": "training",
      "description": "Training step 2517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:54",
      "total_flops_so_far": 1.4959584175177728e+16,
      "budget_used_percent": 14.959584175177728
    },
    {
      "type": "training",
      "description": "Training step 2518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:54",
      "total_flops_so_far": 1.4965525233229824e+16,
      "budget_used_percent": 14.965525233229823
    },
    {
      "type": "training",
      "description": "Training step 2519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:54",
      "total_flops_so_far": 1.497146629128192e+16,
      "budget_used_percent": 14.97146629128192
    },
    {
      "type": "training",
      "description": "Training step 2520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:55",
      "total_flops_so_far": 1.4977407349334016e+16,
      "budget_used_percent": 14.977407349334015
    },
    {
      "type": "training",
      "description": "Training step 2521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:55",
      "total_flops_so_far": 1.4983348407386112e+16,
      "budget_used_percent": 14.983348407386114
    },
    {
      "type": "training",
      "description": "Training step 2522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:56",
      "total_flops_so_far": 1.4989289465438208e+16,
      "budget_used_percent": 14.989289465438208
    },
    {
      "type": "training",
      "description": "Training step 2523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:56",
      "total_flops_so_far": 1.4995230523490304e+16,
      "budget_used_percent": 14.995230523490305
    },
    {
      "type": "training",
      "description": "Training step 2524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:56",
      "total_flops_so_far": 1.50011715815424e+16,
      "budget_used_percent": 15.0011715815424
    },
    {
      "type": "training",
      "description": "Training step 2525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:57",
      "total_flops_so_far": 1.5007112639594496e+16,
      "budget_used_percent": 15.007112639594498
    },
    {
      "type": "training",
      "description": "Training step 2526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:57",
      "total_flops_so_far": 1.5013053697646592e+16,
      "budget_used_percent": 15.013053697646592
    },
    {
      "type": "training",
      "description": "Training step 2527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:57",
      "total_flops_so_far": 1.5018994755698688e+16,
      "budget_used_percent": 15.018994755698689
    },
    {
      "type": "training",
      "description": "Training step 2528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:58",
      "total_flops_so_far": 1.5024935813750784e+16,
      "budget_used_percent": 15.024935813750783
    },
    {
      "type": "training",
      "description": "Training step 2529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:58",
      "total_flops_so_far": 1.503087687180288e+16,
      "budget_used_percent": 15.030876871802882
    },
    {
      "type": "training",
      "description": "Training step 2530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:58",
      "total_flops_so_far": 1.5036817929854976e+16,
      "budget_used_percent": 15.036817929854976
    },
    {
      "type": "training",
      "description": "Training step 2531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:59",
      "total_flops_so_far": 1.5042758987907072e+16,
      "budget_used_percent": 15.042758987907073
    },
    {
      "type": "training",
      "description": "Training step 2532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:43:59",
      "total_flops_so_far": 1.5048700045959168e+16,
      "budget_used_percent": 15.048700045959167
    },
    {
      "type": "training",
      "description": "Training step 2533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:00",
      "total_flops_so_far": 1.5054641104011264e+16,
      "budget_used_percent": 15.054641104011262
    },
    {
      "type": "training",
      "description": "Training step 2534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:00",
      "total_flops_so_far": 1.506058216206336e+16,
      "budget_used_percent": 15.06058216206336
    },
    {
      "type": "training",
      "description": "Training step 2535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:00",
      "total_flops_so_far": 1.5066523220115456e+16,
      "budget_used_percent": 15.066523220115455
    },
    {
      "type": "training",
      "description": "Training step 2536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:01",
      "total_flops_so_far": 1.5072464278167552e+16,
      "budget_used_percent": 15.072464278167551
    },
    {
      "type": "training",
      "description": "Training step 2537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:01",
      "total_flops_so_far": 1.5078405336219648e+16,
      "budget_used_percent": 15.078405336219646
    },
    {
      "type": "training",
      "description": "Training step 2538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:01",
      "total_flops_so_far": 1.5084346394271744e+16,
      "budget_used_percent": 15.084346394271744
    },
    {
      "type": "training",
      "description": "Training step 2539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:02",
      "total_flops_so_far": 1.509028745232384e+16,
      "budget_used_percent": 15.090287452323839
    },
    {
      "type": "training",
      "description": "Training step 2540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:02",
      "total_flops_so_far": 1.5096228510375936e+16,
      "budget_used_percent": 15.096228510375937
    },
    {
      "type": "training",
      "description": "Training step 2541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:02",
      "total_flops_so_far": 1.5102169568428032e+16,
      "budget_used_percent": 15.102169568428032
    },
    {
      "type": "training",
      "description": "Training step 2542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:03",
      "total_flops_so_far": 1.5108110626480128e+16,
      "budget_used_percent": 15.108110626480128
    },
    {
      "type": "training",
      "description": "Training step 2543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:03",
      "total_flops_so_far": 1.5114051684532224e+16,
      "budget_used_percent": 15.114051684532223
    },
    {
      "type": "training",
      "description": "Training step 2544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:04",
      "total_flops_so_far": 1.511999274258432e+16,
      "budget_used_percent": 15.119992742584321
    },
    {
      "type": "training",
      "description": "Training step 2545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:04",
      "total_flops_so_far": 1.5125933800636416e+16,
      "budget_used_percent": 15.125933800636416
    },
    {
      "type": "training",
      "description": "Training step 2546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:04",
      "total_flops_so_far": 1.5131874858688512e+16,
      "budget_used_percent": 15.131874858688512
    },
    {
      "type": "training",
      "description": "Training step 2547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:05",
      "total_flops_so_far": 1.5137815916740608e+16,
      "budget_used_percent": 15.137815916740607
    },
    {
      "type": "training",
      "description": "Training step 2548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:05",
      "total_flops_so_far": 1.5143756974792704e+16,
      "budget_used_percent": 15.143756974792705
    },
    {
      "type": "training",
      "description": "Training step 2549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:05",
      "total_flops_so_far": 1.51496980328448e+16,
      "budget_used_percent": 15.1496980328448
    },
    {
      "type": "training",
      "description": "Training step 2550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:06",
      "total_flops_so_far": 1.5155639090896896e+16,
      "budget_used_percent": 15.155639090896896
    },
    {
      "type": "training",
      "description": "Training step 2551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:06",
      "total_flops_so_far": 1.5161580148948992e+16,
      "budget_used_percent": 15.16158014894899
    },
    {
      "type": "training",
      "description": "Training step 2552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:06",
      "total_flops_so_far": 1.5167521207001088e+16,
      "budget_used_percent": 15.167521207001089
    },
    {
      "type": "training",
      "description": "Training step 2553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:07",
      "total_flops_so_far": 1.5173462265053184e+16,
      "budget_used_percent": 15.173462265053184
    },
    {
      "type": "training",
      "description": "Training step 2554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:07",
      "total_flops_so_far": 1.517940332310528e+16,
      "budget_used_percent": 15.179403323105282
    },
    {
      "type": "training",
      "description": "Training step 2555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:08",
      "total_flops_so_far": 1.5185344381157376e+16,
      "budget_used_percent": 15.185344381157377
    },
    {
      "type": "training",
      "description": "Training step 2556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:08",
      "total_flops_so_far": 1.5191285439209472e+16,
      "budget_used_percent": 15.191285439209473
    },
    {
      "type": "training",
      "description": "Training step 2557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:08",
      "total_flops_so_far": 1.5197226497261568e+16,
      "budget_used_percent": 15.197226497261568
    },
    {
      "type": "training",
      "description": "Training step 2558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:09",
      "total_flops_so_far": 1.5203167555313664e+16,
      "budget_used_percent": 15.203167555313666
    },
    {
      "type": "training",
      "description": "Training step 2559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:09",
      "total_flops_so_far": 1.520910861336576e+16,
      "budget_used_percent": 15.20910861336576
    },
    {
      "type": "training",
      "description": "Training step 2560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:09",
      "total_flops_so_far": 1.5215049671417856e+16,
      "budget_used_percent": 15.215049671417857
    },
    {
      "type": "training",
      "description": "Training step 2561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:10",
      "total_flops_so_far": 1.5220990729469952e+16,
      "budget_used_percent": 15.220990729469952
    },
    {
      "type": "training",
      "description": "Training step 2562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:10",
      "total_flops_so_far": 1.5226931787522048e+16,
      "budget_used_percent": 15.226931787522046
    },
    {
      "type": "training",
      "description": "Training step 2563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:10",
      "total_flops_so_far": 1.5232872845574144e+16,
      "budget_used_percent": 15.232872845574144
    },
    {
      "type": "training",
      "description": "Training step 2564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:11",
      "total_flops_so_far": 1.523881390362624e+16,
      "budget_used_percent": 15.238813903626239
    },
    {
      "type": "training",
      "description": "Training step 2565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:11",
      "total_flops_so_far": 1.5244754961678336e+16,
      "budget_used_percent": 15.244754961678336
    },
    {
      "type": "training",
      "description": "Training step 2566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:12",
      "total_flops_so_far": 1.5250696019730432e+16,
      "budget_used_percent": 15.25069601973043
    },
    {
      "type": "training",
      "description": "Training step 2567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:12",
      "total_flops_so_far": 1.5256637077782528e+16,
      "budget_used_percent": 15.256637077782528
    },
    {
      "type": "training",
      "description": "Training step 2568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:12",
      "total_flops_so_far": 1.5262578135834624e+16,
      "budget_used_percent": 15.262578135834623
    },
    {
      "type": "training",
      "description": "Training step 2569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:13",
      "total_flops_so_far": 1.526851919388672e+16,
      "budget_used_percent": 15.268519193886721
    },
    {
      "type": "training",
      "description": "Training step 2570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:13",
      "total_flops_so_far": 1.5274460251938816e+16,
      "budget_used_percent": 15.274460251938816
    },
    {
      "type": "training",
      "description": "Training step 2571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:13",
      "total_flops_so_far": 1.5280401309990912e+16,
      "budget_used_percent": 15.280401309990912
    },
    {
      "type": "training",
      "description": "Training step 2572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:14",
      "total_flops_so_far": 1.5286342368043008e+16,
      "budget_used_percent": 15.286342368043007
    },
    {
      "type": "training",
      "description": "Training step 2573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:14",
      "total_flops_so_far": 1.5292283426095104e+16,
      "budget_used_percent": 15.292283426095105
    },
    {
      "type": "training",
      "description": "Training step 2574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:14",
      "total_flops_so_far": 1.52982244841472e+16,
      "budget_used_percent": 15.2982244841472
    },
    {
      "type": "training",
      "description": "Training step 2575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:15",
      "total_flops_so_far": 1.5304165542199296e+16,
      "budget_used_percent": 15.304165542199296
    },
    {
      "type": "training",
      "description": "Training step 2576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:15",
      "total_flops_so_far": 1.5310106600251392e+16,
      "budget_used_percent": 15.310106600251391
    },
    {
      "type": "training",
      "description": "Training step 2577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:16",
      "total_flops_so_far": 1.5316047658303488e+16,
      "budget_used_percent": 15.31604765830349
    },
    {
      "type": "training",
      "description": "Training step 2578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:16",
      "total_flops_so_far": 1.5321988716355584e+16,
      "budget_used_percent": 15.321988716355584
    },
    {
      "type": "training",
      "description": "Training step 2579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:16",
      "total_flops_so_far": 1.532792977440768e+16,
      "budget_used_percent": 15.32792977440768
    },
    {
      "type": "training",
      "description": "Training step 2580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:17",
      "total_flops_so_far": 1.5333870832459776e+16,
      "budget_used_percent": 15.333870832459775
    },
    {
      "type": "training",
      "description": "Training step 2581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:17",
      "total_flops_so_far": 1.5339811890511872e+16,
      "budget_used_percent": 15.339811890511873
    },
    {
      "type": "training",
      "description": "Training step 2582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:17",
      "total_flops_so_far": 1.5345752948563968e+16,
      "budget_used_percent": 15.345752948563968
    },
    {
      "type": "training",
      "description": "Training step 2583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:18",
      "total_flops_so_far": 1.5351694006616064e+16,
      "budget_used_percent": 15.351694006616064
    },
    {
      "type": "training",
      "description": "Training step 2584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:18",
      "total_flops_so_far": 1.535763506466816e+16,
      "budget_used_percent": 15.357635064668159
    },
    {
      "type": "training",
      "description": "Training step 2585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:18",
      "total_flops_so_far": 1.5363576122720256e+16,
      "budget_used_percent": 15.363576122720257
    },
    {
      "type": "training",
      "description": "Training step 2586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:19",
      "total_flops_so_far": 1.5369517180772352e+16,
      "budget_used_percent": 15.369517180772352
    },
    {
      "type": "training",
      "description": "Training step 2587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:19",
      "total_flops_so_far": 1.5375458238824448e+16,
      "budget_used_percent": 15.37545823882445
    },
    {
      "type": "training",
      "description": "Training step 2588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:20",
      "total_flops_so_far": 1.5381399296876544e+16,
      "budget_used_percent": 15.381399296876545
    },
    {
      "type": "training",
      "description": "Training step 2589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:20",
      "total_flops_so_far": 1.538734035492864e+16,
      "budget_used_percent": 15.387340354928641
    },
    {
      "type": "training",
      "description": "Training step 2590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:20",
      "total_flops_so_far": 1.5393281412980736e+16,
      "budget_used_percent": 15.393281412980736
    },
    {
      "type": "training",
      "description": "Training step 2591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:21",
      "total_flops_so_far": 1.5399222471032832e+16,
      "budget_used_percent": 15.39922247103283
    },
    {
      "type": "training",
      "description": "Training step 2592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:21",
      "total_flops_so_far": 1.5405163529084928e+16,
      "budget_used_percent": 15.405163529084929
    },
    {
      "type": "training",
      "description": "Training step 2593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:21",
      "total_flops_so_far": 1.5411104587137024e+16,
      "budget_used_percent": 15.411104587137023
    },
    {
      "type": "training",
      "description": "Training step 2594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:22",
      "total_flops_so_far": 1.541704564518912e+16,
      "budget_used_percent": 15.41704564518912
    },
    {
      "type": "training",
      "description": "Training step 2595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:22",
      "total_flops_so_far": 1.5422986703241216e+16,
      "budget_used_percent": 15.422986703241214
    },
    {
      "type": "training",
      "description": "Training step 2596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:22",
      "total_flops_so_far": 1.5428927761293312e+16,
      "budget_used_percent": 15.428927761293313
    },
    {
      "type": "training",
      "description": "Training step 2597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:23",
      "total_flops_so_far": 1.5434868819345408e+16,
      "budget_used_percent": 15.434868819345407
    },
    {
      "type": "training",
      "description": "Training step 2598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:23",
      "total_flops_so_far": 1.5440809877397504e+16,
      "budget_used_percent": 15.440809877397504
    },
    {
      "type": "training",
      "description": "Training step 2599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:24",
      "total_flops_so_far": 1.54467509354496e+16,
      "budget_used_percent": 15.446750935449598
    },
    {
      "type": "training",
      "description": "Training step 2600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:24",
      "total_flops_so_far": 1.5452691993501696e+16,
      "budget_used_percent": 15.452691993501697
    },
    {
      "type": "training",
      "description": "Training step 2601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:24",
      "total_flops_so_far": 1.5458633051553792e+16,
      "budget_used_percent": 15.458633051553791
    },
    {
      "type": "training",
      "description": "Training step 2602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:25",
      "total_flops_so_far": 1.5464574109605888e+16,
      "budget_used_percent": 15.464574109605888
    },
    {
      "type": "training",
      "description": "Training step 2603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:25",
      "total_flops_so_far": 1.5470515167657984e+16,
      "budget_used_percent": 15.470515167657982
    },
    {
      "type": "training",
      "description": "Training step 2604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:26",
      "total_flops_so_far": 1.547645622571008e+16,
      "budget_used_percent": 15.47645622571008
    },
    {
      "type": "training",
      "description": "Training step 2605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:26",
      "total_flops_so_far": 1.5482397283762176e+16,
      "budget_used_percent": 15.482397283762175
    },
    {
      "type": "training",
      "description": "Training step 2606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:26",
      "total_flops_so_far": 1.5488338341814272e+16,
      "budget_used_percent": 15.488338341814273
    },
    {
      "type": "training",
      "description": "Training step 2607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:27",
      "total_flops_so_far": 1.5494279399866368e+16,
      "budget_used_percent": 15.494279399866368
    },
    {
      "type": "training",
      "description": "Training step 2608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:27",
      "total_flops_so_far": 1.5500220457918464e+16,
      "budget_used_percent": 15.500220457918465
    },
    {
      "type": "training",
      "description": "Training step 2609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:28",
      "total_flops_so_far": 1.550616151597056e+16,
      "budget_used_percent": 15.50616151597056
    },
    {
      "type": "training",
      "description": "Training step 2610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:28",
      "total_flops_so_far": 1.5512102574022656e+16,
      "budget_used_percent": 15.512102574022657
    },
    {
      "type": "training",
      "description": "Training step 2611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:28",
      "total_flops_so_far": 1.5518043632074752e+16,
      "budget_used_percent": 15.518043632074752
    },
    {
      "type": "training",
      "description": "Training step 2612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:29",
      "total_flops_so_far": 1.5523984690126848e+16,
      "budget_used_percent": 15.523984690126849
    },
    {
      "type": "training",
      "description": "Training step 2613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:29",
      "total_flops_so_far": 1.5529925748178944e+16,
      "budget_used_percent": 15.529925748178943
    },
    {
      "type": "training",
      "description": "Training step 2614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:29",
      "total_flops_so_far": 1.553586680623104e+16,
      "budget_used_percent": 15.535866806231041
    },
    {
      "type": "training",
      "description": "Training step 2615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:30",
      "total_flops_so_far": 1.5541807864283136e+16,
      "budget_used_percent": 15.541807864283136
    },
    {
      "type": "training",
      "description": "Training step 2616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:30",
      "total_flops_so_far": 1.5547748922335232e+16,
      "budget_used_percent": 15.547748922335233
    },
    {
      "type": "training",
      "description": "Training step 2617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:31",
      "total_flops_so_far": 1.5553689980387328e+16,
      "budget_used_percent": 15.553689980387327
    },
    {
      "type": "training",
      "description": "Training step 2618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:31",
      "total_flops_so_far": 1.5559631038439424e+16,
      "budget_used_percent": 15.559631038439422
    },
    {
      "type": "training",
      "description": "Training step 2619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:31",
      "total_flops_so_far": 1.556557209649152e+16,
      "budget_used_percent": 15.56557209649152
    },
    {
      "type": "training",
      "description": "Training step 2620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:32",
      "total_flops_so_far": 1.5571513154543616e+16,
      "budget_used_percent": 15.571513154543615
    },
    {
      "type": "training",
      "description": "Training step 2621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:32",
      "total_flops_so_far": 1.5577454212595712e+16,
      "budget_used_percent": 15.577454212595713
    },
    {
      "type": "training",
      "description": "Training step 2622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:32",
      "total_flops_so_far": 1.5583395270647808e+16,
      "budget_used_percent": 15.583395270647808
    },
    {
      "type": "training",
      "description": "Training step 2623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:33",
      "total_flops_so_far": 1.5589336328699904e+16,
      "budget_used_percent": 15.589336328699904
    },
    {
      "type": "training",
      "description": "Training step 2624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:33",
      "total_flops_so_far": 1.5595277386752e+16,
      "budget_used_percent": 15.595277386751999
    },
    {
      "type": "training",
      "description": "Training step 2625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:33",
      "total_flops_so_far": 1.5601218444804096e+16,
      "budget_used_percent": 15.601218444804097
    },
    {
      "type": "training",
      "description": "Training step 2626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:34",
      "total_flops_so_far": 1.5607159502856192e+16,
      "budget_used_percent": 15.607159502856192
    },
    {
      "type": "training",
      "description": "Training step 2627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:34",
      "total_flops_so_far": 1.5613100560908288e+16,
      "budget_used_percent": 15.613100560908288
    },
    {
      "type": "training",
      "description": "Training step 2628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:35",
      "total_flops_so_far": 1.5619041618960384e+16,
      "budget_used_percent": 15.619041618960383
    },
    {
      "type": "training",
      "description": "Training step 2629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:35",
      "total_flops_so_far": 1.562498267701248e+16,
      "budget_used_percent": 15.624982677012481
    },
    {
      "type": "training",
      "description": "Training step 2630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:35",
      "total_flops_so_far": 1.5630923735064576e+16,
      "budget_used_percent": 15.630923735064576
    },
    {
      "type": "training",
      "description": "Training step 2631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:36",
      "total_flops_so_far": 1.5636864793116672e+16,
      "budget_used_percent": 15.636864793116672
    },
    {
      "type": "training",
      "description": "Training step 2632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:36",
      "total_flops_so_far": 1.5642805851168768e+16,
      "budget_used_percent": 15.642805851168767
    },
    {
      "type": "training",
      "description": "Training step 2633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:36",
      "total_flops_so_far": 1.5648746909220864e+16,
      "budget_used_percent": 15.648746909220865
    },
    {
      "type": "training",
      "description": "Training step 2634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:37",
      "total_flops_so_far": 1.565468796727296e+16,
      "budget_used_percent": 15.65468796727296
    },
    {
      "type": "training",
      "description": "Training step 2635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:37",
      "total_flops_so_far": 1.5660629025325056e+16,
      "budget_used_percent": 15.660629025325058
    },
    {
      "type": "training",
      "description": "Training step 2636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:37",
      "total_flops_so_far": 1.5666570083377152e+16,
      "budget_used_percent": 15.666570083377152
    },
    {
      "type": "training",
      "description": "Training step 2637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:38",
      "total_flops_so_far": 1.5672511141429248e+16,
      "budget_used_percent": 15.672511141429249
    },
    {
      "type": "training",
      "description": "Training step 2638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:38",
      "total_flops_so_far": 1.5678452199481344e+16,
      "budget_used_percent": 15.678452199481343
    },
    {
      "type": "training",
      "description": "Training step 2639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:39",
      "total_flops_so_far": 1.568439325753344e+16,
      "budget_used_percent": 15.684393257533442
    },
    {
      "type": "training",
      "description": "Training step 2640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:39",
      "total_flops_so_far": 1.5690334315585536e+16,
      "budget_used_percent": 15.690334315585536
    },
    {
      "type": "training",
      "description": "Training step 2641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:39",
      "total_flops_so_far": 1.5696275373637632e+16,
      "budget_used_percent": 15.696275373637633
    },
    {
      "type": "training",
      "description": "Training step 2642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:40",
      "total_flops_so_far": 1.5702216431689728e+16,
      "budget_used_percent": 15.702216431689727
    },
    {
      "type": "training",
      "description": "Training step 2643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:40",
      "total_flops_so_far": 1.5708157489741824e+16,
      "budget_used_percent": 15.708157489741826
    },
    {
      "type": "training",
      "description": "Training step 2644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:40",
      "total_flops_so_far": 1.571409854779392e+16,
      "budget_used_percent": 15.71409854779392
    },
    {
      "type": "training",
      "description": "Training step 2645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:41",
      "total_flops_so_far": 1.5720039605846016e+16,
      "budget_used_percent": 15.720039605846017
    },
    {
      "type": "training",
      "description": "Training step 2646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:41",
      "total_flops_so_far": 1.5725980663898112e+16,
      "budget_used_percent": 15.725980663898111
    },
    {
      "type": "training",
      "description": "Training step 2647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:41",
      "total_flops_so_far": 1.5731921721950208e+16,
      "budget_used_percent": 15.731921721950206
    },
    {
      "type": "training",
      "description": "Training step 2648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:42",
      "total_flops_so_far": 1.5737862780002304e+16,
      "budget_used_percent": 15.737862780002304
    },
    {
      "type": "training",
      "description": "Training step 2649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:42",
      "total_flops_so_far": 1.57438038380544e+16,
      "budget_used_percent": 15.743803838054399
    },
    {
      "type": "training",
      "description": "Training step 2650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:43",
      "total_flops_so_far": 1.5749744896106496e+16,
      "budget_used_percent": 15.749744896106495
    },
    {
      "type": "training",
      "description": "Training step 2651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:43",
      "total_flops_so_far": 1.5755685954158592e+16,
      "budget_used_percent": 15.75568595415859
    },
    {
      "type": "training",
      "description": "Training step 2652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:43",
      "total_flops_so_far": 1.5761627012210688e+16,
      "budget_used_percent": 15.761627012210688
    },
    {
      "type": "training",
      "description": "Training step 2653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:44",
      "total_flops_so_far": 1.5767568070262784e+16,
      "budget_used_percent": 15.767568070262783
    },
    {
      "type": "training",
      "description": "Training step 2654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:44",
      "total_flops_so_far": 1.577350912831488e+16,
      "budget_used_percent": 15.773509128314881
    },
    {
      "type": "training",
      "description": "Training step 2655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:44",
      "total_flops_so_far": 1.5779450186366976e+16,
      "budget_used_percent": 15.779450186366976
    },
    {
      "type": "training",
      "description": "Training step 2656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:45",
      "total_flops_so_far": 1.5785391244419072e+16,
      "budget_used_percent": 15.785391244419072
    },
    {
      "type": "training",
      "description": "Training step 2657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:45",
      "total_flops_so_far": 1.5791332302471168e+16,
      "budget_used_percent": 15.791332302471167
    },
    {
      "type": "training",
      "description": "Training step 2658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:46",
      "total_flops_so_far": 1.5797273360523264e+16,
      "budget_used_percent": 15.797273360523265
    },
    {
      "type": "training",
      "description": "Training step 2659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:46",
      "total_flops_so_far": 1.580321441857536e+16,
      "budget_used_percent": 15.80321441857536
    },
    {
      "type": "training",
      "description": "Training step 2660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:46",
      "total_flops_so_far": 1.5809155476627456e+16,
      "budget_used_percent": 15.809155476627456
    },
    {
      "type": "training",
      "description": "Training step 2661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:47",
      "total_flops_so_far": 1.5815096534679552e+16,
      "budget_used_percent": 15.815096534679551
    },
    {
      "type": "training",
      "description": "Training step 2662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:47",
      "total_flops_so_far": 1.5821037592731648e+16,
      "budget_used_percent": 15.82103759273165
    },
    {
      "type": "training",
      "description": "Training step 2663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:47",
      "total_flops_so_far": 1.5826978650783744e+16,
      "budget_used_percent": 15.826978650783744
    },
    {
      "type": "training",
      "description": "Training step 2664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:48",
      "total_flops_so_far": 1.583291970883584e+16,
      "budget_used_percent": 15.83291970883584
    },
    {
      "type": "training",
      "description": "Training step 2665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:48",
      "total_flops_so_far": 1.5838860766887936e+16,
      "budget_used_percent": 15.838860766887935
    },
    {
      "type": "training",
      "description": "Training step 2666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:48",
      "total_flops_so_far": 1.5844801824940032e+16,
      "budget_used_percent": 15.844801824940033
    },
    {
      "type": "training",
      "description": "Training step 2667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:49",
      "total_flops_so_far": 1.5850742882992128e+16,
      "budget_used_percent": 15.850742882992128
    },
    {
      "type": "training",
      "description": "Training step 2668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:49",
      "total_flops_so_far": 1.5856683941044224e+16,
      "budget_used_percent": 15.856683941044224
    },
    {
      "type": "training",
      "description": "Training step 2669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:50",
      "total_flops_so_far": 1.586262499909632e+16,
      "budget_used_percent": 15.862624999096319
    },
    {
      "type": "training",
      "description": "Training step 2670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:50",
      "total_flops_so_far": 1.5868566057148416e+16,
      "budget_used_percent": 15.868566057148417
    },
    {
      "type": "training",
      "description": "Training step 2671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:50",
      "total_flops_so_far": 1.5874507115200512e+16,
      "budget_used_percent": 15.874507115200512
    },
    {
      "type": "training",
      "description": "Training step 2672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:51",
      "total_flops_so_far": 1.5880448173252608e+16,
      "budget_used_percent": 15.88044817325261
    },
    {
      "type": "training",
      "description": "Training step 2673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:51",
      "total_flops_so_far": 1.5886389231304704e+16,
      "budget_used_percent": 15.886389231304705
    },
    {
      "type": "training",
      "description": "Training step 2674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:51",
      "total_flops_so_far": 1.58923302893568e+16,
      "budget_used_percent": 15.892330289356801
    },
    {
      "type": "training",
      "description": "Training step 2675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:52",
      "total_flops_so_far": 1.5898271347408896e+16,
      "budget_used_percent": 15.898271347408896
    },
    {
      "type": "training",
      "description": "Training step 2676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:52",
      "total_flops_so_far": 1.5904212405460992e+16,
      "budget_used_percent": 15.90421240546099
    },
    {
      "type": "training",
      "description": "Training step 2677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:52",
      "total_flops_so_far": 1.5910153463513088e+16,
      "budget_used_percent": 15.910153463513089
    },
    {
      "type": "training",
      "description": "Training step 2678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:53",
      "total_flops_so_far": 1.5916094521565184e+16,
      "budget_used_percent": 15.916094521565183
    },
    {
      "type": "training",
      "description": "Training step 2679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:53",
      "total_flops_so_far": 1.592203557961728e+16,
      "budget_used_percent": 15.92203557961728
    },
    {
      "type": "training",
      "description": "Training step 2680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:54",
      "total_flops_so_far": 1.5927976637669376e+16,
      "budget_used_percent": 15.927976637669374
    },
    {
      "type": "training",
      "description": "Training step 2681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:54",
      "total_flops_so_far": 1.5933917695721472e+16,
      "budget_used_percent": 15.933917695721473
    },
    {
      "type": "training",
      "description": "Training step 2682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:54",
      "total_flops_so_far": 1.5939858753773568e+16,
      "budget_used_percent": 15.939858753773567
    },
    {
      "type": "training",
      "description": "Training step 2683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:55",
      "total_flops_so_far": 1.5945799811825664e+16,
      "budget_used_percent": 15.945799811825664
    },
    {
      "type": "training",
      "description": "Training step 2684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:55",
      "total_flops_so_far": 1.595174086987776e+16,
      "budget_used_percent": 15.951740869877758
    },
    {
      "type": "training",
      "description": "Training step 2685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:55",
      "total_flops_so_far": 1.5957681927929856e+16,
      "budget_used_percent": 15.957681927929857
    },
    {
      "type": "training",
      "description": "Training step 2686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:56",
      "total_flops_so_far": 1.5963622985981952e+16,
      "budget_used_percent": 15.963622985981951
    },
    {
      "type": "training",
      "description": "Training step 2687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:56",
      "total_flops_so_far": 1.5969564044034048e+16,
      "budget_used_percent": 15.96956404403405
    },
    {
      "type": "training",
      "description": "Training step 2688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:57",
      "total_flops_so_far": 1.5975505102086144e+16,
      "budget_used_percent": 15.975505102086144
    },
    {
      "type": "training",
      "description": "Training step 2689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:57",
      "total_flops_so_far": 1.598144616013824e+16,
      "budget_used_percent": 15.98144616013824
    },
    {
      "type": "training",
      "description": "Training step 2690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:57",
      "total_flops_so_far": 1.5987387218190336e+16,
      "budget_used_percent": 15.987387218190335
    },
    {
      "type": "training",
      "description": "Training step 2691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:58",
      "total_flops_so_far": 1.5993328276242432e+16,
      "budget_used_percent": 15.993328276242433
    },
    {
      "type": "training",
      "description": "Training step 2692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:58",
      "total_flops_so_far": 1.5999269334294528e+16,
      "budget_used_percent": 15.999269334294528
    },
    {
      "type": "training",
      "description": "Training step 2693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:58",
      "total_flops_so_far": 1.6005210392346624e+16,
      "budget_used_percent": 16.005210392346626
    },
    {
      "type": "training",
      "description": "Training step 2694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:59",
      "total_flops_so_far": 1.601115145039872e+16,
      "budget_used_percent": 16.01115145039872
    },
    {
      "type": "training",
      "description": "Training step 2695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:59",
      "total_flops_so_far": 1.6017092508450816e+16,
      "budget_used_percent": 16.017092508450816
    },
    {
      "type": "training",
      "description": "Training step 2696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:44:59",
      "total_flops_so_far": 1.6023033566502912e+16,
      "budget_used_percent": 16.02303356650291
    },
    {
      "type": "training",
      "description": "Training step 2697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:00",
      "total_flops_so_far": 1.6028974624555008e+16,
      "budget_used_percent": 16.02897462455501
    },
    {
      "type": "training",
      "description": "Training step 2698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:00",
      "total_flops_so_far": 1.6034915682607104e+16,
      "budget_used_percent": 16.034915682607103
    },
    {
      "type": "training",
      "description": "Training step 2699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:01",
      "total_flops_so_far": 1.60408567406592e+16,
      "budget_used_percent": 16.0408567406592
    },
    {
      "type": "training",
      "description": "Training step 2700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:01",
      "total_flops_so_far": 1.6046797798711296e+16,
      "budget_used_percent": 16.046797798711296
    },
    {
      "type": "training",
      "description": "Training step 2701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:01",
      "total_flops_so_far": 1.6052738856763392e+16,
      "budget_used_percent": 16.052738856763394
    },
    {
      "type": "training",
      "description": "Training step 2702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:02",
      "total_flops_so_far": 1.6058679914815488e+16,
      "budget_used_percent": 16.05867991481549
    },
    {
      "type": "training",
      "description": "Training step 2703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:02",
      "total_flops_so_far": 1.6064620972867584e+16,
      "budget_used_percent": 16.064620972867584
    },
    {
      "type": "training",
      "description": "Training step 2704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:02",
      "total_flops_so_far": 1.607056203091968e+16,
      "budget_used_percent": 16.07056203091968
    },
    {
      "type": "training",
      "description": "Training step 2705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:03",
      "total_flops_so_far": 1.6076503088971776e+16,
      "budget_used_percent": 16.076503088971776
    },
    {
      "type": "training",
      "description": "Training step 2706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:03",
      "total_flops_so_far": 1.6082444147023872e+16,
      "budget_used_percent": 16.08244414702387
    },
    {
      "type": "training",
      "description": "Training step 2707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:04",
      "total_flops_so_far": 1.6088385205075968e+16,
      "budget_used_percent": 16.088385205075966
    },
    {
      "type": "training",
      "description": "Training step 2708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:04",
      "total_flops_so_far": 1.6094326263128064e+16,
      "budget_used_percent": 16.094326263128064
    },
    {
      "type": "training",
      "description": "Training step 2709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:04",
      "total_flops_so_far": 1.610026732118016e+16,
      "budget_used_percent": 16.10026732118016
    },
    {
      "type": "training",
      "description": "Training step 2710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:05",
      "total_flops_so_far": 1.6106208379232256e+16,
      "budget_used_percent": 16.106208379232257
    },
    {
      "type": "training",
      "description": "Training step 2711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:05",
      "total_flops_so_far": 1.6112149437284352e+16,
      "budget_used_percent": 16.11214943728435
    },
    {
      "type": "training",
      "description": "Training step 2712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:05",
      "total_flops_so_far": 1.6118090495336448e+16,
      "budget_used_percent": 16.11809049533645
    },
    {
      "type": "training",
      "description": "Training step 2713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:06",
      "total_flops_so_far": 1.6124031553388544e+16,
      "budget_used_percent": 16.124031553388544
    },
    {
      "type": "training",
      "description": "Training step 2714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:06",
      "total_flops_so_far": 1.612997261144064e+16,
      "budget_used_percent": 16.12997261144064
    },
    {
      "type": "training",
      "description": "Training step 2715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:06",
      "total_flops_so_far": 1.6135913669492736e+16,
      "budget_used_percent": 16.135913669492734
    },
    {
      "type": "training",
      "description": "Training step 2716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:07",
      "total_flops_so_far": 1.6141854727544832e+16,
      "budget_used_percent": 16.141854727544832
    },
    {
      "type": "training",
      "description": "Training step 2717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:08",
      "total_flops_so_far": 1.6147795785596928e+16,
      "budget_used_percent": 16.147795785596927
    },
    {
      "type": "training",
      "description": "Training step 2718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:08",
      "total_flops_so_far": 1.6153736843649024e+16,
      "budget_used_percent": 16.153736843649025
    },
    {
      "type": "training",
      "description": "Training step 2719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:08",
      "total_flops_so_far": 1.615967790170112e+16,
      "budget_used_percent": 16.15967790170112
    },
    {
      "type": "training",
      "description": "Training step 2720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:09",
      "total_flops_so_far": 1.6165618959753216e+16,
      "budget_used_percent": 16.165618959753218
    },
    {
      "type": "training",
      "description": "Training step 2721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:09",
      "total_flops_so_far": 1.6171560017805312e+16,
      "budget_used_percent": 16.171560017805312
    },
    {
      "type": "training",
      "description": "Training step 2722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:09",
      "total_flops_so_far": 1.6177501075857408e+16,
      "budget_used_percent": 16.17750107585741
    },
    {
      "type": "training",
      "description": "Training step 2723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:10",
      "total_flops_so_far": 1.6183442133909504e+16,
      "budget_used_percent": 16.183442133909505
    },
    {
      "type": "training",
      "description": "Training step 2724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:10",
      "total_flops_so_far": 1.61893831919616e+16,
      "budget_used_percent": 16.1893831919616
    },
    {
      "type": "training",
      "description": "Training step 2725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:10",
      "total_flops_so_far": 1.6195324250013696e+16,
      "budget_used_percent": 16.195324250013694
    },
    {
      "type": "training",
      "description": "Training step 2726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:11",
      "total_flops_so_far": 1.6201265308065792e+16,
      "budget_used_percent": 16.201265308065793
    },
    {
      "type": "training",
      "description": "Training step 2727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:11",
      "total_flops_so_far": 1.6207206366117888e+16,
      "budget_used_percent": 16.207206366117887
    },
    {
      "type": "training",
      "description": "Training step 2728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:12",
      "total_flops_so_far": 1.6213147424169984e+16,
      "budget_used_percent": 16.213147424169986
    },
    {
      "type": "training",
      "description": "Training step 2729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:12",
      "total_flops_so_far": 1.621908848222208e+16,
      "budget_used_percent": 16.21908848222208
    },
    {
      "type": "training",
      "description": "Training step 2730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:12",
      "total_flops_so_far": 1.6225029540274176e+16,
      "budget_used_percent": 16.22502954027418
    },
    {
      "type": "training",
      "description": "Training step 2731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:13",
      "total_flops_so_far": 1.6230970598326272e+16,
      "budget_used_percent": 16.230970598326273
    },
    {
      "type": "training",
      "description": "Training step 2732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:13",
      "total_flops_so_far": 1.6236911656378368e+16,
      "budget_used_percent": 16.236911656378368
    },
    {
      "type": "training",
      "description": "Training step 2733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:13",
      "total_flops_so_far": 1.6242852714430464e+16,
      "budget_used_percent": 16.242852714430462
    },
    {
      "type": "training",
      "description": "Training step 2734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:14",
      "total_flops_so_far": 1.624879377248256e+16,
      "budget_used_percent": 16.248793772482557
    },
    {
      "type": "training",
      "description": "Training step 2735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:14",
      "total_flops_so_far": 1.6254734830534656e+16,
      "budget_used_percent": 16.254734830534655
    },
    {
      "type": "training",
      "description": "Training step 2736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:15",
      "total_flops_so_far": 1.6260675888586752e+16,
      "budget_used_percent": 16.26067588858675
    },
    {
      "type": "training",
      "description": "Training step 2737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:15",
      "total_flops_so_far": 1.6266616946638848e+16,
      "budget_used_percent": 16.266616946638848
    },
    {
      "type": "training",
      "description": "Training step 2738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:15",
      "total_flops_so_far": 1.6272558004690944e+16,
      "budget_used_percent": 16.272558004690943
    },
    {
      "type": "training",
      "description": "Training step 2739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:16",
      "total_flops_so_far": 1.627849906274304e+16,
      "budget_used_percent": 16.27849906274304
    },
    {
      "type": "training",
      "description": "Training step 2740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:16",
      "total_flops_so_far": 1.6284440120795136e+16,
      "budget_used_percent": 16.284440120795136
    },
    {
      "type": "training",
      "description": "Training step 2741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:16",
      "total_flops_so_far": 1.6290381178847232e+16,
      "budget_used_percent": 16.290381178847234
    },
    {
      "type": "training",
      "description": "Training step 2742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:17",
      "total_flops_so_far": 1.6296322236899328e+16,
      "budget_used_percent": 16.29632223689933
    },
    {
      "type": "training",
      "description": "Training step 2743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:17",
      "total_flops_so_far": 1.6302263294951424e+16,
      "budget_used_percent": 16.302263294951423
    },
    {
      "type": "training",
      "description": "Training step 2744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:17",
      "total_flops_so_far": 1.630820435300352e+16,
      "budget_used_percent": 16.308204353003518
    },
    {
      "type": "training",
      "description": "Training step 2745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:18",
      "total_flops_so_far": 1.6314145411055616e+16,
      "budget_used_percent": 16.314145411055616
    },
    {
      "type": "training",
      "description": "Training step 2746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:18",
      "total_flops_so_far": 1.6320086469107712e+16,
      "budget_used_percent": 16.32008646910771
    },
    {
      "type": "training",
      "description": "Training step 2747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:19",
      "total_flops_so_far": 1.6326027527159808e+16,
      "budget_used_percent": 16.32602752715981
    },
    {
      "type": "training",
      "description": "Training step 2748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:19",
      "total_flops_so_far": 1.6331968585211904e+16,
      "budget_used_percent": 16.331968585211904
    },
    {
      "type": "training",
      "description": "Training step 2749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:19",
      "total_flops_so_far": 1.6337909643264e+16,
      "budget_used_percent": 16.337909643264002
    },
    {
      "type": "training",
      "description": "Training step 2750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:20",
      "total_flops_so_far": 1.6343850701316096e+16,
      "budget_used_percent": 16.343850701316097
    },
    {
      "type": "training",
      "description": "Training step 2751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:20",
      "total_flops_so_far": 1.6349791759368192e+16,
      "budget_used_percent": 16.349791759368195
    },
    {
      "type": "training",
      "description": "Training step 2752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:20",
      "total_flops_so_far": 1.6355732817420288e+16,
      "budget_used_percent": 16.35573281742029
    },
    {
      "type": "training",
      "description": "Training step 2753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:21",
      "total_flops_so_far": 1.6361673875472384e+16,
      "budget_used_percent": 16.361673875472384
    },
    {
      "type": "training",
      "description": "Training step 2754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:21",
      "total_flops_so_far": 1.636761493352448e+16,
      "budget_used_percent": 16.36761493352448
    },
    {
      "type": "training",
      "description": "Training step 2755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:22",
      "total_flops_so_far": 1.6373555991576576e+16,
      "budget_used_percent": 16.373555991576577
    },
    {
      "type": "training",
      "description": "Training step 2756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:22",
      "total_flops_so_far": 1.6379497049628672e+16,
      "budget_used_percent": 16.37949704962867
    },
    {
      "type": "training",
      "description": "Training step 2757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:22",
      "total_flops_so_far": 1.6385438107680768e+16,
      "budget_used_percent": 16.38543810768077
    },
    {
      "type": "training",
      "description": "Training step 2758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:23",
      "total_flops_so_far": 1.6391379165732864e+16,
      "budget_used_percent": 16.391379165732864
    },
    {
      "type": "training",
      "description": "Training step 2759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:23",
      "total_flops_so_far": 1.639732022378496e+16,
      "budget_used_percent": 16.397320223784963
    },
    {
      "type": "training",
      "description": "Training step 2760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:23",
      "total_flops_so_far": 1.6403261281837056e+16,
      "budget_used_percent": 16.403261281837057
    },
    {
      "type": "training",
      "description": "Training step 2761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:24",
      "total_flops_so_far": 1.6409202339889152e+16,
      "budget_used_percent": 16.409202339889152
    },
    {
      "type": "training",
      "description": "Training step 2762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:24",
      "total_flops_so_far": 1.6415143397941248e+16,
      "budget_used_percent": 16.415143397941247
    },
    {
      "type": "training",
      "description": "Training step 2763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:25",
      "total_flops_so_far": 1.6421084455993344e+16,
      "budget_used_percent": 16.42108445599334
    },
    {
      "type": "training",
      "description": "Training step 2764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:25",
      "total_flops_so_far": 1.642702551404544e+16,
      "budget_used_percent": 16.42702551404544
    },
    {
      "type": "training",
      "description": "Training step 2765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:25",
      "total_flops_so_far": 1.6432966572097536e+16,
      "budget_used_percent": 16.432966572097534
    },
    {
      "type": "training",
      "description": "Training step 2766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:26",
      "total_flops_so_far": 1.6438907630149632e+16,
      "budget_used_percent": 16.438907630149632
    },
    {
      "type": "training",
      "description": "Training step 2767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:26",
      "total_flops_so_far": 1.6444848688201728e+16,
      "budget_used_percent": 16.444848688201727
    },
    {
      "type": "training",
      "description": "Training step 2768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:26",
      "total_flops_so_far": 1.6450789746253824e+16,
      "budget_used_percent": 16.450789746253825
    },
    {
      "type": "training",
      "description": "Training step 2769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:27",
      "total_flops_so_far": 1.645673080430592e+16,
      "budget_used_percent": 16.45673080430592
    },
    {
      "type": "training",
      "description": "Training step 2770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:27",
      "total_flops_so_far": 1.6462671862358016e+16,
      "budget_used_percent": 16.462671862358018
    },
    {
      "type": "training",
      "description": "Training step 2771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:27",
      "total_flops_so_far": 1.6468612920410112e+16,
      "budget_used_percent": 16.468612920410113
    },
    {
      "type": "training",
      "description": "Training step 2772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:28",
      "total_flops_so_far": 1.6474553978462208e+16,
      "budget_used_percent": 16.474553978462207
    },
    {
      "type": "training",
      "description": "Training step 2773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:28",
      "total_flops_so_far": 1.6480495036514304e+16,
      "budget_used_percent": 16.480495036514302
    },
    {
      "type": "training",
      "description": "Training step 2774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:29",
      "total_flops_so_far": 1.64864360945664e+16,
      "budget_used_percent": 16.4864360945664
    },
    {
      "type": "training",
      "description": "Training step 2775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:29",
      "total_flops_so_far": 1.6492377152618496e+16,
      "budget_used_percent": 16.492377152618495
    },
    {
      "type": "training",
      "description": "Training step 2776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:29",
      "total_flops_so_far": 1.6498318210670592e+16,
      "budget_used_percent": 16.498318210670593
    },
    {
      "type": "training",
      "description": "Training step 2777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:30",
      "total_flops_so_far": 1.6504259268722688e+16,
      "budget_used_percent": 16.504259268722688
    },
    {
      "type": "training",
      "description": "Training step 2778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:30",
      "total_flops_so_far": 1.6510200326774784e+16,
      "budget_used_percent": 16.510200326774786
    },
    {
      "type": "training",
      "description": "Training step 2779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:30",
      "total_flops_so_far": 1.651614138482688e+16,
      "budget_used_percent": 16.51614138482688
    },
    {
      "type": "training",
      "description": "Training step 2780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:31",
      "total_flops_so_far": 1.6522082442878976e+16,
      "budget_used_percent": 16.522082442878975
    },
    {
      "type": "training",
      "description": "Training step 2781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:31",
      "total_flops_so_far": 1.6528023500931072e+16,
      "budget_used_percent": 16.52802350093107
    },
    {
      "type": "training",
      "description": "Training step 2782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:32",
      "total_flops_so_far": 1.6533964558983168e+16,
      "budget_used_percent": 16.53396455898317
    },
    {
      "type": "training",
      "description": "Training step 2783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:32",
      "total_flops_so_far": 1.6539905617035264e+16,
      "budget_used_percent": 16.539905617035263
    },
    {
      "type": "training",
      "description": "Training step 2784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:32",
      "total_flops_so_far": 1.654584667508736e+16,
      "budget_used_percent": 16.54584667508736
    },
    {
      "type": "training",
      "description": "Training step 2785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:33",
      "total_flops_so_far": 1.6551787733139456e+16,
      "budget_used_percent": 16.551787733139456
    },
    {
      "type": "training",
      "description": "Training step 2786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:33",
      "total_flops_so_far": 1.6557728791191552e+16,
      "budget_used_percent": 16.557728791191554
    },
    {
      "type": "training",
      "description": "Training step 2787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:33",
      "total_flops_so_far": 1.6563669849243648e+16,
      "budget_used_percent": 16.56366984924365
    },
    {
      "type": "training",
      "description": "Training step 2788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:34",
      "total_flops_so_far": 1.6569610907295744e+16,
      "budget_used_percent": 16.569610907295747
    },
    {
      "type": "training",
      "description": "Training step 2789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:34",
      "total_flops_so_far": 1.657555196534784e+16,
      "budget_used_percent": 16.57555196534784
    },
    {
      "type": "training",
      "description": "Training step 2790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:34",
      "total_flops_so_far": 1.6581493023399936e+16,
      "budget_used_percent": 16.581493023399936
    },
    {
      "type": "training",
      "description": "Training step 2791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:35",
      "total_flops_so_far": 1.6587434081452032e+16,
      "budget_used_percent": 16.58743408145203
    },
    {
      "type": "training",
      "description": "Training step 2792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:35",
      "total_flops_so_far": 1.6593375139504128e+16,
      "budget_used_percent": 16.593375139504126
    },
    {
      "type": "training",
      "description": "Training step 2793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:36",
      "total_flops_so_far": 1.6599316197556224e+16,
      "budget_used_percent": 16.599316197556224
    },
    {
      "type": "training",
      "description": "Training step 2794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:36",
      "total_flops_so_far": 1.660525725560832e+16,
      "budget_used_percent": 16.60525725560832
    },
    {
      "type": "training",
      "description": "Training step 2795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:36",
      "total_flops_so_far": 1.6611198313660416e+16,
      "budget_used_percent": 16.611198313660417
    },
    {
      "type": "training",
      "description": "Training step 2796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:37",
      "total_flops_so_far": 1.6617139371712512e+16,
      "budget_used_percent": 16.61713937171251
    },
    {
      "type": "training",
      "description": "Training step 2797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:37",
      "total_flops_so_far": 1.6623080429764608e+16,
      "budget_used_percent": 16.62308042976461
    },
    {
      "type": "training",
      "description": "Training step 2798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:37",
      "total_flops_so_far": 1.6629021487816704e+16,
      "budget_used_percent": 16.629021487816704
    },
    {
      "type": "training",
      "description": "Training step 2799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:38",
      "total_flops_so_far": 1.66349625458688e+16,
      "budget_used_percent": 16.6349625458688
    },
    {
      "type": "training",
      "description": "Training step 2800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:38",
      "total_flops_so_far": 1.6640903603920896e+16,
      "budget_used_percent": 16.640903603920894
    },
    {
      "type": "training",
      "description": "Training step 2801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:39",
      "total_flops_so_far": 1.6646844661972992e+16,
      "budget_used_percent": 16.64684466197299
    },
    {
      "type": "training",
      "description": "Training step 2802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:39",
      "total_flops_so_far": 1.6652785720025088e+16,
      "budget_used_percent": 16.652785720025086
    },
    {
      "type": "training",
      "description": "Training step 2803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:39",
      "total_flops_so_far": 1.6658726778077184e+16,
      "budget_used_percent": 16.658726778077185
    },
    {
      "type": "training",
      "description": "Training step 2804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:40",
      "total_flops_so_far": 1.666466783612928e+16,
      "budget_used_percent": 16.66466783612928
    },
    {
      "type": "training",
      "description": "Training step 2805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:40",
      "total_flops_so_far": 1.6670608894181376e+16,
      "budget_used_percent": 16.670608894181377
    },
    {
      "type": "training",
      "description": "Training step 2806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:40",
      "total_flops_so_far": 1.6676549952233472e+16,
      "budget_used_percent": 16.676549952233472
    },
    {
      "type": "training",
      "description": "Training step 2807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:41",
      "total_flops_so_far": 1.6682491010285568e+16,
      "budget_used_percent": 16.68249101028557
    },
    {
      "type": "training",
      "description": "Training step 2808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:41",
      "total_flops_so_far": 1.6688432068337664e+16,
      "budget_used_percent": 16.688432068337665
    },
    {
      "type": "training",
      "description": "Training step 2809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:41",
      "total_flops_so_far": 1.669437312638976e+16,
      "budget_used_percent": 16.69437312638976
    },
    {
      "type": "training",
      "description": "Training step 2810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:42",
      "total_flops_so_far": 1.6700314184441856e+16,
      "budget_used_percent": 16.700314184441854
    },
    {
      "type": "training",
      "description": "Training step 2811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:42",
      "total_flops_so_far": 1.6706255242493952e+16,
      "budget_used_percent": 16.706255242493953
    },
    {
      "type": "training",
      "description": "Training step 2812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:42",
      "total_flops_so_far": 1.6712196300546048e+16,
      "budget_used_percent": 16.712196300546047
    },
    {
      "type": "training",
      "description": "Training step 2813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:43",
      "total_flops_so_far": 1.6718137358598144e+16,
      "budget_used_percent": 16.718137358598145
    },
    {
      "type": "training",
      "description": "Training step 2814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:43",
      "total_flops_so_far": 1.672407841665024e+16,
      "budget_used_percent": 16.72407841665024
    },
    {
      "type": "training",
      "description": "Training step 2815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:44",
      "total_flops_so_far": 1.6730019474702336e+16,
      "budget_used_percent": 16.73001947470234
    },
    {
      "type": "training",
      "description": "Training step 2816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:44",
      "total_flops_so_far": 1.6735960532754432e+16,
      "budget_used_percent": 16.735960532754433
    },
    {
      "type": "training",
      "description": "Training step 2817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:44",
      "total_flops_so_far": 1.6741901590806528e+16,
      "budget_used_percent": 16.741901590806528
    },
    {
      "type": "training",
      "description": "Training step 2818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:45",
      "total_flops_so_far": 1.6747842648858624e+16,
      "budget_used_percent": 16.747842648858622
    },
    {
      "type": "training",
      "description": "Training step 2819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:45",
      "total_flops_so_far": 1.675378370691072e+16,
      "budget_used_percent": 16.753783706910717
    },
    {
      "type": "training",
      "description": "Training step 2820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:45",
      "total_flops_so_far": 1.6759724764962816e+16,
      "budget_used_percent": 16.759724764962815
    },
    {
      "type": "training",
      "description": "Training step 2821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:46",
      "total_flops_so_far": 1.6765665823014912e+16,
      "budget_used_percent": 16.76566582301491
    },
    {
      "type": "training",
      "description": "Training step 2822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:46",
      "total_flops_so_far": 1.6771606881067008e+16,
      "budget_used_percent": 16.771606881067008
    },
    {
      "type": "training",
      "description": "Training step 2823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:47",
      "total_flops_so_far": 1.6777547939119104e+16,
      "budget_used_percent": 16.777547939119103
    },
    {
      "type": "training",
      "description": "Training step 2824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:47",
      "total_flops_so_far": 1.67834889971712e+16,
      "budget_used_percent": 16.7834889971712
    },
    {
      "type": "training",
      "description": "Training step 2825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:48",
      "total_flops_so_far": 1.6789430055223296e+16,
      "budget_used_percent": 16.789430055223296
    },
    {
      "type": "training",
      "description": "Training step 2826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:48",
      "total_flops_so_far": 1.6795371113275392e+16,
      "budget_used_percent": 16.795371113275394
    },
    {
      "type": "training",
      "description": "Training step 2827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:48",
      "total_flops_so_far": 1.6801312171327488e+16,
      "budget_used_percent": 16.80131217132749
    },
    {
      "type": "training",
      "description": "Training step 2828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:49",
      "total_flops_so_far": 1.6807253229379584e+16,
      "budget_used_percent": 16.807253229379583
    },
    {
      "type": "training",
      "description": "Training step 2829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:49",
      "total_flops_so_far": 1.681319428743168e+16,
      "budget_used_percent": 16.813194287431678
    },
    {
      "type": "training",
      "description": "Training step 2830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:49",
      "total_flops_so_far": 1.6819135345483776e+16,
      "budget_used_percent": 16.819135345483776
    },
    {
      "type": "training",
      "description": "Training step 2831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:50",
      "total_flops_so_far": 1.6825076403535872e+16,
      "budget_used_percent": 16.82507640353587
    },
    {
      "type": "training",
      "description": "Training step 2832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:50",
      "total_flops_so_far": 1.6831017461587968e+16,
      "budget_used_percent": 16.83101746158797
    },
    {
      "type": "training",
      "description": "Training step 2833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:51",
      "total_flops_so_far": 1.6836958519640064e+16,
      "budget_used_percent": 16.836958519640064
    },
    {
      "type": "training",
      "description": "Training step 2834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:51",
      "total_flops_so_far": 1.684289957769216e+16,
      "budget_used_percent": 16.84289957769216
    },
    {
      "type": "training",
      "description": "Training step 2835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:51",
      "total_flops_so_far": 1.6848840635744256e+16,
      "budget_used_percent": 16.848840635744256
    },
    {
      "type": "training",
      "description": "Training step 2836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:52",
      "total_flops_so_far": 1.6854781693796352e+16,
      "budget_used_percent": 16.854781693796355
    },
    {
      "type": "training",
      "description": "Training step 2837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:52",
      "total_flops_so_far": 1.6860722751848448e+16,
      "budget_used_percent": 16.86072275184845
    },
    {
      "type": "training",
      "description": "Training step 2838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:52",
      "total_flops_so_far": 1.6866663809900544e+16,
      "budget_used_percent": 16.866663809900544
    },
    {
      "type": "training",
      "description": "Training step 2839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:53",
      "total_flops_so_far": 1.687260486795264e+16,
      "budget_used_percent": 16.87260486795264
    },
    {
      "type": "training",
      "description": "Training step 2840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:53",
      "total_flops_so_far": 1.6878545926004736e+16,
      "budget_used_percent": 16.878545926004737
    },
    {
      "type": "training",
      "description": "Training step 2841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:54",
      "total_flops_so_far": 1.6884486984056832e+16,
      "budget_used_percent": 16.88448698405683
    },
    {
      "type": "training",
      "description": "Training step 2842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:54",
      "total_flops_so_far": 1.6890428042108928e+16,
      "budget_used_percent": 16.89042804210893
    },
    {
      "type": "training",
      "description": "Training step 2843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:54",
      "total_flops_so_far": 1.6896369100161024e+16,
      "budget_used_percent": 16.896369100161024
    },
    {
      "type": "training",
      "description": "Training step 2844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:55",
      "total_flops_so_far": 1.690231015821312e+16,
      "budget_used_percent": 16.902310158213123
    },
    {
      "type": "training",
      "description": "Training step 2845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:55",
      "total_flops_so_far": 1.6908251216265216e+16,
      "budget_used_percent": 16.908251216265217
    },
    {
      "type": "training",
      "description": "Training step 2846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:55",
      "total_flops_so_far": 1.6914192274317312e+16,
      "budget_used_percent": 16.914192274317312
    },
    {
      "type": "training",
      "description": "Training step 2847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:56",
      "total_flops_so_far": 1.6920133332369408e+16,
      "budget_used_percent": 16.920133332369407
    },
    {
      "type": "training",
      "description": "Training step 2848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:56",
      "total_flops_so_far": 1.6926074390421504e+16,
      "budget_used_percent": 16.9260743904215
    },
    {
      "type": "training",
      "description": "Training step 2849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:57",
      "total_flops_so_far": 1.69320154484736e+16,
      "budget_used_percent": 16.9320154484736
    },
    {
      "type": "training",
      "description": "Training step 2850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:57",
      "total_flops_so_far": 1.6937956506525696e+16,
      "budget_used_percent": 16.937956506525694
    },
    {
      "type": "training",
      "description": "Training step 2851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:57",
      "total_flops_so_far": 1.6943897564577792e+16,
      "budget_used_percent": 16.943897564577792
    },
    {
      "type": "training",
      "description": "Training step 2852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:58",
      "total_flops_so_far": 1.6949838622629888e+16,
      "budget_used_percent": 16.949838622629887
    },
    {
      "type": "training",
      "description": "Training step 2853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:58",
      "total_flops_so_far": 1.6955779680681984e+16,
      "budget_used_percent": 16.955779680681985
    },
    {
      "type": "training",
      "description": "Training step 2854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:58",
      "total_flops_so_far": 1.696172073873408e+16,
      "budget_used_percent": 16.96172073873408
    },
    {
      "type": "training",
      "description": "Training step 2855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:59",
      "total_flops_so_far": 1.6967661796786176e+16,
      "budget_used_percent": 16.967661796786178
    },
    {
      "type": "training",
      "description": "Training step 2856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:59",
      "total_flops_so_far": 1.6973602854838272e+16,
      "budget_used_percent": 16.973602854838273
    },
    {
      "type": "training",
      "description": "Training step 2857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:45:59",
      "total_flops_so_far": 1.6979543912890368e+16,
      "budget_used_percent": 16.979543912890367
    },
    {
      "type": "training",
      "description": "Training step 2858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:00",
      "total_flops_so_far": 1.6985484970942464e+16,
      "budget_used_percent": 16.985484970942462
    },
    {
      "type": "training",
      "description": "Training step 2859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:00",
      "total_flops_so_far": 1.699142602899456e+16,
      "budget_used_percent": 16.99142602899456
    },
    {
      "type": "training",
      "description": "Training step 2860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:01",
      "total_flops_so_far": 1.6997367087046656e+16,
      "budget_used_percent": 16.997367087046655
    },
    {
      "type": "training",
      "description": "Training step 2861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:01",
      "total_flops_so_far": 1.7003308145098752e+16,
      "budget_used_percent": 17.003308145098753
    },
    {
      "type": "training",
      "description": "Training step 2862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:01",
      "total_flops_so_far": 1.7009249203150848e+16,
      "budget_used_percent": 17.009249203150848
    },
    {
      "type": "training",
      "description": "Training step 2863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:02",
      "total_flops_so_far": 1.7015190261202944e+16,
      "budget_used_percent": 17.015190261202946
    },
    {
      "type": "training",
      "description": "Training step 2864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:02",
      "total_flops_so_far": 1.702113131925504e+16,
      "budget_used_percent": 17.02113131925504
    },
    {
      "type": "training",
      "description": "Training step 2865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:02",
      "total_flops_so_far": 1.7027072377307136e+16,
      "budget_used_percent": 17.027072377307135
    },
    {
      "type": "training",
      "description": "Training step 2866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:03",
      "total_flops_so_far": 1.7033013435359232e+16,
      "budget_used_percent": 17.03301343535923
    },
    {
      "type": "training",
      "description": "Training step 2867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:03",
      "total_flops_so_far": 1.7038954493411328e+16,
      "budget_used_percent": 17.038954493411328
    },
    {
      "type": "training",
      "description": "Training step 2868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:04",
      "total_flops_so_far": 1.7044895551463424e+16,
      "budget_used_percent": 17.044895551463423
    },
    {
      "type": "training",
      "description": "Training step 2869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:04",
      "total_flops_so_far": 1.705083660951552e+16,
      "budget_used_percent": 17.05083660951552
    },
    {
      "type": "training",
      "description": "Training step 2870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:04",
      "total_flops_so_far": 1.7056777667567616e+16,
      "budget_used_percent": 17.056777667567616
    },
    {
      "type": "training",
      "description": "Training step 2871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:05",
      "total_flops_so_far": 1.7062718725619712e+16,
      "budget_used_percent": 17.062718725619714
    },
    {
      "type": "training",
      "description": "Training step 2872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:05",
      "total_flops_so_far": 1.7068659783671808e+16,
      "budget_used_percent": 17.06865978367181
    },
    {
      "type": "training",
      "description": "Training step 2873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:05",
      "total_flops_so_far": 1.7074600841723904e+16,
      "budget_used_percent": 17.074600841723907
    },
    {
      "type": "training",
      "description": "Training step 2874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:06",
      "total_flops_so_far": 1.7080541899776e+16,
      "budget_used_percent": 17.080541899776
    },
    {
      "type": "training",
      "description": "Training step 2875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:06",
      "total_flops_so_far": 1.7086482957828096e+16,
      "budget_used_percent": 17.086482957828096
    },
    {
      "type": "training",
      "description": "Training step 2876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:07",
      "total_flops_so_far": 1.7092424015880192e+16,
      "budget_used_percent": 17.09242401588019
    },
    {
      "type": "training",
      "description": "Training step 2877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:07",
      "total_flops_so_far": 1.7098365073932288e+16,
      "budget_used_percent": 17.098365073932285
    },
    {
      "type": "training",
      "description": "Training step 2878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:07",
      "total_flops_so_far": 1.7104306131984384e+16,
      "budget_used_percent": 17.104306131984384
    },
    {
      "type": "training",
      "description": "Training step 2879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:08",
      "total_flops_so_far": 1.711024719003648e+16,
      "budget_used_percent": 17.11024719003648
    },
    {
      "type": "training",
      "description": "Training step 2880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:08",
      "total_flops_so_far": 1.7116188248088576e+16,
      "budget_used_percent": 17.116188248088577
    },
    {
      "type": "training",
      "description": "Training step 2881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:08",
      "total_flops_so_far": 1.7122129306140672e+16,
      "budget_used_percent": 17.12212930614067
    },
    {
      "type": "training",
      "description": "Training step 2882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:09",
      "total_flops_so_far": 1.7128070364192768e+16,
      "budget_used_percent": 17.12807036419277
    },
    {
      "type": "training",
      "description": "Training step 2883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:09",
      "total_flops_so_far": 1.7134011422244864e+16,
      "budget_used_percent": 17.134011422244864
    },
    {
      "type": "training",
      "description": "Training step 2884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:10",
      "total_flops_so_far": 1.713995248029696e+16,
      "budget_used_percent": 17.139952480296962
    },
    {
      "type": "training",
      "description": "Training step 2885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:10",
      "total_flops_so_far": 1.7145893538349056e+16,
      "budget_used_percent": 17.145893538349057
    },
    {
      "type": "training",
      "description": "Training step 2886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:10",
      "total_flops_so_far": 1.7151834596401152e+16,
      "budget_used_percent": 17.15183459640115
    },
    {
      "type": "training",
      "description": "Training step 2887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:11",
      "total_flops_so_far": 1.7157775654453248e+16,
      "budget_used_percent": 17.157775654453246
    },
    {
      "type": "training",
      "description": "Training step 2888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:11",
      "total_flops_so_far": 1.7163716712505344e+16,
      "budget_used_percent": 17.163716712505344
    },
    {
      "type": "training",
      "description": "Training step 2889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:11",
      "total_flops_so_far": 1.716965777055744e+16,
      "budget_used_percent": 17.16965777055744
    },
    {
      "type": "training",
      "description": "Training step 2890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:12",
      "total_flops_so_far": 1.7175598828609536e+16,
      "budget_used_percent": 17.175598828609537
    },
    {
      "type": "training",
      "description": "Training step 2891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:12",
      "total_flops_so_far": 1.7181539886661632e+16,
      "budget_used_percent": 17.181539886661632
    },
    {
      "type": "training",
      "description": "Training step 2892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:13",
      "total_flops_so_far": 1.7187480944713728e+16,
      "budget_used_percent": 17.18748094471373
    },
    {
      "type": "training",
      "description": "Training step 2893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:13",
      "total_flops_so_far": 1.7193422002765824e+16,
      "budget_used_percent": 17.193422002765825
    },
    {
      "type": "training",
      "description": "Training step 2894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:14",
      "total_flops_so_far": 1.719936306081792e+16,
      "budget_used_percent": 17.19936306081792
    },
    {
      "type": "training",
      "description": "Training step 2895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:14",
      "total_flops_so_far": 1.7205304118870016e+16,
      "budget_used_percent": 17.205304118870014
    },
    {
      "type": "training",
      "description": "Training step 2896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:14",
      "total_flops_so_far": 1.7211245176922112e+16,
      "budget_used_percent": 17.211245176922112
    },
    {
      "type": "training",
      "description": "Training step 2897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:15",
      "total_flops_so_far": 1.7217186234974208e+16,
      "budget_used_percent": 17.217186234974207
    },
    {
      "type": "training",
      "description": "Training step 2898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:15",
      "total_flops_so_far": 1.7223127293026304e+16,
      "budget_used_percent": 17.223127293026305
    },
    {
      "type": "training",
      "description": "Training step 2899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:15",
      "total_flops_so_far": 1.72290683510784e+16,
      "budget_used_percent": 17.2290683510784
    },
    {
      "type": "training",
      "description": "Training step 2900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:16",
      "total_flops_so_far": 1.7235009409130496e+16,
      "budget_used_percent": 17.235009409130498
    },
    {
      "type": "training",
      "description": "Training step 2901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:16",
      "total_flops_so_far": 1.7240950467182592e+16,
      "budget_used_percent": 17.240950467182593
    },
    {
      "type": "training",
      "description": "Training step 2902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:17",
      "total_flops_so_far": 1.7246891525234688e+16,
      "budget_used_percent": 17.24689152523469
    },
    {
      "type": "training",
      "description": "Training step 2903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:17",
      "total_flops_so_far": 1.7252832583286784e+16,
      "budget_used_percent": 17.252832583286786
    },
    {
      "type": "training",
      "description": "Training step 2904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:17",
      "total_flops_so_far": 1.725877364133888e+16,
      "budget_used_percent": 17.25877364133888
    },
    {
      "type": "training",
      "description": "Training step 2905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:18",
      "total_flops_so_far": 1.7264714699390976e+16,
      "budget_used_percent": 17.264714699390975
    },
    {
      "type": "training",
      "description": "Training step 2906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:18",
      "total_flops_so_far": 1.7270655757443072e+16,
      "budget_used_percent": 17.27065575744307
    },
    {
      "type": "training",
      "description": "Training step 2907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:18",
      "total_flops_so_far": 1.7276596815495168e+16,
      "budget_used_percent": 17.276596815495168
    },
    {
      "type": "training",
      "description": "Training step 2908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:19",
      "total_flops_so_far": 1.7282537873547264e+16,
      "budget_used_percent": 17.282537873547263
    },
    {
      "type": "training",
      "description": "Training step 2909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:19",
      "total_flops_so_far": 1.728847893159936e+16,
      "budget_used_percent": 17.28847893159936
    },
    {
      "type": "training",
      "description": "Training step 2910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:20",
      "total_flops_so_far": 1.7294419989651456e+16,
      "budget_used_percent": 17.294419989651455
    },
    {
      "type": "training",
      "description": "Training step 2911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:20",
      "total_flops_so_far": 1.7300361047703552e+16,
      "budget_used_percent": 17.300361047703554
    },
    {
      "type": "training",
      "description": "Training step 2912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:20",
      "total_flops_so_far": 1.7306302105755648e+16,
      "budget_used_percent": 17.30630210575565
    },
    {
      "type": "training",
      "description": "Training step 2913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:21",
      "total_flops_so_far": 1.7312243163807744e+16,
      "budget_used_percent": 17.312243163807743
    },
    {
      "type": "training",
      "description": "Training step 2914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:21",
      "total_flops_so_far": 1.731818422185984e+16,
      "budget_used_percent": 17.318184221859838
    },
    {
      "type": "training",
      "description": "Training step 2915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:21",
      "total_flops_so_far": 1.7324125279911936e+16,
      "budget_used_percent": 17.324125279911936
    },
    {
      "type": "training",
      "description": "Training step 2916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:22",
      "total_flops_so_far": 1.7330066337964032e+16,
      "budget_used_percent": 17.33006633796403
    },
    {
      "type": "training",
      "description": "Training step 2917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:22",
      "total_flops_so_far": 1.7336007396016128e+16,
      "budget_used_percent": 17.33600739601613
    },
    {
      "type": "training",
      "description": "Training step 2918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:22",
      "total_flops_so_far": 1.7341948454068224e+16,
      "budget_used_percent": 17.341948454068223
    },
    {
      "type": "training",
      "description": "Training step 2919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:23",
      "total_flops_so_far": 1.734788951212032e+16,
      "budget_used_percent": 17.34788951212032
    },
    {
      "type": "training",
      "description": "Training step 2920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:23",
      "total_flops_so_far": 1.7353830570172416e+16,
      "budget_used_percent": 17.353830570172416
    },
    {
      "type": "training",
      "description": "Training step 2921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:24",
      "total_flops_so_far": 1.7359771628224512e+16,
      "budget_used_percent": 17.359771628224514
    },
    {
      "type": "training",
      "description": "Training step 2922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:24",
      "total_flops_so_far": 1.7365712686276608e+16,
      "budget_used_percent": 17.36571268627661
    },
    {
      "type": "training",
      "description": "Training step 2923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:24",
      "total_flops_so_far": 1.7371653744328704e+16,
      "budget_used_percent": 17.371653744328704
    },
    {
      "type": "training",
      "description": "Training step 2924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:25",
      "total_flops_so_far": 1.73775948023808e+16,
      "budget_used_percent": 17.3775948023808
    },
    {
      "type": "training",
      "description": "Training step 2925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:25",
      "total_flops_so_far": 1.7383535860432896e+16,
      "budget_used_percent": 17.383535860432897
    },
    {
      "type": "training",
      "description": "Training step 2926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:25",
      "total_flops_so_far": 1.7389476918484992e+16,
      "budget_used_percent": 17.38947691848499
    },
    {
      "type": "training",
      "description": "Training step 2927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:26",
      "total_flops_so_far": 1.7395417976537088e+16,
      "budget_used_percent": 17.39541797653709
    },
    {
      "type": "training",
      "description": "Training step 2928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:26",
      "total_flops_so_far": 1.7401359034589184e+16,
      "budget_used_percent": 17.401359034589184
    },
    {
      "type": "training",
      "description": "Training step 2929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:27",
      "total_flops_so_far": 1.740730009264128e+16,
      "budget_used_percent": 17.407300092641282
    },
    {
      "type": "training",
      "description": "Training step 2930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:27",
      "total_flops_so_far": 1.7413241150693376e+16,
      "budget_used_percent": 17.413241150693377
    },
    {
      "type": "training",
      "description": "Training step 2931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:27",
      "total_flops_so_far": 1.7419182208745472e+16,
      "budget_used_percent": 17.41918220874547
    },
    {
      "type": "training",
      "description": "Training step 2932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:28",
      "total_flops_so_far": 1.7425123266797568e+16,
      "budget_used_percent": 17.425123266797566
    },
    {
      "type": "training",
      "description": "Training step 2933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:28",
      "total_flops_so_far": 1.7431064324849664e+16,
      "budget_used_percent": 17.43106432484966
    },
    {
      "type": "training",
      "description": "Training step 2934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:28",
      "total_flops_so_far": 1.743700538290176e+16,
      "budget_used_percent": 17.43700538290176
    },
    {
      "type": "training",
      "description": "Training step 2935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:29",
      "total_flops_so_far": 1.7442946440953856e+16,
      "budget_used_percent": 17.442946440953854
    },
    {
      "type": "training",
      "description": "Training step 2936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:29",
      "total_flops_so_far": 1.7448887499005952e+16,
      "budget_used_percent": 17.448887499005952
    },
    {
      "type": "training",
      "description": "Training step 2937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:30",
      "total_flops_so_far": 1.7454828557058048e+16,
      "budget_used_percent": 17.454828557058047
    },
    {
      "type": "training",
      "description": "Training step 2938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:30",
      "total_flops_so_far": 1.7460769615110144e+16,
      "budget_used_percent": 17.460769615110145
    },
    {
      "type": "training",
      "description": "Training step 2939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:30",
      "total_flops_so_far": 1.746671067316224e+16,
      "budget_used_percent": 17.46671067316224
    },
    {
      "type": "training",
      "description": "Training step 2940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:31",
      "total_flops_so_far": 1.7472651731214336e+16,
      "budget_used_percent": 17.472651731214338
    },
    {
      "type": "training",
      "description": "Training step 2941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:31",
      "total_flops_so_far": 1.7478592789266432e+16,
      "budget_used_percent": 17.478592789266433
    },
    {
      "type": "training",
      "description": "Training step 2942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:31",
      "total_flops_so_far": 1.7484533847318528e+16,
      "budget_used_percent": 17.484533847318527
    },
    {
      "type": "training",
      "description": "Training step 2943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:32",
      "total_flops_so_far": 1.7490474905370624e+16,
      "budget_used_percent": 17.490474905370622
    },
    {
      "type": "training",
      "description": "Training step 2944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:32",
      "total_flops_so_far": 1.749641596342272e+16,
      "budget_used_percent": 17.49641596342272
    },
    {
      "type": "training",
      "description": "Training step 2945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:33",
      "total_flops_so_far": 1.7502357021474816e+16,
      "budget_used_percent": 17.502357021474815
    },
    {
      "type": "training",
      "description": "Training step 2946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:33",
      "total_flops_so_far": 1.7508298079526912e+16,
      "budget_used_percent": 17.508298079526913
    },
    {
      "type": "training",
      "description": "Training step 2947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:33",
      "total_flops_so_far": 1.7514239137579008e+16,
      "budget_used_percent": 17.514239137579008
    },
    {
      "type": "training",
      "description": "Training step 2948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:34",
      "total_flops_so_far": 1.7520180195631104e+16,
      "budget_used_percent": 17.520180195631106
    },
    {
      "type": "training",
      "description": "Training step 2949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:34",
      "total_flops_so_far": 1.75261212536832e+16,
      "budget_used_percent": 17.5261212536832
    },
    {
      "type": "training",
      "description": "Training step 2950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:34",
      "total_flops_so_far": 1.7532062311735296e+16,
      "budget_used_percent": 17.532062311735295
    },
    {
      "type": "training",
      "description": "Training step 2951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:35",
      "total_flops_so_far": 1.7538003369787392e+16,
      "budget_used_percent": 17.53800336978739
    },
    {
      "type": "training",
      "description": "Training step 2952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:35",
      "total_flops_so_far": 1.7543944427839488e+16,
      "budget_used_percent": 17.543944427839488
    },
    {
      "type": "training",
      "description": "Training step 2953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:36",
      "total_flops_so_far": 1.7549885485891584e+16,
      "budget_used_percent": 17.549885485891583
    },
    {
      "type": "training",
      "description": "Training step 2954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:36",
      "total_flops_so_far": 1.755582654394368e+16,
      "budget_used_percent": 17.55582654394368
    },
    {
      "type": "training",
      "description": "Training step 2955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:36",
      "total_flops_so_far": 1.7561767601995776e+16,
      "budget_used_percent": 17.561767601995776
    },
    {
      "type": "training",
      "description": "Training step 2956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:37",
      "total_flops_so_far": 1.7567708660047872e+16,
      "budget_used_percent": 17.567708660047874
    },
    {
      "type": "training",
      "description": "Training step 2957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:37",
      "total_flops_so_far": 1.7573649718099968e+16,
      "budget_used_percent": 17.57364971809997
    },
    {
      "type": "training",
      "description": "Training step 2958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:37",
      "total_flops_so_far": 1.7579590776152064e+16,
      "budget_used_percent": 17.579590776152067
    },
    {
      "type": "training",
      "description": "Training step 2959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:38",
      "total_flops_so_far": 1.758553183420416e+16,
      "budget_used_percent": 17.58553183420416
    },
    {
      "type": "training",
      "description": "Training step 2960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:38",
      "total_flops_so_far": 1.7591472892256256e+16,
      "budget_used_percent": 17.591472892256256
    },
    {
      "type": "training",
      "description": "Training step 2961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:39",
      "total_flops_so_far": 1.7597413950308352e+16,
      "budget_used_percent": 17.59741395030835
    },
    {
      "type": "training",
      "description": "Training step 2962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:39",
      "total_flops_so_far": 1.7603355008360448e+16,
      "budget_used_percent": 17.603355008360445
    },
    {
      "type": "training",
      "description": "Training step 2963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:39",
      "total_flops_so_far": 1.7609296066412544e+16,
      "budget_used_percent": 17.609296066412544
    },
    {
      "type": "training",
      "description": "Training step 2964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:40",
      "total_flops_so_far": 1.761523712446464e+16,
      "budget_used_percent": 17.615237124464638
    },
    {
      "type": "training",
      "description": "Training step 2965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:40",
      "total_flops_so_far": 1.7621178182516736e+16,
      "budget_used_percent": 17.621178182516736
    },
    {
      "type": "training",
      "description": "Training step 2966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:40",
      "total_flops_so_far": 1.7627119240568832e+16,
      "budget_used_percent": 17.62711924056883
    },
    {
      "type": "training",
      "description": "Training step 2967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:41",
      "total_flops_so_far": 1.7633060298620928e+16,
      "budget_used_percent": 17.63306029862093
    },
    {
      "type": "training",
      "description": "Training step 2968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:41",
      "total_flops_so_far": 1.7639001356673024e+16,
      "budget_used_percent": 17.639001356673024
    },
    {
      "type": "training",
      "description": "Training step 2969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:42",
      "total_flops_so_far": 1.764494241472512e+16,
      "budget_used_percent": 17.644942414725122
    },
    {
      "type": "training",
      "description": "Training step 2970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:42",
      "total_flops_so_far": 1.7650883472777216e+16,
      "budget_used_percent": 17.650883472777217
    },
    {
      "type": "training",
      "description": "Training step 2971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:42",
      "total_flops_so_far": 1.7656824530829312e+16,
      "budget_used_percent": 17.65682453082931
    },
    {
      "type": "training",
      "description": "Training step 2972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:43",
      "total_flops_so_far": 1.7662765588881408e+16,
      "budget_used_percent": 17.662765588881406
    },
    {
      "type": "training",
      "description": "Training step 2973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:43",
      "total_flops_so_far": 1.7668706646933504e+16,
      "budget_used_percent": 17.668706646933504
    },
    {
      "type": "training",
      "description": "Training step 2974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:43",
      "total_flops_so_far": 1.76746477049856e+16,
      "budget_used_percent": 17.6746477049856
    },
    {
      "type": "training",
      "description": "Training step 2975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:44",
      "total_flops_so_far": 1.7680588763037696e+16,
      "budget_used_percent": 17.680588763037697
    },
    {
      "type": "training",
      "description": "Training step 2976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:44",
      "total_flops_so_far": 1.7686529821089792e+16,
      "budget_used_percent": 17.686529821089792
    },
    {
      "type": "training",
      "description": "Training step 2977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:45",
      "total_flops_so_far": 1.7692470879141888e+16,
      "budget_used_percent": 17.69247087914189
    },
    {
      "type": "training",
      "description": "Training step 2978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:45",
      "total_flops_so_far": 1.7698411937193984e+16,
      "budget_used_percent": 17.698411937193985
    },
    {
      "type": "training",
      "description": "Training step 2979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:45",
      "total_flops_so_far": 1.770435299524608e+16,
      "budget_used_percent": 17.70435299524608
    },
    {
      "type": "training",
      "description": "Training step 2980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:46",
      "total_flops_so_far": 1.7710294053298176e+16,
      "budget_used_percent": 17.710294053298174
    },
    {
      "type": "training",
      "description": "Training step 2981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:46",
      "total_flops_so_far": 1.7716235111350272e+16,
      "budget_used_percent": 17.716235111350272
    },
    {
      "type": "training",
      "description": "Training step 2982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:46",
      "total_flops_so_far": 1.7722176169402368e+16,
      "budget_used_percent": 17.722176169402367
    },
    {
      "type": "training",
      "description": "Training step 2983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:47",
      "total_flops_so_far": 1.7728117227454464e+16,
      "budget_used_percent": 17.728117227454465
    },
    {
      "type": "training",
      "description": "Training step 2984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:47",
      "total_flops_so_far": 1.773405828550656e+16,
      "budget_used_percent": 17.73405828550656
    },
    {
      "type": "training",
      "description": "Training step 2985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:48",
      "total_flops_so_far": 1.7739999343558656e+16,
      "budget_used_percent": 17.739999343558658
    },
    {
      "type": "training",
      "description": "Training step 2986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:48",
      "total_flops_so_far": 1.7745940401610752e+16,
      "budget_used_percent": 17.745940401610753
    },
    {
      "type": "training",
      "description": "Training step 2987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:48",
      "total_flops_so_far": 1.7751881459662848e+16,
      "budget_used_percent": 17.75188145966285
    },
    {
      "type": "training",
      "description": "Training step 2988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:49",
      "total_flops_so_far": 1.7757822517714944e+16,
      "budget_used_percent": 17.757822517714946
    },
    {
      "type": "training",
      "description": "Training step 2989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:49",
      "total_flops_so_far": 1.776376357576704e+16,
      "budget_used_percent": 17.76376357576704
    },
    {
      "type": "training",
      "description": "Training step 2990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:49",
      "total_flops_so_far": 1.7769704633819136e+16,
      "budget_used_percent": 17.769704633819135
    },
    {
      "type": "training",
      "description": "Training step 2991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:50",
      "total_flops_so_far": 1.7775645691871232e+16,
      "budget_used_percent": 17.77564569187123
    },
    {
      "type": "training",
      "description": "Training step 2992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:50",
      "total_flops_so_far": 1.7781586749923328e+16,
      "budget_used_percent": 17.781586749923328
    },
    {
      "type": "training",
      "description": "Training step 2993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:51",
      "total_flops_so_far": 1.7787527807975424e+16,
      "budget_used_percent": 17.787527807975422
    },
    {
      "type": "training",
      "description": "Training step 2994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:51",
      "total_flops_so_far": 1.779346886602752e+16,
      "budget_used_percent": 17.79346886602752
    },
    {
      "type": "training",
      "description": "Training step 2995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:51",
      "total_flops_so_far": 1.7799409924079616e+16,
      "budget_used_percent": 17.799409924079615
    },
    {
      "type": "training",
      "description": "Training step 2996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:52",
      "total_flops_so_far": 1.7805350982131712e+16,
      "budget_used_percent": 17.805350982131714
    },
    {
      "type": "training",
      "description": "Training step 2997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:52",
      "total_flops_so_far": 1.7811292040183808e+16,
      "budget_used_percent": 17.811292040183808
    },
    {
      "type": "training",
      "description": "Training step 2998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:52",
      "total_flops_so_far": 1.7817233098235904e+16,
      "budget_used_percent": 17.817233098235903
    },
    {
      "type": "training",
      "description": "Training step 2999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:46:53",
      "total_flops_so_far": 1.7823174156288e+16,
      "budget_used_percent": 17.823174156287998
    },
    {
      "type": "training",
      "description": "Training step 3000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:16",
      "total_flops_so_far": 1.7829115214340096e+16,
      "budget_used_percent": 17.829115214340096
    },
    {
      "type": "training",
      "description": "Training step 3001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:16",
      "total_flops_so_far": 1.7835056272392192e+16,
      "budget_used_percent": 17.83505627239219
    },
    {
      "type": "training",
      "description": "Training step 3002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:17",
      "total_flops_so_far": 1.7840997330444288e+16,
      "budget_used_percent": 17.84099733044429
    },
    {
      "type": "training",
      "description": "Training step 3003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:17",
      "total_flops_so_far": 1.7846938388496384e+16,
      "budget_used_percent": 17.846938388496383
    },
    {
      "type": "training",
      "description": "Training step 3004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:17",
      "total_flops_so_far": 1.785287944654848e+16,
      "budget_used_percent": 17.85287944654848
    },
    {
      "type": "training",
      "description": "Training step 3005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:18",
      "total_flops_so_far": 1.7858820504600576e+16,
      "budget_used_percent": 17.858820504600576
    },
    {
      "type": "training",
      "description": "Training step 3006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:18",
      "total_flops_so_far": 1.7864761562652672e+16,
      "budget_used_percent": 17.864761562652674
    },
    {
      "type": "training",
      "description": "Training step 3007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:18",
      "total_flops_so_far": 1.7870702620704768e+16,
      "budget_used_percent": 17.87070262070477
    },
    {
      "type": "training",
      "description": "Training step 3008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:19",
      "total_flops_so_far": 1.7876643678756864e+16,
      "budget_used_percent": 17.876643678756864
    },
    {
      "type": "training",
      "description": "Training step 3009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:19",
      "total_flops_so_far": 1.788258473680896e+16,
      "budget_used_percent": 17.88258473680896
    },
    {
      "type": "training",
      "description": "Training step 3010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:20",
      "total_flops_so_far": 1.7888525794861056e+16,
      "budget_used_percent": 17.888525794861057
    },
    {
      "type": "training",
      "description": "Training step 3011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:20",
      "total_flops_so_far": 1.7894466852913152e+16,
      "budget_used_percent": 17.89446685291315
    },
    {
      "type": "training",
      "description": "Training step 3012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:20",
      "total_flops_so_far": 1.7900407910965248e+16,
      "budget_used_percent": 17.90040791096525
    },
    {
      "type": "training",
      "description": "Training step 3013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:21",
      "total_flops_so_far": 1.7906348969017344e+16,
      "budget_used_percent": 17.906348969017344
    },
    {
      "type": "training",
      "description": "Training step 3014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:21",
      "total_flops_so_far": 1.791229002706944e+16,
      "budget_used_percent": 17.912290027069442
    },
    {
      "type": "training",
      "description": "Training step 3015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:21",
      "total_flops_so_far": 1.7918231085121536e+16,
      "budget_used_percent": 17.918231085121537
    },
    {
      "type": "training",
      "description": "Training step 3016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:22",
      "total_flops_so_far": 1.7924172143173632e+16,
      "budget_used_percent": 17.924172143173635
    },
    {
      "type": "training",
      "description": "Training step 3017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:22",
      "total_flops_so_far": 1.7930113201225728e+16,
      "budget_used_percent": 17.93011320122573
    },
    {
      "type": "training",
      "description": "Training step 3018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:23",
      "total_flops_so_far": 1.7936054259277824e+16,
      "budget_used_percent": 17.936054259277824
    },
    {
      "type": "training",
      "description": "Training step 3019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:23",
      "total_flops_so_far": 1.794199531732992e+16,
      "budget_used_percent": 17.94199531732992
    },
    {
      "type": "training",
      "description": "Training step 3020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:24",
      "total_flops_so_far": 1.7947936375382016e+16,
      "budget_used_percent": 17.947936375382014
    },
    {
      "type": "training",
      "description": "Training step 3021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:24",
      "total_flops_so_far": 1.7953877433434112e+16,
      "budget_used_percent": 17.953877433434112
    },
    {
      "type": "training",
      "description": "Training step 3022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:24",
      "total_flops_so_far": 1.7959818491486208e+16,
      "budget_used_percent": 17.959818491486207
    },
    {
      "type": "training",
      "description": "Training step 3023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:25",
      "total_flops_so_far": 1.7965759549538304e+16,
      "budget_used_percent": 17.965759549538305
    },
    {
      "type": "training",
      "description": "Training step 3024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:25",
      "total_flops_so_far": 1.79717006075904e+16,
      "budget_used_percent": 17.9717006075904
    },
    {
      "type": "training",
      "description": "Training step 3025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:26",
      "total_flops_so_far": 1.7977641665642496e+16,
      "budget_used_percent": 17.977641665642498
    },
    {
      "type": "training",
      "description": "Training step 3026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:26",
      "total_flops_so_far": 1.7983582723694592e+16,
      "budget_used_percent": 17.983582723694592
    },
    {
      "type": "training",
      "description": "Training step 3027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:26",
      "total_flops_so_far": 1.7989523781746688e+16,
      "budget_used_percent": 17.989523781746687
    },
    {
      "type": "training",
      "description": "Training step 3028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:27",
      "total_flops_so_far": 1.7995464839798784e+16,
      "budget_used_percent": 17.99546483979878
    },
    {
      "type": "training",
      "description": "Training step 3029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:27",
      "total_flops_so_far": 1.800140589785088e+16,
      "budget_used_percent": 18.00140589785088
    },
    {
      "type": "training",
      "description": "Training step 3030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:27",
      "total_flops_so_far": 1.8007346955902976e+16,
      "budget_used_percent": 18.007346955902975
    },
    {
      "type": "training",
      "description": "Training step 3031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:28",
      "total_flops_so_far": 1.8013288013955072e+16,
      "budget_used_percent": 18.013288013955073
    },
    {
      "type": "training",
      "description": "Training step 3032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:28",
      "total_flops_so_far": 1.801922907200717e+16,
      "budget_used_percent": 18.019229072007168
    },
    {
      "type": "training",
      "description": "Training step 3033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:29",
      "total_flops_so_far": 1.8025170130059264e+16,
      "budget_used_percent": 18.025170130059266
    },
    {
      "type": "training",
      "description": "Training step 3034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:29",
      "total_flops_so_far": 1.803111118811136e+16,
      "budget_used_percent": 18.03111118811136
    },
    {
      "type": "training",
      "description": "Training step 3035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:29",
      "total_flops_so_far": 1.8037052246163456e+16,
      "budget_used_percent": 18.03705224616346
    },
    {
      "type": "training",
      "description": "Training step 3036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:30",
      "total_flops_so_far": 1.804299330421555e+16,
      "budget_used_percent": 18.042993304215553
    },
    {
      "type": "training",
      "description": "Training step 3037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:30",
      "total_flops_so_far": 1.804893436226765e+16,
      "budget_used_percent": 18.048934362267648
    },
    {
      "type": "training",
      "description": "Training step 3038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:30",
      "total_flops_so_far": 1.8054875420319744e+16,
      "budget_used_percent": 18.054875420319743
    },
    {
      "type": "training",
      "description": "Training step 3039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:31",
      "total_flops_so_far": 1.806081647837184e+16,
      "budget_used_percent": 18.06081647837184
    },
    {
      "type": "training",
      "description": "Training step 3040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:31",
      "total_flops_so_far": 1.8066757536423936e+16,
      "budget_used_percent": 18.066757536423935
    },
    {
      "type": "training",
      "description": "Training step 3041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:32",
      "total_flops_so_far": 1.807269859447603e+16,
      "budget_used_percent": 18.072698594476034
    },
    {
      "type": "training",
      "description": "Training step 3042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:32",
      "total_flops_so_far": 1.807863965252813e+16,
      "budget_used_percent": 18.07863965252813
    },
    {
      "type": "training",
      "description": "Training step 3043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:32",
      "total_flops_so_far": 1.8084580710580224e+16,
      "budget_used_percent": 18.084580710580227
    },
    {
      "type": "training",
      "description": "Training step 3044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:33",
      "total_flops_so_far": 1.809052176863232e+16,
      "budget_used_percent": 18.09052176863232
    },
    {
      "type": "training",
      "description": "Training step 3045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:33",
      "total_flops_so_far": 1.8096462826684416e+16,
      "budget_used_percent": 18.096462826684416
    },
    {
      "type": "training",
      "description": "Training step 3046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:33",
      "total_flops_so_far": 1.810240388473651e+16,
      "budget_used_percent": 18.10240388473651
    },
    {
      "type": "training",
      "description": "Training step 3047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:34",
      "total_flops_so_far": 1.810834494278861e+16,
      "budget_used_percent": 18.108344942788605
    },
    {
      "type": "training",
      "description": "Training step 3048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:34",
      "total_flops_so_far": 1.8114286000840704e+16,
      "budget_used_percent": 18.114286000840703
    },
    {
      "type": "training",
      "description": "Training step 3049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:35",
      "total_flops_so_far": 1.81202270588928e+16,
      "budget_used_percent": 18.120227058892798
    },
    {
      "type": "training",
      "description": "Training step 3050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:35",
      "total_flops_so_far": 1.8126168116944896e+16,
      "budget_used_percent": 18.126168116944896
    },
    {
      "type": "training",
      "description": "Training step 3051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:35",
      "total_flops_so_far": 1.813210917499699e+16,
      "budget_used_percent": 18.13210917499699
    },
    {
      "type": "training",
      "description": "Training step 3052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:36",
      "total_flops_so_far": 1.813805023304909e+16,
      "budget_used_percent": 18.13805023304909
    },
    {
      "type": "training",
      "description": "Training step 3053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:36",
      "total_flops_so_far": 1.8143991291101184e+16,
      "budget_used_percent": 18.143991291101184
    },
    {
      "type": "training",
      "description": "Training step 3054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:36",
      "total_flops_so_far": 1.814993234915328e+16,
      "budget_used_percent": 18.149932349153282
    },
    {
      "type": "training",
      "description": "Training step 3055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:37",
      "total_flops_so_far": 1.8155873407205376e+16,
      "budget_used_percent": 18.155873407205377
    },
    {
      "type": "training",
      "description": "Training step 3056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:37",
      "total_flops_so_far": 1.816181446525747e+16,
      "budget_used_percent": 18.16181446525747
    },
    {
      "type": "training",
      "description": "Training step 3057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:38",
      "total_flops_so_far": 1.816775552330957e+16,
      "budget_used_percent": 18.167755523309566
    },
    {
      "type": "training",
      "description": "Training step 3058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:38",
      "total_flops_so_far": 1.8173696581361664e+16,
      "budget_used_percent": 18.173696581361664
    },
    {
      "type": "training",
      "description": "Training step 3059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:38",
      "total_flops_so_far": 1.817963763941376e+16,
      "budget_used_percent": 18.17963763941376
    },
    {
      "type": "training",
      "description": "Training step 3060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:39",
      "total_flops_so_far": 1.8185578697465856e+16,
      "budget_used_percent": 18.185578697465857
    },
    {
      "type": "training",
      "description": "Training step 3061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:39",
      "total_flops_so_far": 1.819151975551795e+16,
      "budget_used_percent": 18.19151975551795
    },
    {
      "type": "training",
      "description": "Training step 3062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:39",
      "total_flops_so_far": 1.819746081357005e+16,
      "budget_used_percent": 18.19746081357005
    },
    {
      "type": "training",
      "description": "Training step 3063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:40",
      "total_flops_so_far": 1.8203401871622144e+16,
      "budget_used_percent": 18.203401871622145
    },
    {
      "type": "training",
      "description": "Training step 3064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:40",
      "total_flops_so_far": 1.820934292967424e+16,
      "budget_used_percent": 18.20934292967424
    },
    {
      "type": "training",
      "description": "Training step 3065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:41",
      "total_flops_so_far": 1.8215283987726336e+16,
      "budget_used_percent": 18.215283987726334
    },
    {
      "type": "training",
      "description": "Training step 3066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:41",
      "total_flops_so_far": 1.822122504577843e+16,
      "budget_used_percent": 18.221225045778432
    },
    {
      "type": "training",
      "description": "Training step 3067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:41",
      "total_flops_so_far": 1.822716610383053e+16,
      "budget_used_percent": 18.227166103830527
    },
    {
      "type": "training",
      "description": "Training step 3068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:42",
      "total_flops_so_far": 1.8233107161882624e+16,
      "budget_used_percent": 18.233107161882625
    },
    {
      "type": "training",
      "description": "Training step 3069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:42",
      "total_flops_so_far": 1.823904821993472e+16,
      "budget_used_percent": 18.23904821993472
    },
    {
      "type": "training",
      "description": "Training step 3070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:42",
      "total_flops_so_far": 1.8244989277986816e+16,
      "budget_used_percent": 18.244989277986818
    },
    {
      "type": "training",
      "description": "Training step 3071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:43",
      "total_flops_so_far": 1.825093033603891e+16,
      "budget_used_percent": 18.250930336038913
    },
    {
      "type": "training",
      "description": "Training step 3072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:43",
      "total_flops_so_far": 1.825687139409101e+16,
      "budget_used_percent": 18.25687139409101
    },
    {
      "type": "training",
      "description": "Training step 3073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:44",
      "total_flops_so_far": 1.8262812452143104e+16,
      "budget_used_percent": 18.262812452143105
    },
    {
      "type": "training",
      "description": "Training step 3074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:44",
      "total_flops_so_far": 1.82687535101952e+16,
      "budget_used_percent": 18.2687535101952
    },
    {
      "type": "training",
      "description": "Training step 3075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:44",
      "total_flops_so_far": 1.8274694568247296e+16,
      "budget_used_percent": 18.274694568247295
    },
    {
      "type": "training",
      "description": "Training step 3076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:45",
      "total_flops_so_far": 1.828063562629939e+16,
      "budget_used_percent": 18.28063562629939
    },
    {
      "type": "training",
      "description": "Training step 3077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:45",
      "total_flops_so_far": 1.828657668435149e+16,
      "budget_used_percent": 18.286576684351488
    },
    {
      "type": "training",
      "description": "Training step 3078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:45",
      "total_flops_so_far": 1.8292517742403584e+16,
      "budget_used_percent": 18.292517742403582
    },
    {
      "type": "training",
      "description": "Training step 3079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:46",
      "total_flops_so_far": 1.829845880045568e+16,
      "budget_used_percent": 18.29845880045568
    },
    {
      "type": "training",
      "description": "Training step 3080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:46",
      "total_flops_so_far": 1.8304399858507776e+16,
      "budget_used_percent": 18.304399858507775
    },
    {
      "type": "training",
      "description": "Training step 3081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:47",
      "total_flops_so_far": 1.831034091655987e+16,
      "budget_used_percent": 18.310340916559873
    },
    {
      "type": "training",
      "description": "Training step 3082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:47",
      "total_flops_so_far": 1.831628197461197e+16,
      "budget_used_percent": 18.316281974611968
    },
    {
      "type": "training",
      "description": "Training step 3083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:47",
      "total_flops_so_far": 1.8322223032664064e+16,
      "budget_used_percent": 18.322223032664063
    },
    {
      "type": "training",
      "description": "Training step 3084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:48",
      "total_flops_so_far": 1.832816409071616e+16,
      "budget_used_percent": 18.328164090716157
    },
    {
      "type": "training",
      "description": "Training step 3085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:48",
      "total_flops_so_far": 1.8334105148768256e+16,
      "budget_used_percent": 18.334105148768256
    },
    {
      "type": "training",
      "description": "Training step 3086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:48",
      "total_flops_so_far": 1.834004620682035e+16,
      "budget_used_percent": 18.34004620682035
    },
    {
      "type": "training",
      "description": "Training step 3087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:49",
      "total_flops_so_far": 1.834598726487245e+16,
      "budget_used_percent": 18.34598726487245
    },
    {
      "type": "training",
      "description": "Training step 3088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:49",
      "total_flops_so_far": 1.8351928322924544e+16,
      "budget_used_percent": 18.351928322924543
    },
    {
      "type": "training",
      "description": "Training step 3089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:50",
      "total_flops_so_far": 1.835786938097664e+16,
      "budget_used_percent": 18.35786938097664
    },
    {
      "type": "training",
      "description": "Training step 3090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:50",
      "total_flops_so_far": 1.8363810439028736e+16,
      "budget_used_percent": 18.363810439028736
    },
    {
      "type": "training",
      "description": "Training step 3091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:50",
      "total_flops_so_far": 1.836975149708083e+16,
      "budget_used_percent": 18.369751497080834
    },
    {
      "type": "training",
      "description": "Training step 3092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:51",
      "total_flops_so_far": 1.837569255513293e+16,
      "budget_used_percent": 18.37569255513293
    },
    {
      "type": "training",
      "description": "Training step 3093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:51",
      "total_flops_so_far": 1.8381633613185024e+16,
      "budget_used_percent": 18.381633613185024
    },
    {
      "type": "training",
      "description": "Training step 3094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:51",
      "total_flops_so_far": 1.838757467123712e+16,
      "budget_used_percent": 18.387574671237118
    },
    {
      "type": "training",
      "description": "Training step 3095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:52",
      "total_flops_so_far": 1.8393515729289216e+16,
      "budget_used_percent": 18.393515729289216
    },
    {
      "type": "training",
      "description": "Training step 3096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:52",
      "total_flops_so_far": 1.839945678734131e+16,
      "budget_used_percent": 18.39945678734131
    },
    {
      "type": "training",
      "description": "Training step 3097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:53",
      "total_flops_so_far": 1.840539784539341e+16,
      "budget_used_percent": 18.40539784539341
    },
    {
      "type": "training",
      "description": "Training step 3098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:53",
      "total_flops_so_far": 1.8411338903445504e+16,
      "budget_used_percent": 18.411338903445504
    },
    {
      "type": "training",
      "description": "Training step 3099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:53",
      "total_flops_so_far": 1.84172799614976e+16,
      "budget_used_percent": 18.417279961497602
    },
    {
      "type": "training",
      "description": "Training step 3100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:54",
      "total_flops_so_far": 1.8423221019549696e+16,
      "budget_used_percent": 18.423221019549697
    },
    {
      "type": "training",
      "description": "Training step 3101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:54",
      "total_flops_so_far": 1.842916207760179e+16,
      "budget_used_percent": 18.429162077601795
    },
    {
      "type": "training",
      "description": "Training step 3102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:54",
      "total_flops_so_far": 1.843510313565389e+16,
      "budget_used_percent": 18.43510313565389
    },
    {
      "type": "training",
      "description": "Training step 3103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:55",
      "total_flops_so_far": 1.8441044193705984e+16,
      "budget_used_percent": 18.441044193705984
    },
    {
      "type": "training",
      "description": "Training step 3104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:55",
      "total_flops_so_far": 1.844698525175808e+16,
      "budget_used_percent": 18.44698525175808
    },
    {
      "type": "training",
      "description": "Training step 3105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:56",
      "total_flops_so_far": 1.8452926309810176e+16,
      "budget_used_percent": 18.452926309810174
    },
    {
      "type": "training",
      "description": "Training step 3106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:56",
      "total_flops_so_far": 1.845886736786227e+16,
      "budget_used_percent": 18.458867367862272
    },
    {
      "type": "training",
      "description": "Training step 3107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:56",
      "total_flops_so_far": 1.846480842591437e+16,
      "budget_used_percent": 18.464808425914367
    },
    {
      "type": "training",
      "description": "Training step 3108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:57",
      "total_flops_so_far": 1.8470749483966464e+16,
      "budget_used_percent": 18.470749483966465
    },
    {
      "type": "training",
      "description": "Training step 3109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:57",
      "total_flops_so_far": 1.847669054201856e+16,
      "budget_used_percent": 18.47669054201856
    },
    {
      "type": "training",
      "description": "Training step 3110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:58",
      "total_flops_so_far": 1.8482631600070656e+16,
      "budget_used_percent": 18.482631600070658
    },
    {
      "type": "training",
      "description": "Training step 3111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:58",
      "total_flops_so_far": 1.848857265812275e+16,
      "budget_used_percent": 18.488572658122752
    },
    {
      "type": "training",
      "description": "Training step 3112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:58",
      "total_flops_so_far": 1.849451371617485e+16,
      "budget_used_percent": 18.494513716174847
    },
    {
      "type": "training",
      "description": "Training step 3113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:59",
      "total_flops_so_far": 1.8500454774226944e+16,
      "budget_used_percent": 18.50045477422694
    },
    {
      "type": "training",
      "description": "Training step 3114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:59",
      "total_flops_so_far": 1.850639583227904e+16,
      "budget_used_percent": 18.50639583227904
    },
    {
      "type": "training",
      "description": "Training step 3115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:49:59",
      "total_flops_so_far": 1.8512336890331136e+16,
      "budget_used_percent": 18.512336890331134
    },
    {
      "type": "training",
      "description": "Training step 3116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:00",
      "total_flops_so_far": 1.851827794838323e+16,
      "budget_used_percent": 18.518277948383233
    },
    {
      "type": "training",
      "description": "Training step 3117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:00",
      "total_flops_so_far": 1.852421900643533e+16,
      "budget_used_percent": 18.524219006435327
    },
    {
      "type": "training",
      "description": "Training step 3118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:01",
      "total_flops_so_far": 1.8530160064487424e+16,
      "budget_used_percent": 18.530160064487426
    },
    {
      "type": "training",
      "description": "Training step 3119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:01",
      "total_flops_so_far": 1.853610112253952e+16,
      "budget_used_percent": 18.53610112253952
    },
    {
      "type": "training",
      "description": "Training step 3120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:02",
      "total_flops_so_far": 1.8542042180591616e+16,
      "budget_used_percent": 18.54204218059162
    },
    {
      "type": "training",
      "description": "Training step 3121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:02",
      "total_flops_so_far": 1.854798323864371e+16,
      "budget_used_percent": 18.547983238643713
    },
    {
      "type": "training",
      "description": "Training step 3122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:02",
      "total_flops_so_far": 1.855392429669581e+16,
      "budget_used_percent": 18.553924296695808
    },
    {
      "type": "training",
      "description": "Training step 3123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:03",
      "total_flops_so_far": 1.8559865354747904e+16,
      "budget_used_percent": 18.559865354747902
    },
    {
      "type": "training",
      "description": "Training step 3124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:03",
      "total_flops_so_far": 1.85658064128e+16,
      "budget_used_percent": 18.5658064128
    },
    {
      "type": "training",
      "description": "Training step 3125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:03",
      "total_flops_so_far": 1.8571747470852096e+16,
      "budget_used_percent": 18.571747470852095
    },
    {
      "type": "training",
      "description": "Training step 3126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:04",
      "total_flops_so_far": 1.857768852890419e+16,
      "budget_used_percent": 18.577688528904194
    },
    {
      "type": "training",
      "description": "Training step 3127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:04",
      "total_flops_so_far": 1.858362958695629e+16,
      "budget_used_percent": 18.583629586956288
    },
    {
      "type": "training",
      "description": "Training step 3128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:05",
      "total_flops_so_far": 1.8589570645008384e+16,
      "budget_used_percent": 18.589570645008386
    },
    {
      "type": "training",
      "description": "Training step 3129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:05",
      "total_flops_so_far": 1.859551170306048e+16,
      "budget_used_percent": 18.59551170306048
    },
    {
      "type": "training",
      "description": "Training step 3130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:05",
      "total_flops_so_far": 1.8601452761112576e+16,
      "budget_used_percent": 18.601452761112576
    },
    {
      "type": "training",
      "description": "Training step 3131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:06",
      "total_flops_so_far": 1.860739381916467e+16,
      "budget_used_percent": 18.60739381916467
    },
    {
      "type": "training",
      "description": "Training step 3132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:06",
      "total_flops_so_far": 1.861333487721677e+16,
      "budget_used_percent": 18.613334877216765
    },
    {
      "type": "training",
      "description": "Training step 3133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:06",
      "total_flops_so_far": 1.8619275935268864e+16,
      "budget_used_percent": 18.619275935268863
    },
    {
      "type": "training",
      "description": "Training step 3134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:07",
      "total_flops_so_far": 1.862521699332096e+16,
      "budget_used_percent": 18.625216993320958
    },
    {
      "type": "training",
      "description": "Training step 3135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:07",
      "total_flops_so_far": 1.8631158051373056e+16,
      "budget_used_percent": 18.631158051373056
    },
    {
      "type": "training",
      "description": "Training step 3136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:08",
      "total_flops_so_far": 1.863709910942515e+16,
      "budget_used_percent": 18.63709910942515
    },
    {
      "type": "training",
      "description": "Training step 3137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:08",
      "total_flops_so_far": 1.864304016747725e+16,
      "budget_used_percent": 18.64304016747725
    },
    {
      "type": "training",
      "description": "Training step 3138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:08",
      "total_flops_so_far": 1.8648981225529344e+16,
      "budget_used_percent": 18.648981225529344
    },
    {
      "type": "training",
      "description": "Training step 3139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:09",
      "total_flops_so_far": 1.865492228358144e+16,
      "budget_used_percent": 18.654922283581442
    },
    {
      "type": "training",
      "description": "Training step 3140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:09",
      "total_flops_so_far": 1.8660863341633536e+16,
      "budget_used_percent": 18.660863341633537
    },
    {
      "type": "training",
      "description": "Training step 3141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:10",
      "total_flops_so_far": 1.866680439968563e+16,
      "budget_used_percent": 18.66680439968563
    },
    {
      "type": "training",
      "description": "Training step 3142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:10",
      "total_flops_so_far": 1.867274545773773e+16,
      "budget_used_percent": 18.672745457737726
    },
    {
      "type": "training",
      "description": "Training step 3143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:10",
      "total_flops_so_far": 1.8678686515789824e+16,
      "budget_used_percent": 18.678686515789824
    },
    {
      "type": "training",
      "description": "Training step 3144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:11",
      "total_flops_so_far": 1.868462757384192e+16,
      "budget_used_percent": 18.68462757384192
    },
    {
      "type": "training",
      "description": "Training step 3145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:11",
      "total_flops_so_far": 1.8690568631894016e+16,
      "budget_used_percent": 18.690568631894017
    },
    {
      "type": "training",
      "description": "Training step 3146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:11",
      "total_flops_so_far": 1.869650968994611e+16,
      "budget_used_percent": 18.69650968994611
    },
    {
      "type": "training",
      "description": "Training step 3147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:12",
      "total_flops_so_far": 1.870245074799821e+16,
      "budget_used_percent": 18.70245074799821
    },
    {
      "type": "training",
      "description": "Training step 3148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:12",
      "total_flops_so_far": 1.8708391806050304e+16,
      "budget_used_percent": 18.708391806050304
    },
    {
      "type": "training",
      "description": "Training step 3149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:13",
      "total_flops_so_far": 1.87143328641024e+16,
      "budget_used_percent": 18.714332864102403
    },
    {
      "type": "training",
      "description": "Training step 3150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:13",
      "total_flops_so_far": 1.8720273922154496e+16,
      "budget_used_percent": 18.720273922154497
    },
    {
      "type": "training",
      "description": "Training step 3151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:13",
      "total_flops_so_far": 1.872621498020659e+16,
      "budget_used_percent": 18.726214980206592
    },
    {
      "type": "training",
      "description": "Training step 3152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:14",
      "total_flops_so_far": 1.873215603825869e+16,
      "budget_used_percent": 18.732156038258687
    },
    {
      "type": "training",
      "description": "Training step 3153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:14",
      "total_flops_so_far": 1.8738097096310784e+16,
      "budget_used_percent": 18.738097096310785
    },
    {
      "type": "training",
      "description": "Training step 3154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:14",
      "total_flops_so_far": 1.874403815436288e+16,
      "budget_used_percent": 18.74403815436288
    },
    {
      "type": "training",
      "description": "Training step 3155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:15",
      "total_flops_so_far": 1.8749979212414976e+16,
      "budget_used_percent": 18.749979212414978
    },
    {
      "type": "training",
      "description": "Training step 3156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:15",
      "total_flops_so_far": 1.875592027046707e+16,
      "budget_used_percent": 18.755920270467072
    },
    {
      "type": "training",
      "description": "Training step 3157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:16",
      "total_flops_so_far": 1.876186132851917e+16,
      "budget_used_percent": 18.76186132851917
    },
    {
      "type": "training",
      "description": "Training step 3158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:16",
      "total_flops_so_far": 1.8767802386571264e+16,
      "budget_used_percent": 18.767802386571265
    },
    {
      "type": "training",
      "description": "Training step 3159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:16",
      "total_flops_so_far": 1.877374344462336e+16,
      "budget_used_percent": 18.77374344462336
    },
    {
      "type": "training",
      "description": "Training step 3160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:17",
      "total_flops_so_far": 1.8779684502675456e+16,
      "budget_used_percent": 18.779684502675455
    },
    {
      "type": "training",
      "description": "Training step 3161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:17",
      "total_flops_so_far": 1.878562556072755e+16,
      "budget_used_percent": 18.78562556072755
    },
    {
      "type": "training",
      "description": "Training step 3162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:17",
      "total_flops_so_far": 1.879156661877965e+16,
      "budget_used_percent": 18.791566618779648
    },
    {
      "type": "training",
      "description": "Training step 3163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:18",
      "total_flops_so_far": 1.8797507676831744e+16,
      "budget_used_percent": 18.797507676831742
    },
    {
      "type": "training",
      "description": "Training step 3164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:18",
      "total_flops_so_far": 1.880344873488384e+16,
      "budget_used_percent": 18.80344873488384
    },
    {
      "type": "training",
      "description": "Training step 3165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:18",
      "total_flops_so_far": 1.8809389792935936e+16,
      "budget_used_percent": 18.809389792935935
    },
    {
      "type": "training",
      "description": "Training step 3166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:19",
      "total_flops_so_far": 1.881533085098803e+16,
      "budget_used_percent": 18.815330850988033
    },
    {
      "type": "training",
      "description": "Training step 3167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:19",
      "total_flops_so_far": 1.882127190904013e+16,
      "budget_used_percent": 18.821271909040128
    },
    {
      "type": "training",
      "description": "Training step 3168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:20",
      "total_flops_so_far": 1.8827212967092224e+16,
      "budget_used_percent": 18.827212967092226
    },
    {
      "type": "training",
      "description": "Training step 3169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:20",
      "total_flops_so_far": 1.883315402514432e+16,
      "budget_used_percent": 18.83315402514432
    },
    {
      "type": "training",
      "description": "Training step 3170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:20",
      "total_flops_so_far": 1.8839095083196416e+16,
      "budget_used_percent": 18.839095083196415
    },
    {
      "type": "training",
      "description": "Training step 3171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:21",
      "total_flops_so_far": 1.884503614124851e+16,
      "budget_used_percent": 18.84503614124851
    },
    {
      "type": "training",
      "description": "Training step 3172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:21",
      "total_flops_so_far": 1.885097719930061e+16,
      "budget_used_percent": 18.85097719930061
    },
    {
      "type": "training",
      "description": "Training step 3173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:22",
      "total_flops_so_far": 1.8856918257352704e+16,
      "budget_used_percent": 18.856918257352703
    },
    {
      "type": "training",
      "description": "Training step 3174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:22",
      "total_flops_so_far": 1.88628593154048e+16,
      "budget_used_percent": 18.8628593154048
    },
    {
      "type": "training",
      "description": "Training step 3175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:22",
      "total_flops_so_far": 1.8868800373456896e+16,
      "budget_used_percent": 18.868800373456896
    },
    {
      "type": "training",
      "description": "Training step 3176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:23",
      "total_flops_so_far": 1.887474143150899e+16,
      "budget_used_percent": 18.874741431508994
    },
    {
      "type": "training",
      "description": "Training step 3177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:23",
      "total_flops_so_far": 1.888068248956109e+16,
      "budget_used_percent": 18.88068248956109
    },
    {
      "type": "training",
      "description": "Training step 3178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:23",
      "total_flops_so_far": 1.8886623547613184e+16,
      "budget_used_percent": 18.886623547613183
    },
    {
      "type": "training",
      "description": "Training step 3179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:24",
      "total_flops_so_far": 1.889256460566528e+16,
      "budget_used_percent": 18.892564605665278
    },
    {
      "type": "training",
      "description": "Training step 3180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:24",
      "total_flops_so_far": 1.8898505663717376e+16,
      "budget_used_percent": 18.898505663717376
    },
    {
      "type": "training",
      "description": "Training step 3181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:25",
      "total_flops_so_far": 1.890444672176947e+16,
      "budget_used_percent": 18.90444672176947
    },
    {
      "type": "training",
      "description": "Training step 3182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:25",
      "total_flops_so_far": 1.891038777982157e+16,
      "budget_used_percent": 18.91038777982157
    },
    {
      "type": "training",
      "description": "Training step 3183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:25",
      "total_flops_so_far": 1.8916328837873664e+16,
      "budget_used_percent": 18.916328837873664
    },
    {
      "type": "training",
      "description": "Training step 3184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:26",
      "total_flops_so_far": 1.892226989592576e+16,
      "budget_used_percent": 18.922269895925762
    },
    {
      "type": "training",
      "description": "Training step 3185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:26",
      "total_flops_so_far": 1.8928210953977856e+16,
      "budget_used_percent": 18.928210953977857
    },
    {
      "type": "training",
      "description": "Training step 3186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:26",
      "total_flops_so_far": 1.893415201202995e+16,
      "budget_used_percent": 18.934152012029955
    },
    {
      "type": "training",
      "description": "Training step 3187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:27",
      "total_flops_so_far": 1.894009307008205e+16,
      "budget_used_percent": 18.94009307008205
    },
    {
      "type": "training",
      "description": "Training step 3188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:27",
      "total_flops_so_far": 1.8946034128134144e+16,
      "budget_used_percent": 18.946034128134144
    },
    {
      "type": "training",
      "description": "Training step 3189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:28",
      "total_flops_so_far": 1.895197518618624e+16,
      "budget_used_percent": 18.95197518618624
    },
    {
      "type": "training",
      "description": "Training step 3190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:28",
      "total_flops_so_far": 1.8957916244238336e+16,
      "budget_used_percent": 18.957916244238334
    },
    {
      "type": "training",
      "description": "Training step 3191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:28",
      "total_flops_so_far": 1.896385730229043e+16,
      "budget_used_percent": 18.96385730229043
    },
    {
      "type": "training",
      "description": "Training step 3192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:29",
      "total_flops_so_far": 1.896979836034253e+16,
      "budget_used_percent": 18.969798360342526
    },
    {
      "type": "training",
      "description": "Training step 3193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:29",
      "total_flops_so_far": 1.8975739418394624e+16,
      "budget_used_percent": 18.975739418394625
    },
    {
      "type": "training",
      "description": "Training step 3194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:29",
      "total_flops_so_far": 1.898168047644672e+16,
      "budget_used_percent": 18.98168047644672
    },
    {
      "type": "training",
      "description": "Training step 3195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:30",
      "total_flops_so_far": 1.8987621534498816e+16,
      "budget_used_percent": 18.987621534498818
    },
    {
      "type": "training",
      "description": "Training step 3196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:31",
      "total_flops_so_far": 1.899356259255091e+16,
      "budget_used_percent": 18.993562592550912
    },
    {
      "type": "training",
      "description": "Training step 3197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:31",
      "total_flops_so_far": 1.899950365060301e+16,
      "budget_used_percent": 18.999503650603007
    },
    {
      "type": "training",
      "description": "Training step 3198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:31",
      "total_flops_so_far": 1.9005444708655104e+16,
      "budget_used_percent": 19.0054447086551
    },
    {
      "type": "training",
      "description": "Training step 3199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:32",
      "total_flops_so_far": 1.90113857667072e+16,
      "budget_used_percent": 19.0113857667072
    },
    {
      "type": "training",
      "description": "Training step 3200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:32",
      "total_flops_so_far": 1.9017326824759296e+16,
      "budget_used_percent": 19.017326824759294
    },
    {
      "type": "training",
      "description": "Training step 3201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:32",
      "total_flops_so_far": 1.902326788281139e+16,
      "budget_used_percent": 19.023267882811393
    },
    {
      "type": "training",
      "description": "Training step 3202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:33",
      "total_flops_so_far": 1.902920894086349e+16,
      "budget_used_percent": 19.029208940863487
    },
    {
      "type": "training",
      "description": "Training step 3203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:33",
      "total_flops_so_far": 1.9035149998915584e+16,
      "budget_used_percent": 19.035149998915585
    },
    {
      "type": "training",
      "description": "Training step 3204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:34",
      "total_flops_so_far": 1.904109105696768e+16,
      "budget_used_percent": 19.04109105696768
    },
    {
      "type": "training",
      "description": "Training step 3205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:34",
      "total_flops_so_far": 1.9047032115019776e+16,
      "budget_used_percent": 19.04703211501978
    },
    {
      "type": "training",
      "description": "Training step 3206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:34",
      "total_flops_so_far": 1.905297317307187e+16,
      "budget_used_percent": 19.052973173071873
    },
    {
      "type": "training",
      "description": "Training step 3207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:35",
      "total_flops_so_far": 1.905891423112397e+16,
      "budget_used_percent": 19.058914231123968
    },
    {
      "type": "training",
      "description": "Training step 3208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:35",
      "total_flops_so_far": 1.9064855289176064e+16,
      "budget_used_percent": 19.064855289176062
    },
    {
      "type": "training",
      "description": "Training step 3209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:35",
      "total_flops_so_far": 1.907079634722816e+16,
      "budget_used_percent": 19.07079634722816
    },
    {
      "type": "training",
      "description": "Training step 3210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:36",
      "total_flops_so_far": 1.9076737405280256e+16,
      "budget_used_percent": 19.076737405280255
    },
    {
      "type": "training",
      "description": "Training step 3211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:36",
      "total_flops_so_far": 1.908267846333235e+16,
      "budget_used_percent": 19.082678463332353
    },
    {
      "type": "training",
      "description": "Training step 3212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:37",
      "total_flops_so_far": 1.908861952138445e+16,
      "budget_used_percent": 19.088619521384448
    },
    {
      "type": "training",
      "description": "Training step 3213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:37",
      "total_flops_so_far": 1.9094560579436544e+16,
      "budget_used_percent": 19.094560579436546
    },
    {
      "type": "training",
      "description": "Training step 3214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:37",
      "total_flops_so_far": 1.910050163748864e+16,
      "budget_used_percent": 19.10050163748864
    },
    {
      "type": "training",
      "description": "Training step 3215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:38",
      "total_flops_so_far": 1.9106442695540736e+16,
      "budget_used_percent": 19.106442695540736
    },
    {
      "type": "training",
      "description": "Training step 3216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:38",
      "total_flops_so_far": 1.911238375359283e+16,
      "budget_used_percent": 19.11238375359283
    },
    {
      "type": "training",
      "description": "Training step 3217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:39",
      "total_flops_so_far": 1.911832481164493e+16,
      "budget_used_percent": 19.118324811644925
    },
    {
      "type": "training",
      "description": "Training step 3218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:39",
      "total_flops_so_far": 1.9124265869697024e+16,
      "budget_used_percent": 19.124265869697023
    },
    {
      "type": "training",
      "description": "Training step 3219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:39",
      "total_flops_so_far": 1.913020692774912e+16,
      "budget_used_percent": 19.130206927749118
    },
    {
      "type": "training",
      "description": "Training step 3220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:40",
      "total_flops_so_far": 1.9136147985801216e+16,
      "budget_used_percent": 19.136147985801216
    },
    {
      "type": "training",
      "description": "Training step 3221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:40",
      "total_flops_so_far": 1.914208904385331e+16,
      "budget_used_percent": 19.14208904385331
    },
    {
      "type": "training",
      "description": "Training step 3222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:40",
      "total_flops_so_far": 1.914803010190541e+16,
      "budget_used_percent": 19.14803010190541
    },
    {
      "type": "training",
      "description": "Training step 3223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:41",
      "total_flops_so_far": 1.9153971159957504e+16,
      "budget_used_percent": 19.153971159957504
    },
    {
      "type": "training",
      "description": "Training step 3224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:41",
      "total_flops_so_far": 1.91599122180096e+16,
      "budget_used_percent": 19.1599122180096
    },
    {
      "type": "training",
      "description": "Training step 3225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:42",
      "total_flops_so_far": 1.9165853276061696e+16,
      "budget_used_percent": 19.165853276061696
    },
    {
      "type": "training",
      "description": "Training step 3226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:42",
      "total_flops_so_far": 1.917179433411379e+16,
      "budget_used_percent": 19.17179433411379
    },
    {
      "type": "training",
      "description": "Training step 3227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:42",
      "total_flops_so_far": 1.917773539216589e+16,
      "budget_used_percent": 19.177735392165886
    },
    {
      "type": "training",
      "description": "Training step 3228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:43",
      "total_flops_so_far": 1.9183676450217984e+16,
      "budget_used_percent": 19.183676450217984
    },
    {
      "type": "training",
      "description": "Training step 3229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:43",
      "total_flops_so_far": 1.918961750827008e+16,
      "budget_used_percent": 19.18961750827008
    },
    {
      "type": "training",
      "description": "Training step 3230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:43",
      "total_flops_so_far": 1.9195558566322176e+16,
      "budget_used_percent": 19.195558566322177
    },
    {
      "type": "training",
      "description": "Training step 3231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:44",
      "total_flops_so_far": 1.920149962437427e+16,
      "budget_used_percent": 19.20149962437427
    },
    {
      "type": "training",
      "description": "Training step 3232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:44",
      "total_flops_so_far": 1.920744068242637e+16,
      "budget_used_percent": 19.20744068242637
    },
    {
      "type": "training",
      "description": "Training step 3233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:45",
      "total_flops_so_far": 1.9213381740478464e+16,
      "budget_used_percent": 19.213381740478464
    },
    {
      "type": "training",
      "description": "Training step 3234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:45",
      "total_flops_so_far": 1.921932279853056e+16,
      "budget_used_percent": 19.219322798530563
    },
    {
      "type": "training",
      "description": "Training step 3235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:45",
      "total_flops_so_far": 1.9225263856582656e+16,
      "budget_used_percent": 19.225263856582657
    },
    {
      "type": "training",
      "description": "Training step 3236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:46",
      "total_flops_so_far": 1.923120491463475e+16,
      "budget_used_percent": 19.231204914634752
    },
    {
      "type": "training",
      "description": "Training step 3237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:46",
      "total_flops_so_far": 1.923714597268685e+16,
      "budget_used_percent": 19.237145972686847
    },
    {
      "type": "training",
      "description": "Training step 3238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:47",
      "total_flops_so_far": 1.9243087030738944e+16,
      "budget_used_percent": 19.243087030738945
    },
    {
      "type": "training",
      "description": "Training step 3239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:47",
      "total_flops_so_far": 1.924902808879104e+16,
      "budget_used_percent": 19.24902808879104
    },
    {
      "type": "training",
      "description": "Training step 3240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:47",
      "total_flops_so_far": 1.9254969146843136e+16,
      "budget_used_percent": 19.254969146843138
    },
    {
      "type": "training",
      "description": "Training step 3241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:48",
      "total_flops_so_far": 1.926091020489523e+16,
      "budget_used_percent": 19.260910204895232
    },
    {
      "type": "training",
      "description": "Training step 3242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:48",
      "total_flops_so_far": 1.926685126294733e+16,
      "budget_used_percent": 19.26685126294733
    },
    {
      "type": "training",
      "description": "Training step 3243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:48",
      "total_flops_so_far": 1.9272792320999424e+16,
      "budget_used_percent": 19.272792320999425
    },
    {
      "type": "training",
      "description": "Training step 3244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:49",
      "total_flops_so_far": 1.927873337905152e+16,
      "budget_used_percent": 19.27873337905152
    },
    {
      "type": "training",
      "description": "Training step 3245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:49",
      "total_flops_so_far": 1.9284674437103616e+16,
      "budget_used_percent": 19.284674437103615
    },
    {
      "type": "training",
      "description": "Training step 3246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:50",
      "total_flops_so_far": 1.929061549515571e+16,
      "budget_used_percent": 19.29061549515571
    },
    {
      "type": "training",
      "description": "Training step 3247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:50",
      "total_flops_so_far": 1.929655655320781e+16,
      "budget_used_percent": 19.296556553207807
    },
    {
      "type": "training",
      "description": "Training step 3248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:50",
      "total_flops_so_far": 1.9302497611259904e+16,
      "budget_used_percent": 19.302497611259902
    },
    {
      "type": "training",
      "description": "Training step 3249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:51",
      "total_flops_so_far": 1.9308438669312e+16,
      "budget_used_percent": 19.308438669312
    },
    {
      "type": "training",
      "description": "Training step 3250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:51",
      "total_flops_so_far": 1.9314379727364096e+16,
      "budget_used_percent": 19.314379727364095
    },
    {
      "type": "training",
      "description": "Training step 3251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:51",
      "total_flops_so_far": 1.932032078541619e+16,
      "budget_used_percent": 19.320320785416193
    },
    {
      "type": "training",
      "description": "Training step 3252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:52",
      "total_flops_so_far": 1.932626184346829e+16,
      "budget_used_percent": 19.326261843468288
    },
    {
      "type": "training",
      "description": "Training step 3253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:52",
      "total_flops_so_far": 1.9332202901520384e+16,
      "budget_used_percent": 19.332202901520386
    },
    {
      "type": "training",
      "description": "Training step 3254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:53",
      "total_flops_so_far": 1.933814395957248e+16,
      "budget_used_percent": 19.33814395957248
    },
    {
      "type": "training",
      "description": "Training step 3255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:53",
      "total_flops_so_far": 1.9344085017624576e+16,
      "budget_used_percent": 19.344085017624575
    },
    {
      "type": "training",
      "description": "Training step 3256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:54",
      "total_flops_so_far": 1.935002607567667e+16,
      "budget_used_percent": 19.35002607567667
    },
    {
      "type": "training",
      "description": "Training step 3257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:54",
      "total_flops_so_far": 1.935596713372877e+16,
      "budget_used_percent": 19.355967133728768
    },
    {
      "type": "training",
      "description": "Training step 3258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:54",
      "total_flops_so_far": 1.9361908191780864e+16,
      "budget_used_percent": 19.361908191780863
    },
    {
      "type": "training",
      "description": "Training step 3259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:55",
      "total_flops_so_far": 1.936784924983296e+16,
      "budget_used_percent": 19.36784924983296
    },
    {
      "type": "training",
      "description": "Training step 3260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:55",
      "total_flops_so_far": 1.9373790307885056e+16,
      "budget_used_percent": 19.373790307885056
    },
    {
      "type": "training",
      "description": "Training step 3261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:56",
      "total_flops_so_far": 1.937973136593715e+16,
      "budget_used_percent": 19.379731365937154
    },
    {
      "type": "training",
      "description": "Training step 3262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:56",
      "total_flops_so_far": 1.938567242398925e+16,
      "budget_used_percent": 19.38567242398925
    },
    {
      "type": "training",
      "description": "Training step 3263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:56",
      "total_flops_so_far": 1.9391613482041344e+16,
      "budget_used_percent": 19.391613482041343
    },
    {
      "type": "training",
      "description": "Training step 3264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:57",
      "total_flops_so_far": 1.939755454009344e+16,
      "budget_used_percent": 19.397554540093438
    },
    {
      "type": "training",
      "description": "Training step 3265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:57",
      "total_flops_so_far": 1.9403495598145536e+16,
      "budget_used_percent": 19.403495598145536
    },
    {
      "type": "training",
      "description": "Training step 3266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:57",
      "total_flops_so_far": 1.940943665619763e+16,
      "budget_used_percent": 19.40943665619763
    },
    {
      "type": "training",
      "description": "Training step 3267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:58",
      "total_flops_so_far": 1.941537771424973e+16,
      "budget_used_percent": 19.41537771424973
    },
    {
      "type": "training",
      "description": "Training step 3268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:58",
      "total_flops_so_far": 1.9421318772301824e+16,
      "budget_used_percent": 19.421318772301824
    },
    {
      "type": "training",
      "description": "Training step 3269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:59",
      "total_flops_so_far": 1.942725983035392e+16,
      "budget_used_percent": 19.427259830353922
    },
    {
      "type": "training",
      "description": "Training step 3270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:59",
      "total_flops_so_far": 1.9433200888406016e+16,
      "budget_used_percent": 19.433200888406017
    },
    {
      "type": "training",
      "description": "Training step 3271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:50:59",
      "total_flops_so_far": 1.943914194645811e+16,
      "budget_used_percent": 19.439141946458115
    },
    {
      "type": "training",
      "description": "Training step 3272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:00",
      "total_flops_so_far": 1.944508300451021e+16,
      "budget_used_percent": 19.44508300451021
    },
    {
      "type": "training",
      "description": "Training step 3273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:00",
      "total_flops_so_far": 1.9451024062562304e+16,
      "budget_used_percent": 19.451024062562304
    },
    {
      "type": "training",
      "description": "Training step 3274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:01",
      "total_flops_so_far": 1.94569651206144e+16,
      "budget_used_percent": 19.4569651206144
    },
    {
      "type": "training",
      "description": "Training step 3275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:01",
      "total_flops_so_far": 1.9462906178666496e+16,
      "budget_used_percent": 19.462906178666493
    },
    {
      "type": "training",
      "description": "Training step 3276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:01",
      "total_flops_so_far": 1.946884723671859e+16,
      "budget_used_percent": 19.46884723671859
    },
    {
      "type": "training",
      "description": "Training step 3277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:02",
      "total_flops_so_far": 1.947478829477069e+16,
      "budget_used_percent": 19.474788294770686
    },
    {
      "type": "training",
      "description": "Training step 3278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:02",
      "total_flops_so_far": 1.9480729352822784e+16,
      "budget_used_percent": 19.480729352822785
    },
    {
      "type": "training",
      "description": "Training step 3279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:02",
      "total_flops_so_far": 1.948667041087488e+16,
      "budget_used_percent": 19.48667041087488
    },
    {
      "type": "training",
      "description": "Training step 3280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:03",
      "total_flops_so_far": 1.9492611468926976e+16,
      "budget_used_percent": 19.492611468926977
    },
    {
      "type": "training",
      "description": "Training step 3281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:03",
      "total_flops_so_far": 1.949855252697907e+16,
      "budget_used_percent": 19.498552526979072
    },
    {
      "type": "training",
      "description": "Training step 3282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:04",
      "total_flops_so_far": 1.950449358503117e+16,
      "budget_used_percent": 19.50449358503117
    },
    {
      "type": "training",
      "description": "Training step 3283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:04",
      "total_flops_so_far": 1.9510434643083264e+16,
      "budget_used_percent": 19.510434643083265
    },
    {
      "type": "training",
      "description": "Training step 3284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:04",
      "total_flops_so_far": 1.951637570113536e+16,
      "budget_used_percent": 19.51637570113536
    },
    {
      "type": "training",
      "description": "Training step 3285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:05",
      "total_flops_so_far": 1.9522316759187456e+16,
      "budget_used_percent": 19.522316759187454
    },
    {
      "type": "training",
      "description": "Training step 3286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:05",
      "total_flops_so_far": 1.952825781723955e+16,
      "budget_used_percent": 19.528257817239552
    },
    {
      "type": "training",
      "description": "Training step 3287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:06",
      "total_flops_so_far": 1.953419887529165e+16,
      "budget_used_percent": 19.534198875291647
    },
    {
      "type": "training",
      "description": "Training step 3288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:06",
      "total_flops_so_far": 1.9540139933343744e+16,
      "budget_used_percent": 19.540139933343745
    },
    {
      "type": "training",
      "description": "Training step 3289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:06",
      "total_flops_so_far": 1.954608099139584e+16,
      "budget_used_percent": 19.54608099139584
    },
    {
      "type": "training",
      "description": "Training step 3290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:07",
      "total_flops_so_far": 1.9552022049447936e+16,
      "budget_used_percent": 19.552022049447938
    },
    {
      "type": "training",
      "description": "Training step 3291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:07",
      "total_flops_so_far": 1.955796310750003e+16,
      "budget_used_percent": 19.557963107500033
    },
    {
      "type": "training",
      "description": "Training step 3292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:07",
      "total_flops_so_far": 1.956390416555213e+16,
      "budget_used_percent": 19.563904165552128
    },
    {
      "type": "training",
      "description": "Training step 3293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:08",
      "total_flops_so_far": 1.9569845223604224e+16,
      "budget_used_percent": 19.569845223604222
    },
    {
      "type": "training",
      "description": "Training step 3294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:08",
      "total_flops_so_far": 1.957578628165632e+16,
      "budget_used_percent": 19.57578628165632
    },
    {
      "type": "training",
      "description": "Training step 3295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:09",
      "total_flops_so_far": 1.9581727339708416e+16,
      "budget_used_percent": 19.581727339708415
    },
    {
      "type": "training",
      "description": "Training step 3296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:09",
      "total_flops_so_far": 1.958766839776051e+16,
      "budget_used_percent": 19.587668397760513
    },
    {
      "type": "training",
      "description": "Training step 3297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:09",
      "total_flops_so_far": 1.959360945581261e+16,
      "budget_used_percent": 19.593609455812608
    },
    {
      "type": "training",
      "description": "Training step 3298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:10",
      "total_flops_so_far": 1.9599550513864704e+16,
      "budget_used_percent": 19.599550513864706
    },
    {
      "type": "training",
      "description": "Training step 3299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:10",
      "total_flops_so_far": 1.96054915719168e+16,
      "budget_used_percent": 19.6054915719168
    },
    {
      "type": "training",
      "description": "Training step 3300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:11",
      "total_flops_so_far": 1.9611432629968896e+16,
      "budget_used_percent": 19.6114326299689
    },
    {
      "type": "training",
      "description": "Training step 3301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:11",
      "total_flops_so_far": 1.961737368802099e+16,
      "budget_used_percent": 19.617373688020994
    },
    {
      "type": "training",
      "description": "Training step 3302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:11",
      "total_flops_so_far": 1.962331474607309e+16,
      "budget_used_percent": 19.62331474607309
    },
    {
      "type": "training",
      "description": "Training step 3303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:12",
      "total_flops_so_far": 1.9629255804125184e+16,
      "budget_used_percent": 19.629255804125183
    },
    {
      "type": "training",
      "description": "Training step 3304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:12",
      "total_flops_so_far": 1.963519686217728e+16,
      "budget_used_percent": 19.635196862177278
    },
    {
      "type": "training",
      "description": "Training step 3305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:12",
      "total_flops_so_far": 1.9641137920229376e+16,
      "budget_used_percent": 19.641137920229376
    },
    {
      "type": "training",
      "description": "Training step 3306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:13",
      "total_flops_so_far": 1.964707897828147e+16,
      "budget_used_percent": 19.64707897828147
    },
    {
      "type": "training",
      "description": "Training step 3307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:13",
      "total_flops_so_far": 1.965302003633357e+16,
      "budget_used_percent": 19.65302003633357
    },
    {
      "type": "training",
      "description": "Training step 3308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:14",
      "total_flops_so_far": 1.9658961094385664e+16,
      "budget_used_percent": 19.658961094385663
    },
    {
      "type": "training",
      "description": "Training step 3309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:14",
      "total_flops_so_far": 1.966490215243776e+16,
      "budget_used_percent": 19.66490215243776
    },
    {
      "type": "training",
      "description": "Training step 3310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:14",
      "total_flops_so_far": 1.9670843210489856e+16,
      "budget_used_percent": 19.670843210489856
    },
    {
      "type": "training",
      "description": "Training step 3311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:15",
      "total_flops_so_far": 1.967678426854195e+16,
      "budget_used_percent": 19.67678426854195
    },
    {
      "type": "training",
      "description": "Training step 3312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:15",
      "total_flops_so_far": 1.968272532659405e+16,
      "budget_used_percent": 19.682725326594046
    },
    {
      "type": "training",
      "description": "Training step 3313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:16",
      "total_flops_so_far": 1.9688666384646144e+16,
      "budget_used_percent": 19.688666384646144
    },
    {
      "type": "training",
      "description": "Training step 3314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:16",
      "total_flops_so_far": 1.969460744269824e+16,
      "budget_used_percent": 19.69460744269824
    },
    {
      "type": "training",
      "description": "Training step 3315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:16",
      "total_flops_so_far": 1.9700548500750336e+16,
      "budget_used_percent": 19.700548500750337
    },
    {
      "type": "training",
      "description": "Training step 3316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:17",
      "total_flops_so_far": 1.970648955880243e+16,
      "budget_used_percent": 19.70648955880243
    },
    {
      "type": "training",
      "description": "Training step 3317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:17",
      "total_flops_so_far": 1.971243061685453e+16,
      "budget_used_percent": 19.71243061685453
    },
    {
      "type": "training",
      "description": "Training step 3318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:17",
      "total_flops_so_far": 1.9718371674906624e+16,
      "budget_used_percent": 19.718371674906624
    },
    {
      "type": "training",
      "description": "Training step 3319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:18",
      "total_flops_so_far": 1.972431273295872e+16,
      "budget_used_percent": 19.724312732958722
    },
    {
      "type": "training",
      "description": "Training step 3320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:18",
      "total_flops_so_far": 1.9730253791010816e+16,
      "budget_used_percent": 19.730253791010817
    },
    {
      "type": "training",
      "description": "Training step 3321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:19",
      "total_flops_so_far": 1.973619484906291e+16,
      "budget_used_percent": 19.73619484906291
    },
    {
      "type": "training",
      "description": "Training step 3322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:19",
      "total_flops_so_far": 1.974213590711501e+16,
      "budget_used_percent": 19.742135907115006
    },
    {
      "type": "training",
      "description": "Training step 3323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:19",
      "total_flops_so_far": 1.9748076965167104e+16,
      "budget_used_percent": 19.748076965167105
    },
    {
      "type": "training",
      "description": "Training step 3324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:20",
      "total_flops_so_far": 1.97540180232192e+16,
      "budget_used_percent": 19.7540180232192
    },
    {
      "type": "training",
      "description": "Training step 3325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:20",
      "total_flops_so_far": 1.9759959081271296e+16,
      "budget_used_percent": 19.759959081271298
    },
    {
      "type": "training",
      "description": "Training step 3326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:20",
      "total_flops_so_far": 1.976590013932339e+16,
      "budget_used_percent": 19.765900139323392
    },
    {
      "type": "training",
      "description": "Training step 3327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:21",
      "total_flops_so_far": 1.977184119737549e+16,
      "budget_used_percent": 19.77184119737549
    },
    {
      "type": "training",
      "description": "Training step 3328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:21",
      "total_flops_so_far": 1.9777782255427584e+16,
      "budget_used_percent": 19.777782255427585
    },
    {
      "type": "training",
      "description": "Training step 3329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:22",
      "total_flops_so_far": 1.978372331347968e+16,
      "budget_used_percent": 19.78372331347968
    },
    {
      "type": "training",
      "description": "Training step 3330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:22",
      "total_flops_so_far": 1.9789664371531776e+16,
      "budget_used_percent": 19.789664371531774
    },
    {
      "type": "training",
      "description": "Training step 3331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:22",
      "total_flops_so_far": 1.979560542958387e+16,
      "budget_used_percent": 19.79560542958387
    },
    {
      "type": "training",
      "description": "Training step 3332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:23",
      "total_flops_so_far": 1.980154648763597e+16,
      "budget_used_percent": 19.801546487635967
    },
    {
      "type": "training",
      "description": "Training step 3333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:23",
      "total_flops_so_far": 1.9807487545688064e+16,
      "budget_used_percent": 19.807487545688062
    },
    {
      "type": "training",
      "description": "Training step 3334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:24",
      "total_flops_so_far": 1.981342860374016e+16,
      "budget_used_percent": 19.81342860374016
    },
    {
      "type": "training",
      "description": "Training step 3335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:24",
      "total_flops_so_far": 1.9819369661792256e+16,
      "budget_used_percent": 19.819369661792255
    },
    {
      "type": "training",
      "description": "Training step 3336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:24",
      "total_flops_so_far": 1.982531071984435e+16,
      "budget_used_percent": 19.825310719844353
    },
    {
      "type": "training",
      "description": "Training step 3337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:25",
      "total_flops_so_far": 1.983125177789645e+16,
      "budget_used_percent": 19.831251777896448
    },
    {
      "type": "training",
      "description": "Training step 3338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:25",
      "total_flops_so_far": 1.9837192835948544e+16,
      "budget_used_percent": 19.837192835948546
    },
    {
      "type": "training",
      "description": "Training step 3339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:25",
      "total_flops_so_far": 1.984313389400064e+16,
      "budget_used_percent": 19.84313389400064
    },
    {
      "type": "training",
      "description": "Training step 3340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:26",
      "total_flops_so_far": 1.9849074952052736e+16,
      "budget_used_percent": 19.849074952052735
    },
    {
      "type": "training",
      "description": "Training step 3341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:26",
      "total_flops_so_far": 1.985501601010483e+16,
      "budget_used_percent": 19.85501601010483
    },
    {
      "type": "training",
      "description": "Training step 3342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:27",
      "total_flops_so_far": 1.986095706815693e+16,
      "budget_used_percent": 19.860957068156928
    },
    {
      "type": "training",
      "description": "Training step 3343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:27",
      "total_flops_so_far": 1.9866898126209024e+16,
      "budget_used_percent": 19.866898126209023
    },
    {
      "type": "training",
      "description": "Training step 3344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:28",
      "total_flops_so_far": 1.987283918426112e+16,
      "budget_used_percent": 19.87283918426112
    },
    {
      "type": "training",
      "description": "Training step 3345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:28",
      "total_flops_so_far": 1.9878780242313216e+16,
      "budget_used_percent": 19.878780242313216
    },
    {
      "type": "training",
      "description": "Training step 3346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:28",
      "total_flops_so_far": 1.988472130036531e+16,
      "budget_used_percent": 19.884721300365314
    },
    {
      "type": "training",
      "description": "Training step 3347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:29",
      "total_flops_so_far": 1.989066235841741e+16,
      "budget_used_percent": 19.89066235841741
    },
    {
      "type": "training",
      "description": "Training step 3348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:29",
      "total_flops_so_far": 1.9896603416469504e+16,
      "budget_used_percent": 19.896603416469503
    },
    {
      "type": "training",
      "description": "Training step 3349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:30",
      "total_flops_so_far": 1.99025444745216e+16,
      "budget_used_percent": 19.902544474521598
    },
    {
      "type": "training",
      "description": "Training step 3350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:30",
      "total_flops_so_far": 1.9908485532573696e+16,
      "budget_used_percent": 19.908485532573696
    },
    {
      "type": "training",
      "description": "Training step 3351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:30",
      "total_flops_so_far": 1.991442659062579e+16,
      "budget_used_percent": 19.91442659062579
    },
    {
      "type": "training",
      "description": "Training step 3352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:31",
      "total_flops_so_far": 1.992036764867789e+16,
      "budget_used_percent": 19.92036764867789
    },
    {
      "type": "training",
      "description": "Training step 3353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:31",
      "total_flops_so_far": 1.9926308706729984e+16,
      "budget_used_percent": 19.926308706729984
    },
    {
      "type": "training",
      "description": "Training step 3354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:32",
      "total_flops_so_far": 1.993224976478208e+16,
      "budget_used_percent": 19.93224976478208
    },
    {
      "type": "training",
      "description": "Training step 3355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:32",
      "total_flops_so_far": 1.9938190822834176e+16,
      "budget_used_percent": 19.938190822834176
    },
    {
      "type": "training",
      "description": "Training step 3356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:32",
      "total_flops_so_far": 1.994413188088627e+16,
      "budget_used_percent": 19.944131880886275
    },
    {
      "type": "training",
      "description": "Training step 3357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:33",
      "total_flops_so_far": 1.995007293893837e+16,
      "budget_used_percent": 19.95007293893837
    },
    {
      "type": "training",
      "description": "Training step 3358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:33",
      "total_flops_so_far": 1.9956013996990464e+16,
      "budget_used_percent": 19.956013996990464
    },
    {
      "type": "training",
      "description": "Training step 3359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:33",
      "total_flops_so_far": 1.996195505504256e+16,
      "budget_used_percent": 19.96195505504256
    },
    {
      "type": "training",
      "description": "Training step 3360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:34",
      "total_flops_so_far": 1.9967896113094656e+16,
      "budget_used_percent": 19.967896113094653
    },
    {
      "type": "training",
      "description": "Training step 3361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:34",
      "total_flops_so_far": 1.997383717114675e+16,
      "budget_used_percent": 19.97383717114675
    },
    {
      "type": "training",
      "description": "Training step 3362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:35",
      "total_flops_so_far": 1.997977822919885e+16,
      "budget_used_percent": 19.979778229198846
    },
    {
      "type": "training",
      "description": "Training step 3363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:35",
      "total_flops_so_far": 1.9985719287250944e+16,
      "budget_used_percent": 19.985719287250944
    },
    {
      "type": "training",
      "description": "Training step 3364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:35",
      "total_flops_so_far": 1.999166034530304e+16,
      "budget_used_percent": 19.99166034530304
    },
    {
      "type": "training",
      "description": "Training step 3365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:36",
      "total_flops_so_far": 1.9997601403355136e+16,
      "budget_used_percent": 19.997601403355137
    },
    {
      "type": "training",
      "description": "Training step 3366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:36",
      "total_flops_so_far": 2.000354246140723e+16,
      "budget_used_percent": 20.003542461407232
    },
    {
      "type": "training",
      "description": "Training step 3367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:37",
      "total_flops_so_far": 2.000948351945933e+16,
      "budget_used_percent": 20.00948351945933
    },
    {
      "type": "training",
      "description": "Training step 3368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:37",
      "total_flops_so_far": 2.0015424577511424e+16,
      "budget_used_percent": 20.015424577511425
    },
    {
      "type": "training",
      "description": "Training step 3369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:37",
      "total_flops_so_far": 2.002136563556352e+16,
      "budget_used_percent": 20.02136563556352
    },
    {
      "type": "training",
      "description": "Training step 3370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:38",
      "total_flops_so_far": 2.0027306693615616e+16,
      "budget_used_percent": 20.027306693615614
    },
    {
      "type": "training",
      "description": "Training step 3371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:38",
      "total_flops_so_far": 2.003324775166771e+16,
      "budget_used_percent": 20.033247751667712
    },
    {
      "type": "training",
      "description": "Training step 3372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:38",
      "total_flops_so_far": 2.003918880971981e+16,
      "budget_used_percent": 20.039188809719807
    },
    {
      "type": "training",
      "description": "Training step 3373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:39",
      "total_flops_so_far": 2.0045129867771904e+16,
      "budget_used_percent": 20.045129867771905
    },
    {
      "type": "training",
      "description": "Training step 3374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:39",
      "total_flops_so_far": 2.0051070925824e+16,
      "budget_used_percent": 20.051070925824
    },
    {
      "type": "training",
      "description": "Training step 3375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:40",
      "total_flops_so_far": 2.0057011983876096e+16,
      "budget_used_percent": 20.057011983876098
    },
    {
      "type": "training",
      "description": "Training step 3376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:40",
      "total_flops_so_far": 2.006295304192819e+16,
      "budget_used_percent": 20.062953041928193
    },
    {
      "type": "training",
      "description": "Training step 3377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:40",
      "total_flops_so_far": 2.006889409998029e+16,
      "budget_used_percent": 20.068894099980287
    },
    {
      "type": "training",
      "description": "Training step 3378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:41",
      "total_flops_so_far": 2.0074835158032384e+16,
      "budget_used_percent": 20.074835158032382
    },
    {
      "type": "training",
      "description": "Training step 3379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:41",
      "total_flops_so_far": 2.008077621608448e+16,
      "budget_used_percent": 20.08077621608448
    },
    {
      "type": "training",
      "description": "Training step 3380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:42",
      "total_flops_so_far": 2.0086717274136576e+16,
      "budget_used_percent": 20.086717274136575
    },
    {
      "type": "training",
      "description": "Training step 3381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:42",
      "total_flops_so_far": 2.009265833218867e+16,
      "budget_used_percent": 20.092658332188673
    },
    {
      "type": "training",
      "description": "Training step 3382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:42",
      "total_flops_so_far": 2.009859939024077e+16,
      "budget_used_percent": 20.098599390240768
    },
    {
      "type": "training",
      "description": "Training step 3383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:43",
      "total_flops_so_far": 2.0104540448292864e+16,
      "budget_used_percent": 20.104540448292866
    },
    {
      "type": "training",
      "description": "Training step 3384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:43",
      "total_flops_so_far": 2.011048150634496e+16,
      "budget_used_percent": 20.11048150634496
    },
    {
      "type": "training",
      "description": "Training step 3385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:43",
      "total_flops_so_far": 2.0116422564397056e+16,
      "budget_used_percent": 20.11642256439706
    },
    {
      "type": "training",
      "description": "Training step 3386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:44",
      "total_flops_so_far": 2.012236362244915e+16,
      "budget_used_percent": 20.122363622449154
    },
    {
      "type": "training",
      "description": "Training step 3387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:44",
      "total_flops_so_far": 2.012830468050125e+16,
      "budget_used_percent": 20.128304680501248
    },
    {
      "type": "training",
      "description": "Training step 3388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:45",
      "total_flops_so_far": 2.0134245738553344e+16,
      "budget_used_percent": 20.134245738553343
    },
    {
      "type": "training",
      "description": "Training step 3389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:45",
      "total_flops_so_far": 2.014018679660544e+16,
      "budget_used_percent": 20.140186796605438
    },
    {
      "type": "training",
      "description": "Training step 3390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:45",
      "total_flops_so_far": 2.0146127854657536e+16,
      "budget_used_percent": 20.146127854657536
    },
    {
      "type": "training",
      "description": "Training step 3391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:46",
      "total_flops_so_far": 2.015206891270963e+16,
      "budget_used_percent": 20.15206891270963
    },
    {
      "type": "training",
      "description": "Training step 3392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:46",
      "total_flops_so_far": 2.015800997076173e+16,
      "budget_used_percent": 20.15800997076173
    },
    {
      "type": "training",
      "description": "Training step 3393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:47",
      "total_flops_so_far": 2.0163951028813824e+16,
      "budget_used_percent": 20.163951028813823
    },
    {
      "type": "training",
      "description": "Training step 3394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:47",
      "total_flops_so_far": 2.016989208686592e+16,
      "budget_used_percent": 20.16989208686592
    },
    {
      "type": "training",
      "description": "Training step 3395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:47",
      "total_flops_so_far": 2.0175833144918016e+16,
      "budget_used_percent": 20.175833144918016
    },
    {
      "type": "training",
      "description": "Training step 3396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:48",
      "total_flops_so_far": 2.018177420297011e+16,
      "budget_used_percent": 20.18177420297011
    },
    {
      "type": "training",
      "description": "Training step 3397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:48",
      "total_flops_so_far": 2.018771526102221e+16,
      "budget_used_percent": 20.187715261022205
    },
    {
      "type": "training",
      "description": "Training step 3398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:48",
      "total_flops_so_far": 2.0193656319074304e+16,
      "budget_used_percent": 20.193656319074304
    },
    {
      "type": "training",
      "description": "Training step 3399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:49",
      "total_flops_so_far": 2.01995973771264e+16,
      "budget_used_percent": 20.1995973771264
    },
    {
      "type": "training",
      "description": "Training step 3400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:49",
      "total_flops_so_far": 2.0205538435178496e+16,
      "budget_used_percent": 20.205538435178497
    },
    {
      "type": "training",
      "description": "Training step 3401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:50",
      "total_flops_so_far": 2.021147949323059e+16,
      "budget_used_percent": 20.21147949323059
    },
    {
      "type": "training",
      "description": "Training step 3402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:50",
      "total_flops_so_far": 2.021742055128269e+16,
      "budget_used_percent": 20.21742055128269
    },
    {
      "type": "training",
      "description": "Training step 3403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:50",
      "total_flops_so_far": 2.0223361609334784e+16,
      "budget_used_percent": 20.223361609334784
    },
    {
      "type": "training",
      "description": "Training step 3404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:51",
      "total_flops_so_far": 2.022930266738688e+16,
      "budget_used_percent": 20.229302667386882
    },
    {
      "type": "training",
      "description": "Training step 3405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:51",
      "total_flops_so_far": 2.0235243725438976e+16,
      "budget_used_percent": 20.235243725438977
    },
    {
      "type": "training",
      "description": "Training step 3406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:52",
      "total_flops_so_far": 2.024118478349107e+16,
      "budget_used_percent": 20.24118478349107
    },
    {
      "type": "training",
      "description": "Training step 3407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:52",
      "total_flops_so_far": 2.024712584154317e+16,
      "budget_used_percent": 20.247125841543166
    },
    {
      "type": "training",
      "description": "Training step 3408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:52",
      "total_flops_so_far": 2.0253066899595264e+16,
      "budget_used_percent": 20.253066899595265
    },
    {
      "type": "training",
      "description": "Training step 3409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:53",
      "total_flops_so_far": 2.025900795764736e+16,
      "budget_used_percent": 20.25900795764736
    },
    {
      "type": "training",
      "description": "Training step 3410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:53",
      "total_flops_so_far": 2.0264949015699456e+16,
      "budget_used_percent": 20.264949015699457
    },
    {
      "type": "training",
      "description": "Training step 3411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:53",
      "total_flops_so_far": 2.027089007375155e+16,
      "budget_used_percent": 20.270890073751552
    },
    {
      "type": "training",
      "description": "Training step 3412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:54",
      "total_flops_so_far": 2.027683113180365e+16,
      "budget_used_percent": 20.27683113180365
    },
    {
      "type": "training",
      "description": "Training step 3413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:54",
      "total_flops_so_far": 2.0282772189855744e+16,
      "budget_used_percent": 20.282772189855745
    },
    {
      "type": "training",
      "description": "Training step 3414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:55",
      "total_flops_so_far": 2.028871324790784e+16,
      "budget_used_percent": 20.288713247907843
    },
    {
      "type": "training",
      "description": "Training step 3415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:55",
      "total_flops_so_far": 2.0294654305959936e+16,
      "budget_used_percent": 20.294654305959938
    },
    {
      "type": "training",
      "description": "Training step 3416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:56",
      "total_flops_so_far": 2.030059536401203e+16,
      "budget_used_percent": 20.300595364012032
    },
    {
      "type": "training",
      "description": "Training step 3417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:56",
      "total_flops_so_far": 2.030653642206413e+16,
      "budget_used_percent": 20.306536422064127
    },
    {
      "type": "training",
      "description": "Training step 3418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:56",
      "total_flops_so_far": 2.0312477480116224e+16,
      "budget_used_percent": 20.312477480116222
    },
    {
      "type": "training",
      "description": "Training step 3419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:57",
      "total_flops_so_far": 2.031841853816832e+16,
      "budget_used_percent": 20.31841853816832
    },
    {
      "type": "training",
      "description": "Training step 3420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:57",
      "total_flops_so_far": 2.0324359596220416e+16,
      "budget_used_percent": 20.324359596220415
    },
    {
      "type": "training",
      "description": "Training step 3421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:58",
      "total_flops_so_far": 2.033030065427251e+16,
      "budget_used_percent": 20.330300654272513
    },
    {
      "type": "training",
      "description": "Training step 3422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:58",
      "total_flops_so_far": 2.033624171232461e+16,
      "budget_used_percent": 20.336241712324608
    },
    {
      "type": "training",
      "description": "Training step 3423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:58",
      "total_flops_so_far": 2.0342182770376704e+16,
      "budget_used_percent": 20.342182770376706
    },
    {
      "type": "training",
      "description": "Training step 3424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:59",
      "total_flops_so_far": 2.03481238284288e+16,
      "budget_used_percent": 20.3481238284288
    },
    {
      "type": "training",
      "description": "Training step 3425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:51:59",
      "total_flops_so_far": 2.0354064886480896e+16,
      "budget_used_percent": 20.354064886480895
    },
    {
      "type": "training",
      "description": "Training step 3426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:00",
      "total_flops_so_far": 2.036000594453299e+16,
      "budget_used_percent": 20.36000594453299
    },
    {
      "type": "training",
      "description": "Training step 3427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:00",
      "total_flops_so_far": 2.036594700258509e+16,
      "budget_used_percent": 20.365947002585088
    },
    {
      "type": "training",
      "description": "Training step 3428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:00",
      "total_flops_so_far": 2.0371888060637184e+16,
      "budget_used_percent": 20.371888060637183
    },
    {
      "type": "training",
      "description": "Training step 3429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:01",
      "total_flops_so_far": 2.037782911868928e+16,
      "budget_used_percent": 20.37782911868928
    },
    {
      "type": "training",
      "description": "Training step 3430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:01",
      "total_flops_so_far": 2.0383770176741376e+16,
      "budget_used_percent": 20.383770176741375
    },
    {
      "type": "training",
      "description": "Training step 3431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:01",
      "total_flops_so_far": 2.038971123479347e+16,
      "budget_used_percent": 20.389711234793474
    },
    {
      "type": "training",
      "description": "Training step 3432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:02",
      "total_flops_so_far": 2.039565229284557e+16,
      "budget_used_percent": 20.39565229284557
    },
    {
      "type": "training",
      "description": "Training step 3433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:02",
      "total_flops_so_far": 2.0401593350897664e+16,
      "budget_used_percent": 20.401593350897667
    },
    {
      "type": "training",
      "description": "Training step 3434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:03",
      "total_flops_so_far": 2.040753440894976e+16,
      "budget_used_percent": 20.40753440894976
    },
    {
      "type": "training",
      "description": "Training step 3435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:03",
      "total_flops_so_far": 2.0413475467001856e+16,
      "budget_used_percent": 20.413475467001856
    },
    {
      "type": "training",
      "description": "Training step 3436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:03",
      "total_flops_so_far": 2.041941652505395e+16,
      "budget_used_percent": 20.41941652505395
    },
    {
      "type": "training",
      "description": "Training step 3437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:04",
      "total_flops_so_far": 2.042535758310605e+16,
      "budget_used_percent": 20.42535758310605
    },
    {
      "type": "training",
      "description": "Training step 3438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:04",
      "total_flops_so_far": 2.0431298641158144e+16,
      "budget_used_percent": 20.431298641158143
    },
    {
      "type": "training",
      "description": "Training step 3439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:05",
      "total_flops_so_far": 2.043723969921024e+16,
      "budget_used_percent": 20.43723969921024
    },
    {
      "type": "training",
      "description": "Training step 3440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:05",
      "total_flops_so_far": 2.0443180757262336e+16,
      "budget_used_percent": 20.443180757262336
    },
    {
      "type": "training",
      "description": "Training step 3441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:05",
      "total_flops_so_far": 2.044912181531443e+16,
      "budget_used_percent": 20.449121815314435
    },
    {
      "type": "training",
      "description": "Training step 3442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:06",
      "total_flops_so_far": 2.045506287336653e+16,
      "budget_used_percent": 20.45506287336653
    },
    {
      "type": "training",
      "description": "Training step 3443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:06",
      "total_flops_so_far": 2.0461003931418624e+16,
      "budget_used_percent": 20.461003931418624
    },
    {
      "type": "training",
      "description": "Training step 3444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:07",
      "total_flops_so_far": 2.046694498947072e+16,
      "budget_used_percent": 20.46694498947072
    },
    {
      "type": "training",
      "description": "Training step 3445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:07",
      "total_flops_so_far": 2.0472886047522816e+16,
      "budget_used_percent": 20.472886047522813
    },
    {
      "type": "training",
      "description": "Training step 3446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:07",
      "total_flops_so_far": 2.047882710557491e+16,
      "budget_used_percent": 20.47882710557491
    },
    {
      "type": "training",
      "description": "Training step 3447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:08",
      "total_flops_so_far": 2.048476816362701e+16,
      "budget_used_percent": 20.484768163627006
    },
    {
      "type": "training",
      "description": "Training step 3448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:08",
      "total_flops_so_far": 2.0490709221679104e+16,
      "budget_used_percent": 20.490709221679104
    },
    {
      "type": "training",
      "description": "Training step 3449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:08",
      "total_flops_so_far": 2.04966502797312e+16,
      "budget_used_percent": 20.4966502797312
    },
    {
      "type": "training",
      "description": "Training step 3450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:09",
      "total_flops_so_far": 2.0502591337783296e+16,
      "budget_used_percent": 20.502591337783297
    },
    {
      "type": "training",
      "description": "Training step 3451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:09",
      "total_flops_so_far": 2.050853239583539e+16,
      "budget_used_percent": 20.508532395835392
    },
    {
      "type": "training",
      "description": "Training step 3452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:10",
      "total_flops_so_far": 2.051447345388749e+16,
      "budget_used_percent": 20.51447345388749
    },
    {
      "type": "training",
      "description": "Training step 3453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:10",
      "total_flops_so_far": 2.0520414511939584e+16,
      "budget_used_percent": 20.520414511939585
    },
    {
      "type": "training",
      "description": "Training step 3454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:10",
      "total_flops_so_far": 2.052635556999168e+16,
      "budget_used_percent": 20.52635556999168
    },
    {
      "type": "training",
      "description": "Training step 3455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:11",
      "total_flops_so_far": 2.0532296628043776e+16,
      "budget_used_percent": 20.532296628043774
    },
    {
      "type": "training",
      "description": "Training step 3456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:11",
      "total_flops_so_far": 2.053823768609587e+16,
      "budget_used_percent": 20.538237686095872
    },
    {
      "type": "training",
      "description": "Training step 3457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:12",
      "total_flops_so_far": 2.054417874414797e+16,
      "budget_used_percent": 20.544178744147967
    },
    {
      "type": "training",
      "description": "Training step 3458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:12",
      "total_flops_so_far": 2.0550119802200064e+16,
      "budget_used_percent": 20.550119802200065
    },
    {
      "type": "training",
      "description": "Training step 3459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:12",
      "total_flops_so_far": 2.055606086025216e+16,
      "budget_used_percent": 20.55606086025216
    },
    {
      "type": "training",
      "description": "Training step 3460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:13",
      "total_flops_so_far": 2.0562001918304256e+16,
      "budget_used_percent": 20.562001918304258
    },
    {
      "type": "training",
      "description": "Training step 3461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:13",
      "total_flops_so_far": 2.056794297635635e+16,
      "budget_used_percent": 20.567942976356353
    },
    {
      "type": "training",
      "description": "Training step 3462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:13",
      "total_flops_so_far": 2.057388403440845e+16,
      "budget_used_percent": 20.573884034408447
    },
    {
      "type": "training",
      "description": "Training step 3463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:14",
      "total_flops_so_far": 2.0579825092460544e+16,
      "budget_used_percent": 20.579825092460542
    },
    {
      "type": "training",
      "description": "Training step 3464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:14",
      "total_flops_so_far": 2.058576615051264e+16,
      "budget_used_percent": 20.58576615051264
    },
    {
      "type": "training",
      "description": "Training step 3465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:15",
      "total_flops_so_far": 2.0591707208564736e+16,
      "budget_used_percent": 20.591707208564735
    },
    {
      "type": "training",
      "description": "Training step 3466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:15",
      "total_flops_so_far": 2.059764826661683e+16,
      "budget_used_percent": 20.597648266616833
    },
    {
      "type": "training",
      "description": "Training step 3467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:15",
      "total_flops_so_far": 2.060358932466893e+16,
      "budget_used_percent": 20.603589324668928
    },
    {
      "type": "training",
      "description": "Training step 3468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:16",
      "total_flops_so_far": 2.0609530382721024e+16,
      "budget_used_percent": 20.609530382721026
    },
    {
      "type": "training",
      "description": "Training step 3469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:16",
      "total_flops_so_far": 2.061547144077312e+16,
      "budget_used_percent": 20.61547144077312
    },
    {
      "type": "training",
      "description": "Training step 3470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:17",
      "total_flops_so_far": 2.0621412498825216e+16,
      "budget_used_percent": 20.62141249882522
    },
    {
      "type": "training",
      "description": "Training step 3471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:17",
      "total_flops_so_far": 2.062735355687731e+16,
      "budget_used_percent": 20.627353556877313
    },
    {
      "type": "training",
      "description": "Training step 3472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:17",
      "total_flops_so_far": 2.063329461492941e+16,
      "budget_used_percent": 20.633294614929408
    },
    {
      "type": "training",
      "description": "Training step 3473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:18",
      "total_flops_so_far": 2.0639235672981504e+16,
      "budget_used_percent": 20.639235672981503
    },
    {
      "type": "training",
      "description": "Training step 3474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:18",
      "total_flops_so_far": 2.06451767310336e+16,
      "budget_used_percent": 20.645176731033597
    },
    {
      "type": "training",
      "description": "Training step 3475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:18",
      "total_flops_so_far": 2.0651117789085696e+16,
      "budget_used_percent": 20.651117789085696
    },
    {
      "type": "training",
      "description": "Training step 3476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:19",
      "total_flops_so_far": 2.065705884713779e+16,
      "budget_used_percent": 20.65705884713779
    },
    {
      "type": "training",
      "description": "Training step 3477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:20",
      "total_flops_so_far": 2.066299990518989e+16,
      "budget_used_percent": 20.66299990518989
    },
    {
      "type": "training",
      "description": "Training step 3478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:20",
      "total_flops_so_far": 2.0668940963241984e+16,
      "budget_used_percent": 20.668940963241983
    },
    {
      "type": "training",
      "description": "Training step 3479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:20",
      "total_flops_so_far": 2.067488202129408e+16,
      "budget_used_percent": 20.67488202129408
    },
    {
      "type": "training",
      "description": "Training step 3480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:21",
      "total_flops_so_far": 2.0680823079346176e+16,
      "budget_used_percent": 20.680823079346176
    },
    {
      "type": "training",
      "description": "Training step 3481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:21",
      "total_flops_so_far": 2.068676413739827e+16,
      "budget_used_percent": 20.68676413739827
    },
    {
      "type": "training",
      "description": "Training step 3482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:21",
      "total_flops_so_far": 2.069270519545037e+16,
      "budget_used_percent": 20.692705195450365
    },
    {
      "type": "training",
      "description": "Training step 3483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:22",
      "total_flops_so_far": 2.0698646253502464e+16,
      "budget_used_percent": 20.698646253502464
    },
    {
      "type": "training",
      "description": "Training step 3484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:22",
      "total_flops_so_far": 2.070458731155456e+16,
      "budget_used_percent": 20.70458731155456
    },
    {
      "type": "training",
      "description": "Training step 3485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:23",
      "total_flops_so_far": 2.0710528369606656e+16,
      "budget_used_percent": 20.710528369606656
    },
    {
      "type": "training",
      "description": "Training step 3486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:23",
      "total_flops_so_far": 2.071646942765875e+16,
      "budget_used_percent": 20.71646942765875
    },
    {
      "type": "training",
      "description": "Training step 3487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:23",
      "total_flops_so_far": 2.072241048571085e+16,
      "budget_used_percent": 20.72241048571085
    },
    {
      "type": "training",
      "description": "Training step 3488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:24",
      "total_flops_so_far": 2.0728351543762944e+16,
      "budget_used_percent": 20.728351543762944
    },
    {
      "type": "training",
      "description": "Training step 3489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:24",
      "total_flops_so_far": 2.073429260181504e+16,
      "budget_used_percent": 20.734292601815042
    },
    {
      "type": "training",
      "description": "Training step 3490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:25",
      "total_flops_so_far": 2.0740233659867136e+16,
      "budget_used_percent": 20.740233659867137
    },
    {
      "type": "training",
      "description": "Training step 3491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:25",
      "total_flops_so_far": 2.074617471791923e+16,
      "budget_used_percent": 20.74617471791923
    },
    {
      "type": "training",
      "description": "Training step 3492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:25",
      "total_flops_so_far": 2.075211577597133e+16,
      "budget_used_percent": 20.752115775971326
    },
    {
      "type": "training",
      "description": "Training step 3493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:26",
      "total_flops_so_far": 2.0758056834023424e+16,
      "budget_used_percent": 20.758056834023424
    },
    {
      "type": "training",
      "description": "Training step 3494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:26",
      "total_flops_so_far": 2.076399789207552e+16,
      "budget_used_percent": 20.76399789207552
    },
    {
      "type": "training",
      "description": "Training step 3495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:27",
      "total_flops_so_far": 2.0769938950127616e+16,
      "budget_used_percent": 20.769938950127617
    },
    {
      "type": "training",
      "description": "Training step 3496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:27",
      "total_flops_so_far": 2.077588000817971e+16,
      "budget_used_percent": 20.775880008179712
    },
    {
      "type": "training",
      "description": "Training step 3497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:27",
      "total_flops_so_far": 2.078182106623181e+16,
      "budget_used_percent": 20.78182106623181
    },
    {
      "type": "training",
      "description": "Training step 3498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:28",
      "total_flops_so_far": 2.0787762124283904e+16,
      "budget_used_percent": 20.787762124283905
    },
    {
      "type": "training",
      "description": "Training step 3499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:52:28",
      "total_flops_so_far": 2.0793703182336e+16,
      "budget_used_percent": 20.793703182336003
    },
    {
      "type": "training",
      "description": "Training step 3500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:52",
      "total_flops_so_far": 2.0799644240388096e+16,
      "budget_used_percent": 20.799644240388098
    },
    {
      "type": "training",
      "description": "Training step 3501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:52",
      "total_flops_so_far": 2.080558529844019e+16,
      "budget_used_percent": 20.805585298440192
    },
    {
      "type": "training",
      "description": "Training step 3502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:53",
      "total_flops_so_far": 2.081152635649229e+16,
      "budget_used_percent": 20.811526356492287
    },
    {
      "type": "training",
      "description": "Training step 3503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:53",
      "total_flops_so_far": 2.0817467414544384e+16,
      "budget_used_percent": 20.81746741454438
    },
    {
      "type": "training",
      "description": "Training step 3504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:53",
      "total_flops_so_far": 2.082340847259648e+16,
      "budget_used_percent": 20.82340847259648
    },
    {
      "type": "training",
      "description": "Training step 3505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:54",
      "total_flops_so_far": 2.0829349530648576e+16,
      "budget_used_percent": 20.829349530648575
    },
    {
      "type": "training",
      "description": "Training step 3506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:54",
      "total_flops_so_far": 2.083529058870067e+16,
      "budget_used_percent": 20.835290588700673
    },
    {
      "type": "training",
      "description": "Training step 3507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:55",
      "total_flops_so_far": 2.084123164675277e+16,
      "budget_used_percent": 20.841231646752767
    },
    {
      "type": "training",
      "description": "Training step 3508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:55",
      "total_flops_so_far": 2.0847172704804864e+16,
      "budget_used_percent": 20.847172704804866
    },
    {
      "type": "training",
      "description": "Training step 3509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:55",
      "total_flops_so_far": 2.085311376285696e+16,
      "budget_used_percent": 20.85311376285696
    },
    {
      "type": "training",
      "description": "Training step 3510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:56",
      "total_flops_so_far": 2.0859054820909056e+16,
      "budget_used_percent": 20.859054820909055
    },
    {
      "type": "training",
      "description": "Training step 3511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:56",
      "total_flops_so_far": 2.086499587896115e+16,
      "budget_used_percent": 20.86499587896115
    },
    {
      "type": "training",
      "description": "Training step 3512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:56",
      "total_flops_so_far": 2.087093693701325e+16,
      "budget_used_percent": 20.870936937013248
    },
    {
      "type": "training",
      "description": "Training step 3513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:57",
      "total_flops_so_far": 2.0876877995065344e+16,
      "budget_used_percent": 20.876877995065342
    },
    {
      "type": "training",
      "description": "Training step 3514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:57",
      "total_flops_so_far": 2.088281905311744e+16,
      "budget_used_percent": 20.88281905311744
    },
    {
      "type": "training",
      "description": "Training step 3515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:57",
      "total_flops_so_far": 2.0888760111169536e+16,
      "budget_used_percent": 20.888760111169535
    },
    {
      "type": "training",
      "description": "Training step 3516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:58",
      "total_flops_so_far": 2.089470116922163e+16,
      "budget_used_percent": 20.894701169221634
    },
    {
      "type": "training",
      "description": "Training step 3517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:58",
      "total_flops_so_far": 2.090064222727373e+16,
      "budget_used_percent": 20.90064222727373
    },
    {
      "type": "training",
      "description": "Training step 3518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:59",
      "total_flops_so_far": 2.0906583285325824e+16,
      "budget_used_percent": 20.906583285325826
    },
    {
      "type": "training",
      "description": "Training step 3519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:59",
      "total_flops_so_far": 2.091252434337792e+16,
      "budget_used_percent": 20.91252434337792
    },
    {
      "type": "training",
      "description": "Training step 3520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:54:59",
      "total_flops_so_far": 2.0918465401430016e+16,
      "budget_used_percent": 20.918465401430016
    },
    {
      "type": "training",
      "description": "Training step 3521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:00",
      "total_flops_so_far": 2.092440645948211e+16,
      "budget_used_percent": 20.92440645948211
    },
    {
      "type": "training",
      "description": "Training step 3522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:00",
      "total_flops_so_far": 2.093034751753421e+16,
      "budget_used_percent": 20.93034751753421
    },
    {
      "type": "training",
      "description": "Training step 3523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:01",
      "total_flops_so_far": 2.0936288575586304e+16,
      "budget_used_percent": 20.936288575586303
    },
    {
      "type": "training",
      "description": "Training step 3524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:01",
      "total_flops_so_far": 2.09422296336384e+16,
      "budget_used_percent": 20.9422296336384
    },
    {
      "type": "training",
      "description": "Training step 3525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:01",
      "total_flops_so_far": 2.0948170691690496e+16,
      "budget_used_percent": 20.948170691690496
    },
    {
      "type": "training",
      "description": "Training step 3526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:02",
      "total_flops_so_far": 2.095411174974259e+16,
      "budget_used_percent": 20.954111749742594
    },
    {
      "type": "training",
      "description": "Training step 3527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:02",
      "total_flops_so_far": 2.096005280779469e+16,
      "budget_used_percent": 20.96005280779469
    },
    {
      "type": "training",
      "description": "Training step 3528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:03",
      "total_flops_so_far": 2.0965993865846784e+16,
      "budget_used_percent": 20.965993865846784
    },
    {
      "type": "training",
      "description": "Training step 3529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:03",
      "total_flops_so_far": 2.097193492389888e+16,
      "budget_used_percent": 20.97193492389888
    },
    {
      "type": "training",
      "description": "Training step 3530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:03",
      "total_flops_so_far": 2.0977875981950976e+16,
      "budget_used_percent": 20.977875981950973
    },
    {
      "type": "training",
      "description": "Training step 3531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:04",
      "total_flops_so_far": 2.098381704000307e+16,
      "budget_used_percent": 20.98381704000307
    },
    {
      "type": "training",
      "description": "Training step 3532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:04",
      "total_flops_so_far": 2.098975809805517e+16,
      "budget_used_percent": 20.989758098055166
    },
    {
      "type": "training",
      "description": "Training step 3533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:04",
      "total_flops_so_far": 2.0995699156107264e+16,
      "budget_used_percent": 20.995699156107264
    },
    {
      "type": "training",
      "description": "Training step 3534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:05",
      "total_flops_so_far": 2.100164021415936e+16,
      "budget_used_percent": 21.00164021415936
    },
    {
      "type": "training",
      "description": "Training step 3535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:05",
      "total_flops_so_far": 2.1007581272211456e+16,
      "budget_used_percent": 21.007581272211457
    },
    {
      "type": "training",
      "description": "Training step 3536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:06",
      "total_flops_so_far": 2.101352233026355e+16,
      "budget_used_percent": 21.01352233026355
    },
    {
      "type": "training",
      "description": "Training step 3537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:06",
      "total_flops_so_far": 2.101946338831565e+16,
      "budget_used_percent": 21.01946338831565
    },
    {
      "type": "training",
      "description": "Training step 3538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:06",
      "total_flops_so_far": 2.1025404446367744e+16,
      "budget_used_percent": 21.025404446367745
    },
    {
      "type": "training",
      "description": "Training step 3539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:07",
      "total_flops_so_far": 2.103134550441984e+16,
      "budget_used_percent": 21.03134550441984
    },
    {
      "type": "training",
      "description": "Training step 3540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:07",
      "total_flops_so_far": 2.1037286562471936e+16,
      "budget_used_percent": 21.037286562471934
    },
    {
      "type": "training",
      "description": "Training step 3541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:08",
      "total_flops_so_far": 2.104322762052403e+16,
      "budget_used_percent": 21.043227620524032
    },
    {
      "type": "training",
      "description": "Training step 3542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:08",
      "total_flops_so_far": 2.104916867857613e+16,
      "budget_used_percent": 21.049168678576127
    },
    {
      "type": "training",
      "description": "Training step 3543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:08",
      "total_flops_so_far": 2.1055109736628224e+16,
      "budget_used_percent": 21.055109736628225
    },
    {
      "type": "training",
      "description": "Training step 3544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:09",
      "total_flops_so_far": 2.106105079468032e+16,
      "budget_used_percent": 21.06105079468032
    },
    {
      "type": "training",
      "description": "Training step 3545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:09",
      "total_flops_so_far": 2.1066991852732416e+16,
      "budget_used_percent": 21.066991852732418
    },
    {
      "type": "training",
      "description": "Training step 3546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:09",
      "total_flops_so_far": 2.107293291078451e+16,
      "budget_used_percent": 21.072932910784512
    },
    {
      "type": "training",
      "description": "Training step 3547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:10",
      "total_flops_so_far": 2.107887396883661e+16,
      "budget_used_percent": 21.07887396883661
    },
    {
      "type": "training",
      "description": "Training step 3548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:11",
      "total_flops_so_far": 2.1084815026888704e+16,
      "budget_used_percent": 21.084815026888705
    },
    {
      "type": "training",
      "description": "Training step 3549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:11",
      "total_flops_so_far": 2.10907560849408e+16,
      "budget_used_percent": 21.0907560849408
    },
    {
      "type": "training",
      "description": "Training step 3550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:11",
      "total_flops_so_far": 2.1096697142992896e+16,
      "budget_used_percent": 21.096697142992895
    },
    {
      "type": "training",
      "description": "Training step 3551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:12",
      "total_flops_so_far": 2.110263820104499e+16,
      "budget_used_percent": 21.102638201044993
    },
    {
      "type": "training",
      "description": "Training step 3552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:12",
      "total_flops_so_far": 2.110857925909709e+16,
      "budget_used_percent": 21.108579259097088
    },
    {
      "type": "training",
      "description": "Training step 3553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:13",
      "total_flops_so_far": 2.1114520317149184e+16,
      "budget_used_percent": 21.114520317149186
    },
    {
      "type": "training",
      "description": "Training step 3554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:13",
      "total_flops_so_far": 2.112046137520128e+16,
      "budget_used_percent": 21.12046137520128
    },
    {
      "type": "training",
      "description": "Training step 3555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:13",
      "total_flops_so_far": 2.1126402433253376e+16,
      "budget_used_percent": 21.12640243325338
    },
    {
      "type": "training",
      "description": "Training step 3556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:14",
      "total_flops_so_far": 2.113234349130547e+16,
      "budget_used_percent": 21.132343491305473
    },
    {
      "type": "training",
      "description": "Training step 3557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:14",
      "total_flops_so_far": 2.113828454935757e+16,
      "budget_used_percent": 21.138284549357568
    },
    {
      "type": "training",
      "description": "Training step 3558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:14",
      "total_flops_so_far": 2.1144225607409664e+16,
      "budget_used_percent": 21.144225607409663
    },
    {
      "type": "training",
      "description": "Training step 3559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:15",
      "total_flops_so_far": 2.115016666546176e+16,
      "budget_used_percent": 21.150166665461757
    },
    {
      "type": "training",
      "description": "Training step 3560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:15",
      "total_flops_so_far": 2.1156107723513856e+16,
      "budget_used_percent": 21.156107723513855
    },
    {
      "type": "training",
      "description": "Training step 3561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:16",
      "total_flops_so_far": 2.116204878156595e+16,
      "budget_used_percent": 21.16204878156595
    },
    {
      "type": "training",
      "description": "Training step 3562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:16",
      "total_flops_so_far": 2.116798983961805e+16,
      "budget_used_percent": 21.16798983961805
    },
    {
      "type": "training",
      "description": "Training step 3563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:16",
      "total_flops_so_far": 2.1173930897670144e+16,
      "budget_used_percent": 21.173930897670143
    },
    {
      "type": "training",
      "description": "Training step 3564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:17",
      "total_flops_so_far": 2.117987195572224e+16,
      "budget_used_percent": 21.17987195572224
    },
    {
      "type": "training",
      "description": "Training step 3565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:17",
      "total_flops_so_far": 2.1185813013774336e+16,
      "budget_used_percent": 21.185813013774336
    },
    {
      "type": "training",
      "description": "Training step 3566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:18",
      "total_flops_so_far": 2.119175407182643e+16,
      "budget_used_percent": 21.191754071826434
    },
    {
      "type": "training",
      "description": "Training step 3567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:18",
      "total_flops_so_far": 2.119769512987853e+16,
      "budget_used_percent": 21.19769512987853
    },
    {
      "type": "training",
      "description": "Training step 3568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:18",
      "total_flops_so_far": 2.1203636187930624e+16,
      "budget_used_percent": 21.203636187930623
    },
    {
      "type": "training",
      "description": "Training step 3569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:19",
      "total_flops_so_far": 2.120957724598272e+16,
      "budget_used_percent": 21.209577245982718
    },
    {
      "type": "training",
      "description": "Training step 3570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:19",
      "total_flops_so_far": 2.1215518304034816e+16,
      "budget_used_percent": 21.215518304034816
    },
    {
      "type": "training",
      "description": "Training step 3571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:20",
      "total_flops_so_far": 2.122145936208691e+16,
      "budget_used_percent": 21.22145936208691
    },
    {
      "type": "training",
      "description": "Training step 3572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:20",
      "total_flops_so_far": 2.122740042013901e+16,
      "budget_used_percent": 21.22740042013901
    },
    {
      "type": "training",
      "description": "Training step 3573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:20",
      "total_flops_so_far": 2.1233341478191104e+16,
      "budget_used_percent": 21.233341478191104
    },
    {
      "type": "training",
      "description": "Training step 3574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:21",
      "total_flops_so_far": 2.12392825362432e+16,
      "budget_used_percent": 21.239282536243202
    },
    {
      "type": "training",
      "description": "Training step 3575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:21",
      "total_flops_so_far": 2.1245223594295296e+16,
      "budget_used_percent": 21.245223594295297
    },
    {
      "type": "training",
      "description": "Training step 3576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:21",
      "total_flops_so_far": 2.125116465234739e+16,
      "budget_used_percent": 21.25116465234739
    },
    {
      "type": "training",
      "description": "Training step 3577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:22",
      "total_flops_so_far": 2.125710571039949e+16,
      "budget_used_percent": 21.257105710399486
    },
    {
      "type": "training",
      "description": "Training step 3578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:22",
      "total_flops_so_far": 2.1263046768451584e+16,
      "budget_used_percent": 21.263046768451584
    },
    {
      "type": "training",
      "description": "Training step 3579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:23",
      "total_flops_so_far": 2.126898782650368e+16,
      "budget_used_percent": 21.26898782650368
    },
    {
      "type": "training",
      "description": "Training step 3580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:23",
      "total_flops_so_far": 2.1274928884555776e+16,
      "budget_used_percent": 21.274928884555777
    },
    {
      "type": "training",
      "description": "Training step 3581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:23",
      "total_flops_so_far": 2.128086994260787e+16,
      "budget_used_percent": 21.280869942607872
    },
    {
      "type": "training",
      "description": "Training step 3582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:24",
      "total_flops_so_far": 2.128681100065997e+16,
      "budget_used_percent": 21.28681100065997
    },
    {
      "type": "training",
      "description": "Training step 3583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:24",
      "total_flops_so_far": 2.1292752058712064e+16,
      "budget_used_percent": 21.292752058712065
    },
    {
      "type": "training",
      "description": "Training step 3584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:25",
      "total_flops_so_far": 2.129869311676416e+16,
      "budget_used_percent": 21.298693116764163
    },
    {
      "type": "training",
      "description": "Training step 3585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:25",
      "total_flops_so_far": 2.1304634174816256e+16,
      "budget_used_percent": 21.304634174816258
    },
    {
      "type": "training",
      "description": "Training step 3586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:25",
      "total_flops_so_far": 2.131057523286835e+16,
      "budget_used_percent": 21.310575232868352
    },
    {
      "type": "training",
      "description": "Training step 3587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:26",
      "total_flops_so_far": 2.131651629092045e+16,
      "budget_used_percent": 21.316516290920447
    },
    {
      "type": "training",
      "description": "Training step 3588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:26",
      "total_flops_so_far": 2.1322457348972544e+16,
      "budget_used_percent": 21.32245734897254
    },
    {
      "type": "training",
      "description": "Training step 3589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:27",
      "total_flops_so_far": 2.132839840702464e+16,
      "budget_used_percent": 21.32839840702464
    },
    {
      "type": "training",
      "description": "Training step 3590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:27",
      "total_flops_so_far": 2.1334339465076736e+16,
      "budget_used_percent": 21.334339465076734
    },
    {
      "type": "training",
      "description": "Training step 3591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:27",
      "total_flops_so_far": 2.134028052312883e+16,
      "budget_used_percent": 21.340280523128833
    },
    {
      "type": "training",
      "description": "Training step 3592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:28",
      "total_flops_so_far": 2.134622158118093e+16,
      "budget_used_percent": 21.346221581180927
    },
    {
      "type": "training",
      "description": "Training step 3593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:28",
      "total_flops_so_far": 2.1352162639233024e+16,
      "budget_used_percent": 21.352162639233025
    },
    {
      "type": "training",
      "description": "Training step 3594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:29",
      "total_flops_so_far": 2.135810369728512e+16,
      "budget_used_percent": 21.35810369728512
    },
    {
      "type": "training",
      "description": "Training step 3595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:29",
      "total_flops_so_far": 2.1364044755337216e+16,
      "budget_used_percent": 21.364044755337215
    },
    {
      "type": "training",
      "description": "Training step 3596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:29",
      "total_flops_so_far": 2.136998581338931e+16,
      "budget_used_percent": 21.36998581338931
    },
    {
      "type": "training",
      "description": "Training step 3597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:30",
      "total_flops_so_far": 2.137592687144141e+16,
      "budget_used_percent": 21.375926871441408
    },
    {
      "type": "training",
      "description": "Training step 3598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:30",
      "total_flops_so_far": 2.1381867929493504e+16,
      "budget_used_percent": 21.381867929493502
    },
    {
      "type": "training",
      "description": "Training step 3599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:30",
      "total_flops_so_far": 2.13878089875456e+16,
      "budget_used_percent": 21.3878089875456
    },
    {
      "type": "training",
      "description": "Training step 3600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:31",
      "total_flops_so_far": 2.1393750045597696e+16,
      "budget_used_percent": 21.393750045597695
    },
    {
      "type": "training",
      "description": "Training step 3601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:31",
      "total_flops_so_far": 2.139969110364979e+16,
      "budget_used_percent": 21.399691103649793
    },
    {
      "type": "training",
      "description": "Training step 3602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:32",
      "total_flops_so_far": 2.140563216170189e+16,
      "budget_used_percent": 21.405632161701888
    },
    {
      "type": "training",
      "description": "Training step 3603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:32",
      "total_flops_so_far": 2.1411573219753984e+16,
      "budget_used_percent": 21.411573219753986
    },
    {
      "type": "training",
      "description": "Training step 3604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:33",
      "total_flops_so_far": 2.141751427780608e+16,
      "budget_used_percent": 21.41751427780608
    },
    {
      "type": "training",
      "description": "Training step 3605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:33",
      "total_flops_so_far": 2.1423455335858176e+16,
      "budget_used_percent": 21.423455335858176
    },
    {
      "type": "training",
      "description": "Training step 3606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:34",
      "total_flops_so_far": 2.142939639391027e+16,
      "budget_used_percent": 21.42939639391027
    },
    {
      "type": "training",
      "description": "Training step 3607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:34",
      "total_flops_so_far": 2.143533745196237e+16,
      "budget_used_percent": 21.43533745196237
    },
    {
      "type": "training",
      "description": "Training step 3608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:34",
      "total_flops_so_far": 2.1441278510014464e+16,
      "budget_used_percent": 21.441278510014463
    },
    {
      "type": "training",
      "description": "Training step 3609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:35",
      "total_flops_so_far": 2.144721956806656e+16,
      "budget_used_percent": 21.44721956806656
    },
    {
      "type": "training",
      "description": "Training step 3610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:35",
      "total_flops_so_far": 2.1453160626118656e+16,
      "budget_used_percent": 21.453160626118656
    },
    {
      "type": "training",
      "description": "Training step 3611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:35",
      "total_flops_so_far": 2.145910168417075e+16,
      "budget_used_percent": 21.459101684170754
    },
    {
      "type": "training",
      "description": "Training step 3612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:36",
      "total_flops_so_far": 2.146504274222285e+16,
      "budget_used_percent": 21.46504274222285
    },
    {
      "type": "training",
      "description": "Training step 3613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:36",
      "total_flops_so_far": 2.1470983800274944e+16,
      "budget_used_percent": 21.470983800274944
    },
    {
      "type": "training",
      "description": "Training step 3614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:37",
      "total_flops_so_far": 2.147692485832704e+16,
      "budget_used_percent": 21.47692485832704
    },
    {
      "type": "training",
      "description": "Training step 3615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:37",
      "total_flops_so_far": 2.1482865916379136e+16,
      "budget_used_percent": 21.482865916379133
    },
    {
      "type": "training",
      "description": "Training step 3616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:37",
      "total_flops_so_far": 2.148880697443123e+16,
      "budget_used_percent": 21.48880697443123
    },
    {
      "type": "training",
      "description": "Training step 3617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:38",
      "total_flops_so_far": 2.149474803248333e+16,
      "budget_used_percent": 21.494748032483326
    },
    {
      "type": "training",
      "description": "Training step 3618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:38",
      "total_flops_so_far": 2.1500689090535424e+16,
      "budget_used_percent": 21.500689090535424
    },
    {
      "type": "training",
      "description": "Training step 3619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:39",
      "total_flops_so_far": 2.150663014858752e+16,
      "budget_used_percent": 21.50663014858752
    },
    {
      "type": "training",
      "description": "Training step 3620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:39",
      "total_flops_so_far": 2.1512571206639616e+16,
      "budget_used_percent": 21.512571206639617
    },
    {
      "type": "training",
      "description": "Training step 3621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:39",
      "total_flops_so_far": 2.151851226469171e+16,
      "budget_used_percent": 21.51851226469171
    },
    {
      "type": "training",
      "description": "Training step 3622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:40",
      "total_flops_so_far": 2.152445332274381e+16,
      "budget_used_percent": 21.52445332274381
    },
    {
      "type": "training",
      "description": "Training step 3623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:40",
      "total_flops_so_far": 2.1530394380795904e+16,
      "budget_used_percent": 21.530394380795904
    },
    {
      "type": "training",
      "description": "Training step 3624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:41",
      "total_flops_so_far": 2.1536335438848e+16,
      "budget_used_percent": 21.536335438848
    },
    {
      "type": "training",
      "description": "Training step 3625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:41",
      "total_flops_so_far": 2.1542276496900096e+16,
      "budget_used_percent": 21.542276496900094
    },
    {
      "type": "training",
      "description": "Training step 3626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:41",
      "total_flops_so_far": 2.154821755495219e+16,
      "budget_used_percent": 21.548217554952192
    },
    {
      "type": "training",
      "description": "Training step 3627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:42",
      "total_flops_so_far": 2.155415861300429e+16,
      "budget_used_percent": 21.554158613004287
    },
    {
      "type": "training",
      "description": "Training step 3628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:42",
      "total_flops_so_far": 2.1560099671056384e+16,
      "budget_used_percent": 21.560099671056385
    },
    {
      "type": "training",
      "description": "Training step 3629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:42",
      "total_flops_so_far": 2.156604072910848e+16,
      "budget_used_percent": 21.56604072910848
    },
    {
      "type": "training",
      "description": "Training step 3630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:43",
      "total_flops_so_far": 2.1571981787160576e+16,
      "budget_used_percent": 21.571981787160578
    },
    {
      "type": "training",
      "description": "Training step 3631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:43",
      "total_flops_so_far": 2.157792284521267e+16,
      "budget_used_percent": 21.577922845212672
    },
    {
      "type": "training",
      "description": "Training step 3632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:44",
      "total_flops_so_far": 2.158386390326477e+16,
      "budget_used_percent": 21.58386390326477
    },
    {
      "type": "training",
      "description": "Training step 3633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:44",
      "total_flops_so_far": 2.1589804961316864e+16,
      "budget_used_percent": 21.589804961316865
    },
    {
      "type": "training",
      "description": "Training step 3634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:44",
      "total_flops_so_far": 2.159574601936896e+16,
      "budget_used_percent": 21.59574601936896
    },
    {
      "type": "training",
      "description": "Training step 3635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:45",
      "total_flops_so_far": 2.1601687077421056e+16,
      "budget_used_percent": 21.601687077421055
    },
    {
      "type": "training",
      "description": "Training step 3636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:45",
      "total_flops_so_far": 2.160762813547315e+16,
      "budget_used_percent": 21.607628135473153
    },
    {
      "type": "training",
      "description": "Training step 3637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:46",
      "total_flops_so_far": 2.161356919352525e+16,
      "budget_used_percent": 21.613569193525247
    },
    {
      "type": "training",
      "description": "Training step 3638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:46",
      "total_flops_so_far": 2.1619510251577344e+16,
      "budget_used_percent": 21.619510251577346
    },
    {
      "type": "training",
      "description": "Training step 3639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:46",
      "total_flops_so_far": 2.162545130962944e+16,
      "budget_used_percent": 21.62545130962944
    },
    {
      "type": "training",
      "description": "Training step 3640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:47",
      "total_flops_so_far": 2.1631392367681536e+16,
      "budget_used_percent": 21.63139236768154
    },
    {
      "type": "training",
      "description": "Training step 3641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:47",
      "total_flops_so_far": 2.163733342573363e+16,
      "budget_used_percent": 21.637333425733633
    },
    {
      "type": "training",
      "description": "Training step 3642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:48",
      "total_flops_so_far": 2.164327448378573e+16,
      "budget_used_percent": 21.643274483785728
    },
    {
      "type": "training",
      "description": "Training step 3643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:48",
      "total_flops_so_far": 2.1649215541837824e+16,
      "budget_used_percent": 21.649215541837822
    },
    {
      "type": "training",
      "description": "Training step 3644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:48",
      "total_flops_so_far": 2.165515659988992e+16,
      "budget_used_percent": 21.655156599889917
    },
    {
      "type": "training",
      "description": "Training step 3645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:49",
      "total_flops_so_far": 2.1661097657942016e+16,
      "budget_used_percent": 21.661097657942015
    },
    {
      "type": "training",
      "description": "Training step 3646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:49",
      "total_flops_so_far": 2.166703871599411e+16,
      "budget_used_percent": 21.66703871599411
    },
    {
      "type": "training",
      "description": "Training step 3647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:50",
      "total_flops_so_far": 2.167297977404621e+16,
      "budget_used_percent": 21.67297977404621
    },
    {
      "type": "training",
      "description": "Training step 3648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:50",
      "total_flops_so_far": 2.1678920832098304e+16,
      "budget_used_percent": 21.678920832098303
    },
    {
      "type": "training",
      "description": "Training step 3649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:50",
      "total_flops_so_far": 2.16848618901504e+16,
      "budget_used_percent": 21.6848618901504
    },
    {
      "type": "training",
      "description": "Training step 3650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:51",
      "total_flops_so_far": 2.1690802948202496e+16,
      "budget_used_percent": 21.690802948202496
    },
    {
      "type": "training",
      "description": "Training step 3651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:51",
      "total_flops_so_far": 2.169674400625459e+16,
      "budget_used_percent": 21.696744006254594
    },
    {
      "type": "training",
      "description": "Training step 3652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:51",
      "total_flops_so_far": 2.170268506430669e+16,
      "budget_used_percent": 21.70268506430669
    },
    {
      "type": "training",
      "description": "Training step 3653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:52",
      "total_flops_so_far": 2.1708626122358784e+16,
      "budget_used_percent": 21.708626122358783
    },
    {
      "type": "training",
      "description": "Training step 3654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:52",
      "total_flops_so_far": 2.171456718041088e+16,
      "budget_used_percent": 21.714567180410878
    },
    {
      "type": "training",
      "description": "Training step 3655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:53",
      "total_flops_so_far": 2.1720508238462976e+16,
      "budget_used_percent": 21.720508238462976
    },
    {
      "type": "training",
      "description": "Training step 3656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:53",
      "total_flops_so_far": 2.172644929651507e+16,
      "budget_used_percent": 21.72644929651507
    },
    {
      "type": "training",
      "description": "Training step 3657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:53",
      "total_flops_so_far": 2.173239035456717e+16,
      "budget_used_percent": 21.73239035456717
    },
    {
      "type": "training",
      "description": "Training step 3658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:54",
      "total_flops_so_far": 2.1738331412619264e+16,
      "budget_used_percent": 21.738331412619264
    },
    {
      "type": "training",
      "description": "Training step 3659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:54",
      "total_flops_so_far": 2.174427247067136e+16,
      "budget_used_percent": 21.744272470671362
    },
    {
      "type": "training",
      "description": "Training step 3660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:55",
      "total_flops_so_far": 2.1750213528723456e+16,
      "budget_used_percent": 21.750213528723457
    },
    {
      "type": "training",
      "description": "Training step 3661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:55",
      "total_flops_so_far": 2.175615458677555e+16,
      "budget_used_percent": 21.75615458677555
    },
    {
      "type": "training",
      "description": "Training step 3662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:55",
      "total_flops_so_far": 2.176209564482765e+16,
      "budget_used_percent": 21.762095644827646
    },
    {
      "type": "training",
      "description": "Training step 3663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:56",
      "total_flops_so_far": 2.1768036702879744e+16,
      "budget_used_percent": 21.768036702879744
    },
    {
      "type": "training",
      "description": "Training step 3664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:56",
      "total_flops_so_far": 2.177397776093184e+16,
      "budget_used_percent": 21.77397776093184
    },
    {
      "type": "training",
      "description": "Training step 3665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:57",
      "total_flops_so_far": 2.1779918818983936e+16,
      "budget_used_percent": 21.779918818983937
    },
    {
      "type": "training",
      "description": "Training step 3666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:57",
      "total_flops_so_far": 2.178585987703603e+16,
      "budget_used_percent": 21.78585987703603
    },
    {
      "type": "training",
      "description": "Training step 3667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:58",
      "total_flops_so_far": 2.179180093508813e+16,
      "budget_used_percent": 21.79180093508813
    },
    {
      "type": "training",
      "description": "Training step 3668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:58",
      "total_flops_so_far": 2.1797741993140224e+16,
      "budget_used_percent": 21.797741993140225
    },
    {
      "type": "training",
      "description": "Training step 3669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:58",
      "total_flops_so_far": 2.180368305119232e+16,
      "budget_used_percent": 21.803683051192323
    },
    {
      "type": "training",
      "description": "Training step 3670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:59",
      "total_flops_so_far": 2.1809624109244416e+16,
      "budget_used_percent": 21.809624109244417
    },
    {
      "type": "training",
      "description": "Training step 3671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:55:59",
      "total_flops_so_far": 2.181556516729651e+16,
      "budget_used_percent": 21.815565167296512
    },
    {
      "type": "training",
      "description": "Training step 3672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:00",
      "total_flops_so_far": 2.182150622534861e+16,
      "budget_used_percent": 21.821506225348607
    },
    {
      "type": "training",
      "description": "Training step 3673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:00",
      "total_flops_so_far": 2.1827447283400704e+16,
      "budget_used_percent": 21.8274472834007
    },
    {
      "type": "training",
      "description": "Training step 3674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:00",
      "total_flops_so_far": 2.18333883414528e+16,
      "budget_used_percent": 21.8333883414528
    },
    {
      "type": "training",
      "description": "Training step 3675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:01",
      "total_flops_so_far": 2.1839329399504896e+16,
      "budget_used_percent": 21.839329399504894
    },
    {
      "type": "training",
      "description": "Training step 3676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:01",
      "total_flops_so_far": 2.184527045755699e+16,
      "budget_used_percent": 21.845270457556992
    },
    {
      "type": "training",
      "description": "Training step 3677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:02",
      "total_flops_so_far": 2.185121151560909e+16,
      "budget_used_percent": 21.851211515609087
    },
    {
      "type": "training",
      "description": "Training step 3678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:02",
      "total_flops_so_far": 2.1857152573661184e+16,
      "budget_used_percent": 21.857152573661185
    },
    {
      "type": "training",
      "description": "Training step 3679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:02",
      "total_flops_so_far": 2.186309363171328e+16,
      "budget_used_percent": 21.86309363171328
    },
    {
      "type": "training",
      "description": "Training step 3680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:03",
      "total_flops_so_far": 2.1869034689765376e+16,
      "budget_used_percent": 21.86903468976538
    },
    {
      "type": "training",
      "description": "Training step 3681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:03",
      "total_flops_so_far": 2.187497574781747e+16,
      "budget_used_percent": 21.874975747817473
    },
    {
      "type": "training",
      "description": "Training step 3682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:03",
      "total_flops_so_far": 2.188091680586957e+16,
      "budget_used_percent": 21.880916805869568
    },
    {
      "type": "training",
      "description": "Training step 3683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:04",
      "total_flops_so_far": 2.1886857863921664e+16,
      "budget_used_percent": 21.886857863921662
    },
    {
      "type": "training",
      "description": "Training step 3684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:04",
      "total_flops_so_far": 2.189279892197376e+16,
      "budget_used_percent": 21.89279892197376
    },
    {
      "type": "training",
      "description": "Training step 3685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:05",
      "total_flops_so_far": 2.1898739980025856e+16,
      "budget_used_percent": 21.898739980025855
    },
    {
      "type": "training",
      "description": "Training step 3686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:05",
      "total_flops_so_far": 2.190468103807795e+16,
      "budget_used_percent": 21.904681038077953
    },
    {
      "type": "training",
      "description": "Training step 3687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:05",
      "total_flops_so_far": 2.191062209613005e+16,
      "budget_used_percent": 21.910622096130048
    },
    {
      "type": "training",
      "description": "Training step 3688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:06",
      "total_flops_so_far": 2.1916563154182144e+16,
      "budget_used_percent": 21.916563154182146
    },
    {
      "type": "training",
      "description": "Training step 3689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:06",
      "total_flops_so_far": 2.192250421223424e+16,
      "budget_used_percent": 21.92250421223424
    },
    {
      "type": "training",
      "description": "Training step 3690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:07",
      "total_flops_so_far": 2.1928445270286336e+16,
      "budget_used_percent": 21.928445270286336
    },
    {
      "type": "training",
      "description": "Training step 3691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:07",
      "total_flops_so_far": 2.193438632833843e+16,
      "budget_used_percent": 21.93438632833843
    },
    {
      "type": "training",
      "description": "Training step 3692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:07",
      "total_flops_so_far": 2.194032738639053e+16,
      "budget_used_percent": 21.94032738639053
    },
    {
      "type": "training",
      "description": "Training step 3693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:08",
      "total_flops_so_far": 2.1946268444442624e+16,
      "budget_used_percent": 21.946268444442623
    },
    {
      "type": "training",
      "description": "Training step 3694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:08",
      "total_flops_so_far": 2.195220950249472e+16,
      "budget_used_percent": 21.95220950249472
    },
    {
      "type": "training",
      "description": "Training step 3695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:09",
      "total_flops_so_far": 2.1958150560546816e+16,
      "budget_used_percent": 21.958150560546816
    },
    {
      "type": "training",
      "description": "Training step 3696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:09",
      "total_flops_so_far": 2.196409161859891e+16,
      "budget_used_percent": 21.964091618598914
    },
    {
      "type": "training",
      "description": "Training step 3697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:09",
      "total_flops_so_far": 2.197003267665101e+16,
      "budget_used_percent": 21.97003267665101
    },
    {
      "type": "training",
      "description": "Training step 3698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:10",
      "total_flops_so_far": 2.1975973734703104e+16,
      "budget_used_percent": 21.975973734703107
    },
    {
      "type": "training",
      "description": "Training step 3699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:10",
      "total_flops_so_far": 2.19819147927552e+16,
      "budget_used_percent": 21.9819147927552
    },
    {
      "type": "training",
      "description": "Training step 3700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:11",
      "total_flops_so_far": 2.1987855850807296e+16,
      "budget_used_percent": 21.987855850807296
    },
    {
      "type": "training",
      "description": "Training step 3701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:11",
      "total_flops_so_far": 2.199379690885939e+16,
      "budget_used_percent": 21.99379690885939
    },
    {
      "type": "training",
      "description": "Training step 3702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:11",
      "total_flops_so_far": 2.199973796691149e+16,
      "budget_used_percent": 21.999737966911486
    },
    {
      "type": "training",
      "description": "Training step 3703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:12",
      "total_flops_so_far": 2.2005679024963584e+16,
      "budget_used_percent": 22.005679024963584
    },
    {
      "type": "training",
      "description": "Training step 3704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:12",
      "total_flops_so_far": 2.201162008301568e+16,
      "budget_used_percent": 22.01162008301568
    },
    {
      "type": "training",
      "description": "Training step 3705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:13",
      "total_flops_so_far": 2.2017561141067776e+16,
      "budget_used_percent": 22.017561141067777
    },
    {
      "type": "training",
      "description": "Training step 3706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:13",
      "total_flops_so_far": 2.202350219911987e+16,
      "budget_used_percent": 22.02350219911987
    },
    {
      "type": "training",
      "description": "Training step 3707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:13",
      "total_flops_so_far": 2.202944325717197e+16,
      "budget_used_percent": 22.02944325717197
    },
    {
      "type": "training",
      "description": "Training step 3708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:14",
      "total_flops_so_far": 2.2035384315224064e+16,
      "budget_used_percent": 22.035384315224064
    },
    {
      "type": "training",
      "description": "Training step 3709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:14",
      "total_flops_so_far": 2.204132537327616e+16,
      "budget_used_percent": 22.04132537327616
    },
    {
      "type": "training",
      "description": "Training step 3710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:14",
      "total_flops_so_far": 2.2047266431328256e+16,
      "budget_used_percent": 22.047266431328254
    },
    {
      "type": "training",
      "description": "Training step 3711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:15",
      "total_flops_so_far": 2.205320748938035e+16,
      "budget_used_percent": 22.053207489380352
    },
    {
      "type": "training",
      "description": "Training step 3712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:15",
      "total_flops_so_far": 2.205914854743245e+16,
      "budget_used_percent": 22.059148547432446
    },
    {
      "type": "training",
      "description": "Training step 3713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:16",
      "total_flops_so_far": 2.2065089605484544e+16,
      "budget_used_percent": 22.065089605484545
    },
    {
      "type": "training",
      "description": "Training step 3714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:16",
      "total_flops_so_far": 2.207103066353664e+16,
      "budget_used_percent": 22.07103066353664
    },
    {
      "type": "training",
      "description": "Training step 3715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:16",
      "total_flops_so_far": 2.2076971721588736e+16,
      "budget_used_percent": 22.076971721588738
    },
    {
      "type": "training",
      "description": "Training step 3716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:17",
      "total_flops_so_far": 2.208291277964083e+16,
      "budget_used_percent": 22.082912779640832
    },
    {
      "type": "training",
      "description": "Training step 3717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:17",
      "total_flops_so_far": 2.208885383769293e+16,
      "budget_used_percent": 22.08885383769293
    },
    {
      "type": "training",
      "description": "Training step 3718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:18",
      "total_flops_so_far": 2.2094794895745024e+16,
      "budget_used_percent": 22.094794895745025
    },
    {
      "type": "training",
      "description": "Training step 3719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:18",
      "total_flops_so_far": 2.210073595379712e+16,
      "budget_used_percent": 22.10073595379712
    },
    {
      "type": "training",
      "description": "Training step 3720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:19",
      "total_flops_so_far": 2.2106677011849216e+16,
      "budget_used_percent": 22.106677011849214
    },
    {
      "type": "training",
      "description": "Training step 3721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:19",
      "total_flops_so_far": 2.211261806990131e+16,
      "budget_used_percent": 22.112618069901313
    },
    {
      "type": "training",
      "description": "Training step 3722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:20",
      "total_flops_so_far": 2.211855912795341e+16,
      "budget_used_percent": 22.118559127953407
    },
    {
      "type": "training",
      "description": "Training step 3723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:20",
      "total_flops_so_far": 2.2124500186005504e+16,
      "budget_used_percent": 22.124500186005505
    },
    {
      "type": "training",
      "description": "Training step 3724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:20",
      "total_flops_so_far": 2.21304412440576e+16,
      "budget_used_percent": 22.1304412440576
    },
    {
      "type": "training",
      "description": "Training step 3725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:21",
      "total_flops_so_far": 2.2136382302109696e+16,
      "budget_used_percent": 22.1363823021097
    },
    {
      "type": "training",
      "description": "Training step 3726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:21",
      "total_flops_so_far": 2.214232336016179e+16,
      "budget_used_percent": 22.142323360161793
    },
    {
      "type": "training",
      "description": "Training step 3727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:21",
      "total_flops_so_far": 2.214826441821389e+16,
      "budget_used_percent": 22.148264418213888
    },
    {
      "type": "training",
      "description": "Training step 3728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:22",
      "total_flops_so_far": 2.2154205476265984e+16,
      "budget_used_percent": 22.154205476265982
    },
    {
      "type": "training",
      "description": "Training step 3729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:22",
      "total_flops_so_far": 2.216014653431808e+16,
      "budget_used_percent": 22.160146534318077
    },
    {
      "type": "training",
      "description": "Training step 3730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:23",
      "total_flops_so_far": 2.2166087592370176e+16,
      "budget_used_percent": 22.166087592370175
    },
    {
      "type": "training",
      "description": "Training step 3731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:23",
      "total_flops_so_far": 2.217202865042227e+16,
      "budget_used_percent": 22.17202865042227
    },
    {
      "type": "training",
      "description": "Training step 3732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:23",
      "total_flops_so_far": 2.217796970847437e+16,
      "budget_used_percent": 22.177969708474368
    },
    {
      "type": "training",
      "description": "Training step 3733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:24",
      "total_flops_so_far": 2.2183910766526464e+16,
      "budget_used_percent": 22.183910766526463
    },
    {
      "type": "training",
      "description": "Training step 3734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:24",
      "total_flops_so_far": 2.218985182457856e+16,
      "budget_used_percent": 22.18985182457856
    },
    {
      "type": "training",
      "description": "Training step 3735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:25",
      "total_flops_so_far": 2.2195792882630656e+16,
      "budget_used_percent": 22.195792882630656
    },
    {
      "type": "training",
      "description": "Training step 3736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:25",
      "total_flops_so_far": 2.220173394068275e+16,
      "budget_used_percent": 22.201733940682754
    },
    {
      "type": "training",
      "description": "Training step 3737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:25",
      "total_flops_so_far": 2.220767499873485e+16,
      "budget_used_percent": 22.20767499873485
    },
    {
      "type": "training",
      "description": "Training step 3738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:26",
      "total_flops_so_far": 2.2213616056786944e+16,
      "budget_used_percent": 22.213616056786943
    },
    {
      "type": "training",
      "description": "Training step 3739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:26",
      "total_flops_so_far": 2.221955711483904e+16,
      "budget_used_percent": 22.219557114839038
    },
    {
      "type": "training",
      "description": "Training step 3740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:27",
      "total_flops_so_far": 2.2225498172891136e+16,
      "budget_used_percent": 22.225498172891136
    },
    {
      "type": "training",
      "description": "Training step 3741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:27",
      "total_flops_so_far": 2.223143923094323e+16,
      "budget_used_percent": 22.23143923094323
    },
    {
      "type": "training",
      "description": "Training step 3742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:27",
      "total_flops_so_far": 2.223738028899533e+16,
      "budget_used_percent": 22.23738028899533
    },
    {
      "type": "training",
      "description": "Training step 3743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:28",
      "total_flops_so_far": 2.2243321347047424e+16,
      "budget_used_percent": 22.243321347047424
    },
    {
      "type": "training",
      "description": "Training step 3744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:28",
      "total_flops_so_far": 2.224926240509952e+16,
      "budget_used_percent": 22.249262405099522
    },
    {
      "type": "training",
      "description": "Training step 3745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:29",
      "total_flops_so_far": 2.2255203463151616e+16,
      "budget_used_percent": 22.255203463151616
    },
    {
      "type": "training",
      "description": "Training step 3746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:29",
      "total_flops_so_far": 2.226114452120371e+16,
      "budget_used_percent": 22.26114452120371
    },
    {
      "type": "training",
      "description": "Training step 3747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:29",
      "total_flops_so_far": 2.226708557925581e+16,
      "budget_used_percent": 22.267085579255806
    },
    {
      "type": "training",
      "description": "Training step 3748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:30",
      "total_flops_so_far": 2.2273026637307904e+16,
      "budget_used_percent": 22.273026637307904
    },
    {
      "type": "training",
      "description": "Training step 3749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:30",
      "total_flops_so_far": 2.227896769536e+16,
      "budget_used_percent": 22.27896769536
    },
    {
      "type": "training",
      "description": "Training step 3750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:31",
      "total_flops_so_far": 2.2284908753412096e+16,
      "budget_used_percent": 22.284908753412097
    },
    {
      "type": "training",
      "description": "Training step 3751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:31",
      "total_flops_so_far": 2.229084981146419e+16,
      "budget_used_percent": 22.29084981146419
    },
    {
      "type": "training",
      "description": "Training step 3752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:31",
      "total_flops_so_far": 2.229679086951629e+16,
      "budget_used_percent": 22.29679086951629
    },
    {
      "type": "training",
      "description": "Training step 3753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:32",
      "total_flops_so_far": 2.2302731927568384e+16,
      "budget_used_percent": 22.302731927568384
    },
    {
      "type": "training",
      "description": "Training step 3754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:32",
      "total_flops_so_far": 2.230867298562048e+16,
      "budget_used_percent": 22.308672985620483
    },
    {
      "type": "training",
      "description": "Training step 3755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:33",
      "total_flops_so_far": 2.2314614043672576e+16,
      "budget_used_percent": 22.314614043672577
    },
    {
      "type": "training",
      "description": "Training step 3756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:33",
      "total_flops_so_far": 2.232055510172467e+16,
      "budget_used_percent": 22.320555101724672
    },
    {
      "type": "training",
      "description": "Training step 3757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:33",
      "total_flops_so_far": 2.232649615977677e+16,
      "budget_used_percent": 22.326496159776767
    },
    {
      "type": "training",
      "description": "Training step 3758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:34",
      "total_flops_so_far": 2.2332437217828864e+16,
      "budget_used_percent": 22.33243721782886
    },
    {
      "type": "training",
      "description": "Training step 3759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:34",
      "total_flops_so_far": 2.233837827588096e+16,
      "budget_used_percent": 22.33837827588096
    },
    {
      "type": "training",
      "description": "Training step 3760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:34",
      "total_flops_so_far": 2.2344319333933056e+16,
      "budget_used_percent": 22.344319333933054
    },
    {
      "type": "training",
      "description": "Training step 3761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:35",
      "total_flops_so_far": 2.235026039198515e+16,
      "budget_used_percent": 22.350260391985152
    },
    {
      "type": "training",
      "description": "Training step 3762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:35",
      "total_flops_so_far": 2.235620145003725e+16,
      "budget_used_percent": 22.356201450037247
    },
    {
      "type": "training",
      "description": "Training step 3763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:36",
      "total_flops_so_far": 2.2362142508089344e+16,
      "budget_used_percent": 22.362142508089345
    },
    {
      "type": "training",
      "description": "Training step 3764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:36",
      "total_flops_so_far": 2.236808356614144e+16,
      "budget_used_percent": 22.36808356614144
    },
    {
      "type": "training",
      "description": "Training step 3765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:36",
      "total_flops_so_far": 2.2374024624193536e+16,
      "budget_used_percent": 22.374024624193538
    },
    {
      "type": "training",
      "description": "Training step 3766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:37",
      "total_flops_so_far": 2.237996568224563e+16,
      "budget_used_percent": 22.379965682245633
    },
    {
      "type": "training",
      "description": "Training step 3767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:37",
      "total_flops_so_far": 2.238590674029773e+16,
      "budget_used_percent": 22.385906740297727
    },
    {
      "type": "training",
      "description": "Training step 3768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:38",
      "total_flops_so_far": 2.2391847798349824e+16,
      "budget_used_percent": 22.391847798349822
    },
    {
      "type": "training",
      "description": "Training step 3769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:38",
      "total_flops_so_far": 2.239778885640192e+16,
      "budget_used_percent": 22.39778885640192
    },
    {
      "type": "training",
      "description": "Training step 3770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:38",
      "total_flops_so_far": 2.2403729914454016e+16,
      "budget_used_percent": 22.403729914454015
    },
    {
      "type": "training",
      "description": "Training step 3771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:39",
      "total_flops_so_far": 2.240967097250611e+16,
      "budget_used_percent": 22.409670972506113
    },
    {
      "type": "training",
      "description": "Training step 3772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:39",
      "total_flops_so_far": 2.241561203055821e+16,
      "budget_used_percent": 22.415612030558208
    },
    {
      "type": "training",
      "description": "Training step 3773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:40",
      "total_flops_so_far": 2.2421553088610304e+16,
      "budget_used_percent": 22.421553088610306
    },
    {
      "type": "training",
      "description": "Training step 3774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:40",
      "total_flops_so_far": 2.24274941466624e+16,
      "budget_used_percent": 22.4274941466624
    },
    {
      "type": "training",
      "description": "Training step 3775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:40",
      "total_flops_so_far": 2.2433435204714496e+16,
      "budget_used_percent": 22.433435204714495
    },
    {
      "type": "training",
      "description": "Training step 3776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:41",
      "total_flops_so_far": 2.243937626276659e+16,
      "budget_used_percent": 22.43937626276659
    },
    {
      "type": "training",
      "description": "Training step 3777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:41",
      "total_flops_so_far": 2.244531732081869e+16,
      "budget_used_percent": 22.44531732081869
    },
    {
      "type": "training",
      "description": "Training step 3778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:42",
      "total_flops_so_far": 2.2451258378870784e+16,
      "budget_used_percent": 22.451258378870783
    },
    {
      "type": "training",
      "description": "Training step 3779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:42",
      "total_flops_so_far": 2.245719943692288e+16,
      "budget_used_percent": 22.45719943692288
    },
    {
      "type": "training",
      "description": "Training step 3780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:42",
      "total_flops_so_far": 2.2463140494974976e+16,
      "budget_used_percent": 22.463140494974976
    },
    {
      "type": "training",
      "description": "Training step 3781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:43",
      "total_flops_so_far": 2.246908155302707e+16,
      "budget_used_percent": 22.469081553027074
    },
    {
      "type": "training",
      "description": "Training step 3782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:43",
      "total_flops_so_far": 2.247502261107917e+16,
      "budget_used_percent": 22.47502261107917
    },
    {
      "type": "training",
      "description": "Training step 3783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:44",
      "total_flops_so_far": 2.2480963669131264e+16,
      "budget_used_percent": 22.480963669131267
    },
    {
      "type": "training",
      "description": "Training step 3784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:44",
      "total_flops_so_far": 2.248690472718336e+16,
      "budget_used_percent": 22.48690472718336
    },
    {
      "type": "training",
      "description": "Training step 3785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:45",
      "total_flops_so_far": 2.2492845785235456e+16,
      "budget_used_percent": 22.492845785235456
    },
    {
      "type": "training",
      "description": "Training step 3786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:45",
      "total_flops_so_far": 2.249878684328755e+16,
      "budget_used_percent": 22.49878684328755
    },
    {
      "type": "training",
      "description": "Training step 3787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:45",
      "total_flops_so_far": 2.250472790133965e+16,
      "budget_used_percent": 22.504727901339646
    },
    {
      "type": "training",
      "description": "Training step 3788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:46",
      "total_flops_so_far": 2.2510668959391744e+16,
      "budget_used_percent": 22.510668959391744
    },
    {
      "type": "training",
      "description": "Training step 3789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:46",
      "total_flops_so_far": 2.251661001744384e+16,
      "budget_used_percent": 22.51661001744384
    },
    {
      "type": "training",
      "description": "Training step 3790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:47",
      "total_flops_so_far": 2.2522551075495936e+16,
      "budget_used_percent": 22.522551075495937
    },
    {
      "type": "training",
      "description": "Training step 3791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:47",
      "total_flops_so_far": 2.252849213354803e+16,
      "budget_used_percent": 22.52849213354803
    },
    {
      "type": "training",
      "description": "Training step 3792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:47",
      "total_flops_so_far": 2.253443319160013e+16,
      "budget_used_percent": 22.53443319160013
    },
    {
      "type": "training",
      "description": "Training step 3793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:48",
      "total_flops_so_far": 2.2540374249652224e+16,
      "budget_used_percent": 22.540374249652224
    },
    {
      "type": "training",
      "description": "Training step 3794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:48",
      "total_flops_so_far": 2.254631530770432e+16,
      "budget_used_percent": 22.54631530770432
    },
    {
      "type": "training",
      "description": "Training step 3795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:49",
      "total_flops_so_far": 2.2552256365756416e+16,
      "budget_used_percent": 22.552256365756413
    },
    {
      "type": "training",
      "description": "Training step 3796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:49",
      "total_flops_so_far": 2.255819742380851e+16,
      "budget_used_percent": 22.55819742380851
    },
    {
      "type": "training",
      "description": "Training step 3797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:49",
      "total_flops_so_far": 2.256413848186061e+16,
      "budget_used_percent": 22.564138481860606
    },
    {
      "type": "training",
      "description": "Training step 3798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:50",
      "total_flops_so_far": 2.2570079539912704e+16,
      "budget_used_percent": 22.570079539912705
    },
    {
      "type": "training",
      "description": "Training step 3799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:50",
      "total_flops_so_far": 2.25760205979648e+16,
      "budget_used_percent": 22.5760205979648
    },
    {
      "type": "training",
      "description": "Training step 3800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:51",
      "total_flops_so_far": 2.2581961656016896e+16,
      "budget_used_percent": 22.581961656016897
    },
    {
      "type": "training",
      "description": "Training step 3801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:51",
      "total_flops_so_far": 2.258790271406899e+16,
      "budget_used_percent": 22.587902714068992
    },
    {
      "type": "training",
      "description": "Training step 3802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:51",
      "total_flops_so_far": 2.259384377212109e+16,
      "budget_used_percent": 22.59384377212109
    },
    {
      "type": "training",
      "description": "Training step 3803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:52",
      "total_flops_so_far": 2.2599784830173184e+16,
      "budget_used_percent": 22.599784830173185
    },
    {
      "type": "training",
      "description": "Training step 3804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:52",
      "total_flops_so_far": 2.260572588822528e+16,
      "budget_used_percent": 22.60572588822528
    },
    {
      "type": "training",
      "description": "Training step 3805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:53",
      "total_flops_so_far": 2.2611666946277376e+16,
      "budget_used_percent": 22.611666946277374
    },
    {
      "type": "training",
      "description": "Training step 3806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:53",
      "total_flops_so_far": 2.261760800432947e+16,
      "budget_used_percent": 22.617608004329472
    },
    {
      "type": "training",
      "description": "Training step 3807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:53",
      "total_flops_so_far": 2.262354906238157e+16,
      "budget_used_percent": 22.623549062381567
    },
    {
      "type": "training",
      "description": "Training step 3808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:54",
      "total_flops_so_far": 2.2629490120433664e+16,
      "budget_used_percent": 22.629490120433665
    },
    {
      "type": "training",
      "description": "Training step 3809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:54",
      "total_flops_so_far": 2.263543117848576e+16,
      "budget_used_percent": 22.63543117848576
    },
    {
      "type": "training",
      "description": "Training step 3810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:55",
      "total_flops_so_far": 2.2641372236537856e+16,
      "budget_used_percent": 22.64137223653786
    },
    {
      "type": "training",
      "description": "Training step 3811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:55",
      "total_flops_so_far": 2.264731329458995e+16,
      "budget_used_percent": 22.647313294589953
    },
    {
      "type": "training",
      "description": "Training step 3812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:55",
      "total_flops_so_far": 2.265325435264205e+16,
      "budget_used_percent": 22.65325435264205
    },
    {
      "type": "training",
      "description": "Training step 3813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:56",
      "total_flops_so_far": 2.2659195410694144e+16,
      "budget_used_percent": 22.659195410694146
    },
    {
      "type": "training",
      "description": "Training step 3814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:56",
      "total_flops_so_far": 2.266513646874624e+16,
      "budget_used_percent": 22.66513646874624
    },
    {
      "type": "training",
      "description": "Training step 3815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:56",
      "total_flops_so_far": 2.2671077526798336e+16,
      "budget_used_percent": 22.671077526798335
    },
    {
      "type": "training",
      "description": "Training step 3816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:57",
      "total_flops_so_far": 2.267701858485043e+16,
      "budget_used_percent": 22.67701858485043
    },
    {
      "type": "training",
      "description": "Training step 3817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:57",
      "total_flops_so_far": 2.268295964290253e+16,
      "budget_used_percent": 22.682959642902528
    },
    {
      "type": "training",
      "description": "Training step 3818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:58",
      "total_flops_so_far": 2.2688900700954624e+16,
      "budget_used_percent": 22.688900700954623
    },
    {
      "type": "training",
      "description": "Training step 3819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:58",
      "total_flops_so_far": 2.269484175900672e+16,
      "budget_used_percent": 22.69484175900672
    },
    {
      "type": "training",
      "description": "Training step 3820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:58",
      "total_flops_so_far": 2.2700782817058816e+16,
      "budget_used_percent": 22.700782817058816
    },
    {
      "type": "training",
      "description": "Training step 3821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:59",
      "total_flops_so_far": 2.270672387511091e+16,
      "budget_used_percent": 22.706723875110914
    },
    {
      "type": "training",
      "description": "Training step 3822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:56:59",
      "total_flops_so_far": 2.271266493316301e+16,
      "budget_used_percent": 22.71266493316301
    },
    {
      "type": "training",
      "description": "Training step 3823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:00",
      "total_flops_so_far": 2.2718605991215104e+16,
      "budget_used_percent": 22.718605991215103
    },
    {
      "type": "training",
      "description": "Training step 3824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:00",
      "total_flops_so_far": 2.27245470492672e+16,
      "budget_used_percent": 22.724547049267198
    },
    {
      "type": "training",
      "description": "Training step 3825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:00",
      "total_flops_so_far": 2.2730488107319296e+16,
      "budget_used_percent": 22.730488107319296
    },
    {
      "type": "training",
      "description": "Training step 3826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:01",
      "total_flops_so_far": 2.273642916537139e+16,
      "budget_used_percent": 22.73642916537139
    },
    {
      "type": "training",
      "description": "Training step 3827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:01",
      "total_flops_so_far": 2.274237022342349e+16,
      "budget_used_percent": 22.74237022342349
    },
    {
      "type": "training",
      "description": "Training step 3828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:02",
      "total_flops_so_far": 2.2748311281475584e+16,
      "budget_used_percent": 22.748311281475583
    },
    {
      "type": "training",
      "description": "Training step 3829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:02",
      "total_flops_so_far": 2.275425233952768e+16,
      "budget_used_percent": 22.75425233952768
    },
    {
      "type": "training",
      "description": "Training step 3830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:02",
      "total_flops_so_far": 2.2760193397579776e+16,
      "budget_used_percent": 22.760193397579776
    },
    {
      "type": "training",
      "description": "Training step 3831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:03",
      "total_flops_so_far": 2.276613445563187e+16,
      "budget_used_percent": 22.766134455631875
    },
    {
      "type": "training",
      "description": "Training step 3832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:03",
      "total_flops_so_far": 2.277207551368397e+16,
      "budget_used_percent": 22.77207551368397
    },
    {
      "type": "training",
      "description": "Training step 3833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:04",
      "total_flops_so_far": 2.2778016571736064e+16,
      "budget_used_percent": 22.778016571736064
    },
    {
      "type": "training",
      "description": "Training step 3834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:04",
      "total_flops_so_far": 2.278395762978816e+16,
      "budget_used_percent": 22.78395762978816
    },
    {
      "type": "training",
      "description": "Training step 3835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:04",
      "total_flops_so_far": 2.2789898687840256e+16,
      "budget_used_percent": 22.789898687840257
    },
    {
      "type": "training",
      "description": "Training step 3836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:05",
      "total_flops_so_far": 2.279583974589235e+16,
      "budget_used_percent": 22.79583974589235
    },
    {
      "type": "training",
      "description": "Training step 3837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:05",
      "total_flops_so_far": 2.280178080394445e+16,
      "budget_used_percent": 22.80178080394445
    },
    {
      "type": "training",
      "description": "Training step 3838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:06",
      "total_flops_so_far": 2.2807721861996544e+16,
      "budget_used_percent": 22.807721861996544
    },
    {
      "type": "training",
      "description": "Training step 3839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:06",
      "total_flops_so_far": 2.281366292004864e+16,
      "budget_used_percent": 22.813662920048642
    },
    {
      "type": "training",
      "description": "Training step 3840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:07",
      "total_flops_so_far": 2.2819603978100736e+16,
      "budget_used_percent": 22.819603978100737
    },
    {
      "type": "training",
      "description": "Training step 3841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:07",
      "total_flops_so_far": 2.282554503615283e+16,
      "budget_used_percent": 22.825545036152832
    },
    {
      "type": "training",
      "description": "Training step 3842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:07",
      "total_flops_so_far": 2.283148609420493e+16,
      "budget_used_percent": 22.831486094204926
    },
    {
      "type": "training",
      "description": "Training step 3843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:08",
      "total_flops_so_far": 2.2837427152257024e+16,
      "budget_used_percent": 22.83742715225702
    },
    {
      "type": "training",
      "description": "Training step 3844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:08",
      "total_flops_so_far": 2.284336821030912e+16,
      "budget_used_percent": 22.84336821030912
    },
    {
      "type": "training",
      "description": "Training step 3845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:09",
      "total_flops_so_far": 2.2849309268361216e+16,
      "budget_used_percent": 22.849309268361214
    },
    {
      "type": "training",
      "description": "Training step 3846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:09",
      "total_flops_so_far": 2.285525032641331e+16,
      "budget_used_percent": 22.855250326413312
    },
    {
      "type": "training",
      "description": "Training step 3847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:09",
      "total_flops_so_far": 2.286119138446541e+16,
      "budget_used_percent": 22.861191384465407
    },
    {
      "type": "training",
      "description": "Training step 3848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:10",
      "total_flops_so_far": 2.2867132442517504e+16,
      "budget_used_percent": 22.867132442517505
    },
    {
      "type": "training",
      "description": "Training step 3849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:10",
      "total_flops_so_far": 2.28730735005696e+16,
      "budget_used_percent": 22.8730735005696
    },
    {
      "type": "training",
      "description": "Training step 3850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:11",
      "total_flops_so_far": 2.2879014558621696e+16,
      "budget_used_percent": 22.879014558621698
    },
    {
      "type": "training",
      "description": "Training step 3851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:11",
      "total_flops_so_far": 2.288495561667379e+16,
      "budget_used_percent": 22.884955616673793
    },
    {
      "type": "training",
      "description": "Training step 3852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:11",
      "total_flops_so_far": 2.289089667472589e+16,
      "budget_used_percent": 22.890896674725887
    },
    {
      "type": "training",
      "description": "Training step 3853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:12",
      "total_flops_so_far": 2.2896837732777984e+16,
      "budget_used_percent": 22.896837732777982
    },
    {
      "type": "training",
      "description": "Training step 3854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:12",
      "total_flops_so_far": 2.290277879083008e+16,
      "budget_used_percent": 22.90277879083008
    },
    {
      "type": "training",
      "description": "Training step 3855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:13",
      "total_flops_so_far": 2.2908719848882176e+16,
      "budget_used_percent": 22.908719848882175
    },
    {
      "type": "training",
      "description": "Training step 3856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:13",
      "total_flops_so_far": 2.291466090693427e+16,
      "budget_used_percent": 22.914660906934273
    },
    {
      "type": "training",
      "description": "Training step 3857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:13",
      "total_flops_so_far": 2.292060196498637e+16,
      "budget_used_percent": 22.920601964986368
    },
    {
      "type": "training",
      "description": "Training step 3858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:14",
      "total_flops_so_far": 2.2926543023038464e+16,
      "budget_used_percent": 22.926543023038466
    },
    {
      "type": "training",
      "description": "Training step 3859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:14",
      "total_flops_so_far": 2.293248408109056e+16,
      "budget_used_percent": 22.93248408109056
    },
    {
      "type": "training",
      "description": "Training step 3860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:15",
      "total_flops_so_far": 2.2938425139142656e+16,
      "budget_used_percent": 22.938425139142655
    },
    {
      "type": "training",
      "description": "Training step 3861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:15",
      "total_flops_so_far": 2.294436619719475e+16,
      "budget_used_percent": 22.94436619719475
    },
    {
      "type": "training",
      "description": "Training step 3862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:15",
      "total_flops_so_far": 2.295030725524685e+16,
      "budget_used_percent": 22.950307255246848
    },
    {
      "type": "training",
      "description": "Training step 3863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:16",
      "total_flops_so_far": 2.2956248313298944e+16,
      "budget_used_percent": 22.956248313298943
    },
    {
      "type": "training",
      "description": "Training step 3864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:16",
      "total_flops_so_far": 2.296218937135104e+16,
      "budget_used_percent": 22.96218937135104
    },
    {
      "type": "training",
      "description": "Training step 3865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:16",
      "total_flops_so_far": 2.2968130429403136e+16,
      "budget_used_percent": 22.968130429403136
    },
    {
      "type": "training",
      "description": "Training step 3866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:17",
      "total_flops_so_far": 2.297407148745523e+16,
      "budget_used_percent": 22.974071487455234
    },
    {
      "type": "training",
      "description": "Training step 3867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:17",
      "total_flops_so_far": 2.298001254550733e+16,
      "budget_used_percent": 22.98001254550733
    },
    {
      "type": "training",
      "description": "Training step 3868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:18",
      "total_flops_so_far": 2.2985953603559424e+16,
      "budget_used_percent": 22.985953603559427
    },
    {
      "type": "training",
      "description": "Training step 3869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:18",
      "total_flops_so_far": 2.299189466161152e+16,
      "budget_used_percent": 22.99189466161152
    },
    {
      "type": "training",
      "description": "Training step 3870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:18",
      "total_flops_so_far": 2.2997835719663616e+16,
      "budget_used_percent": 22.997835719663616
    },
    {
      "type": "training",
      "description": "Training step 3871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:19",
      "total_flops_so_far": 2.300377677771571e+16,
      "budget_used_percent": 23.00377677771571
    },
    {
      "type": "training",
      "description": "Training step 3872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:19",
      "total_flops_so_far": 2.300971783576781e+16,
      "budget_used_percent": 23.009717835767805
    },
    {
      "type": "training",
      "description": "Training step 3873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:20",
      "total_flops_so_far": 2.3015658893819904e+16,
      "budget_used_percent": 23.015658893819904
    },
    {
      "type": "training",
      "description": "Training step 3874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:20",
      "total_flops_so_far": 2.3021599951872e+16,
      "budget_used_percent": 23.021599951872
    },
    {
      "type": "training",
      "description": "Training step 3875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:20",
      "total_flops_so_far": 2.3027541009924096e+16,
      "budget_used_percent": 23.027541009924096
    },
    {
      "type": "training",
      "description": "Training step 3876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:21",
      "total_flops_so_far": 2.303348206797619e+16,
      "budget_used_percent": 23.03348206797619
    },
    {
      "type": "training",
      "description": "Training step 3877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:21",
      "total_flops_so_far": 2.303942312602829e+16,
      "budget_used_percent": 23.03942312602829
    },
    {
      "type": "training",
      "description": "Training step 3878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:22",
      "total_flops_so_far": 2.3045364184080384e+16,
      "budget_used_percent": 23.045364184080384
    },
    {
      "type": "training",
      "description": "Training step 3879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:22",
      "total_flops_so_far": 2.305130524213248e+16,
      "budget_used_percent": 23.05130524213248
    },
    {
      "type": "training",
      "description": "Training step 3880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:22",
      "total_flops_so_far": 2.3057246300184576e+16,
      "budget_used_percent": 23.057246300184573
    },
    {
      "type": "training",
      "description": "Training step 3881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:23",
      "total_flops_so_far": 2.306318735823667e+16,
      "budget_used_percent": 23.06318735823667
    },
    {
      "type": "training",
      "description": "Training step 3882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:23",
      "total_flops_so_far": 2.306912841628877e+16,
      "budget_used_percent": 23.069128416288766
    },
    {
      "type": "training",
      "description": "Training step 3883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:24",
      "total_flops_so_far": 2.3075069474340864e+16,
      "budget_used_percent": 23.075069474340864
    },
    {
      "type": "training",
      "description": "Training step 3884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:24",
      "total_flops_so_far": 2.308101053239296e+16,
      "budget_used_percent": 23.08101053239296
    },
    {
      "type": "training",
      "description": "Training step 3885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:24",
      "total_flops_so_far": 2.3086951590445056e+16,
      "budget_used_percent": 23.086951590445057
    },
    {
      "type": "training",
      "description": "Training step 3886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:25",
      "total_flops_so_far": 2.309289264849715e+16,
      "budget_used_percent": 23.092892648497152
    },
    {
      "type": "training",
      "description": "Training step 3887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:25",
      "total_flops_so_far": 2.309883370654925e+16,
      "budget_used_percent": 23.09883370654925
    },
    {
      "type": "training",
      "description": "Training step 3888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:26",
      "total_flops_so_far": 2.3104774764601344e+16,
      "budget_used_percent": 23.104774764601345
    },
    {
      "type": "training",
      "description": "Training step 3889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:26",
      "total_flops_so_far": 2.311071582265344e+16,
      "budget_used_percent": 23.11071582265344
    },
    {
      "type": "training",
      "description": "Training step 3890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:26",
      "total_flops_so_far": 2.3116656880705536e+16,
      "budget_used_percent": 23.116656880705534
    },
    {
      "type": "training",
      "description": "Training step 3891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:27",
      "total_flops_so_far": 2.312259793875763e+16,
      "budget_used_percent": 23.122597938757632
    },
    {
      "type": "training",
      "description": "Training step 3892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:27",
      "total_flops_so_far": 2.312853899680973e+16,
      "budget_used_percent": 23.128538996809727
    },
    {
      "type": "training",
      "description": "Training step 3893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:28",
      "total_flops_so_far": 2.3134480054861824e+16,
      "budget_used_percent": 23.134480054861825
    },
    {
      "type": "training",
      "description": "Training step 3894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:28",
      "total_flops_so_far": 2.314042111291392e+16,
      "budget_used_percent": 23.14042111291392
    },
    {
      "type": "training",
      "description": "Training step 3895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:29",
      "total_flops_so_far": 2.3146362170966016e+16,
      "budget_used_percent": 23.146362170966018
    },
    {
      "type": "training",
      "description": "Training step 3896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:29",
      "total_flops_so_far": 2.315230322901811e+16,
      "budget_used_percent": 23.152303229018113
    },
    {
      "type": "training",
      "description": "Training step 3897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:30",
      "total_flops_so_far": 2.315824428707021e+16,
      "budget_used_percent": 23.15824428707021
    },
    {
      "type": "training",
      "description": "Training step 3898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:30",
      "total_flops_so_far": 2.3164185345122304e+16,
      "budget_used_percent": 23.164185345122306
    },
    {
      "type": "training",
      "description": "Training step 3899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:30",
      "total_flops_so_far": 2.31701264031744e+16,
      "budget_used_percent": 23.1701264031744
    },
    {
      "type": "training",
      "description": "Training step 3900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:31",
      "total_flops_so_far": 2.3176067461226496e+16,
      "budget_used_percent": 23.176067461226495
    },
    {
      "type": "training",
      "description": "Training step 3901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:31",
      "total_flops_so_far": 2.318200851927859e+16,
      "budget_used_percent": 23.18200851927859
    },
    {
      "type": "training",
      "description": "Training step 3902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:32",
      "total_flops_so_far": 2.318794957733069e+16,
      "budget_used_percent": 23.187949577330688
    },
    {
      "type": "training",
      "description": "Training step 3903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:32",
      "total_flops_so_far": 2.3193890635382784e+16,
      "budget_used_percent": 23.193890635382783
    },
    {
      "type": "training",
      "description": "Training step 3904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:32",
      "total_flops_so_far": 2.319983169343488e+16,
      "budget_used_percent": 23.19983169343488
    },
    {
      "type": "training",
      "description": "Training step 3905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:33",
      "total_flops_so_far": 2.3205772751486976e+16,
      "budget_used_percent": 23.205772751486975
    },
    {
      "type": "training",
      "description": "Training step 3906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:33",
      "total_flops_so_far": 2.321171380953907e+16,
      "budget_used_percent": 23.211713809539074
    },
    {
      "type": "training",
      "description": "Training step 3907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:33",
      "total_flops_so_far": 2.321765486759117e+16,
      "budget_used_percent": 23.21765486759117
    },
    {
      "type": "training",
      "description": "Training step 3908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:34",
      "total_flops_so_far": 2.3223595925643264e+16,
      "budget_used_percent": 23.223595925643263
    },
    {
      "type": "training",
      "description": "Training step 3909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:34",
      "total_flops_so_far": 2.322953698369536e+16,
      "budget_used_percent": 23.229536983695358
    },
    {
      "type": "training",
      "description": "Training step 3910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:35",
      "total_flops_so_far": 2.3235478041747456e+16,
      "budget_used_percent": 23.235478041747456
    },
    {
      "type": "training",
      "description": "Training step 3911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:35",
      "total_flops_so_far": 2.324141909979955e+16,
      "budget_used_percent": 23.24141909979955
    },
    {
      "type": "training",
      "description": "Training step 3912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:35",
      "total_flops_so_far": 2.324736015785165e+16,
      "budget_used_percent": 23.24736015785165
    },
    {
      "type": "training",
      "description": "Training step 3913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:36",
      "total_flops_so_far": 2.3253301215903744e+16,
      "budget_used_percent": 23.253301215903743
    },
    {
      "type": "training",
      "description": "Training step 3914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:36",
      "total_flops_so_far": 2.325924227395584e+16,
      "budget_used_percent": 23.25924227395584
    },
    {
      "type": "training",
      "description": "Training step 3915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:37",
      "total_flops_so_far": 2.3265183332007936e+16,
      "budget_used_percent": 23.265183332007936
    },
    {
      "type": "training",
      "description": "Training step 3916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:37",
      "total_flops_so_far": 2.327112439006003e+16,
      "budget_used_percent": 23.271124390060034
    },
    {
      "type": "training",
      "description": "Training step 3917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:37",
      "total_flops_so_far": 2.327706544811213e+16,
      "budget_used_percent": 23.27706544811213
    },
    {
      "type": "training",
      "description": "Training step 3918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:38",
      "total_flops_so_far": 2.3283006506164224e+16,
      "budget_used_percent": 23.283006506164224
    },
    {
      "type": "training",
      "description": "Training step 3919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:38",
      "total_flops_so_far": 2.328894756421632e+16,
      "budget_used_percent": 23.28894756421632
    },
    {
      "type": "training",
      "description": "Training step 3920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:39",
      "total_flops_so_far": 2.3294888622268416e+16,
      "budget_used_percent": 23.294888622268417
    },
    {
      "type": "training",
      "description": "Training step 3921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:39",
      "total_flops_so_far": 2.330082968032051e+16,
      "budget_used_percent": 23.30082968032051
    },
    {
      "type": "training",
      "description": "Training step 3922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:39",
      "total_flops_so_far": 2.330677073837261e+16,
      "budget_used_percent": 23.30677073837261
    },
    {
      "type": "training",
      "description": "Training step 3923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:40",
      "total_flops_so_far": 2.3312711796424704e+16,
      "budget_used_percent": 23.312711796424704
    },
    {
      "type": "training",
      "description": "Training step 3924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:40",
      "total_flops_so_far": 2.33186528544768e+16,
      "budget_used_percent": 23.318652854476802
    },
    {
      "type": "training",
      "description": "Training step 3925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:41",
      "total_flops_so_far": 2.3324593912528896e+16,
      "budget_used_percent": 23.324593912528897
    },
    {
      "type": "training",
      "description": "Training step 3926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:41",
      "total_flops_so_far": 2.333053497058099e+16,
      "budget_used_percent": 23.33053497058099
    },
    {
      "type": "training",
      "description": "Training step 3927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:41",
      "total_flops_so_far": 2.333647602863309e+16,
      "budget_used_percent": 23.336476028633086
    },
    {
      "type": "training",
      "description": "Training step 3928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:42",
      "total_flops_so_far": 2.3342417086685184e+16,
      "budget_used_percent": 23.34241708668518
    },
    {
      "type": "training",
      "description": "Training step 3929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:42",
      "total_flops_so_far": 2.334835814473728e+16,
      "budget_used_percent": 23.34835814473728
    },
    {
      "type": "training",
      "description": "Training step 3930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:43",
      "total_flops_so_far": 2.3354299202789376e+16,
      "budget_used_percent": 23.354299202789374
    },
    {
      "type": "training",
      "description": "Training step 3931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:43",
      "total_flops_so_far": 2.336024026084147e+16,
      "budget_used_percent": 23.360240260841472
    },
    {
      "type": "training",
      "description": "Training step 3932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:43",
      "total_flops_so_far": 2.336618131889357e+16,
      "budget_used_percent": 23.366181318893567
    },
    {
      "type": "training",
      "description": "Training step 3933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:44",
      "total_flops_so_far": 2.3372122376945664e+16,
      "budget_used_percent": 23.372122376945665
    },
    {
      "type": "training",
      "description": "Training step 3934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:44",
      "total_flops_so_far": 2.337806343499776e+16,
      "budget_used_percent": 23.37806343499776
    },
    {
      "type": "training",
      "description": "Training step 3935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:45",
      "total_flops_so_far": 2.3384004493049856e+16,
      "budget_used_percent": 23.384004493049858
    },
    {
      "type": "training",
      "description": "Training step 3936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:45",
      "total_flops_so_far": 2.338994555110195e+16,
      "budget_used_percent": 23.389945551101953
    },
    {
      "type": "training",
      "description": "Training step 3937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:45",
      "total_flops_so_far": 2.339588660915405e+16,
      "budget_used_percent": 23.395886609154047
    },
    {
      "type": "training",
      "description": "Training step 3938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:46",
      "total_flops_so_far": 2.3401827667206144e+16,
      "budget_used_percent": 23.401827667206142
    },
    {
      "type": "training",
      "description": "Training step 3939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:46",
      "total_flops_so_far": 2.340776872525824e+16,
      "budget_used_percent": 23.40776872525824
    },
    {
      "type": "training",
      "description": "Training step 3940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:47",
      "total_flops_so_far": 2.3413709783310336e+16,
      "budget_used_percent": 23.413709783310335
    },
    {
      "type": "training",
      "description": "Training step 3941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:47",
      "total_flops_so_far": 2.341965084136243e+16,
      "budget_used_percent": 23.419650841362433
    },
    {
      "type": "training",
      "description": "Training step 3942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:47",
      "total_flops_so_far": 2.342559189941453e+16,
      "budget_used_percent": 23.425591899414528
    },
    {
      "type": "training",
      "description": "Training step 3943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:48",
      "total_flops_so_far": 2.3431532957466624e+16,
      "budget_used_percent": 23.431532957466626
    },
    {
      "type": "training",
      "description": "Training step 3944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:49",
      "total_flops_so_far": 2.343747401551872e+16,
      "budget_used_percent": 23.43747401551872
    },
    {
      "type": "training",
      "description": "Training step 3945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:49",
      "total_flops_so_far": 2.3443415073570816e+16,
      "budget_used_percent": 23.44341507357082
    },
    {
      "type": "training",
      "description": "Training step 3946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:49",
      "total_flops_so_far": 2.344935613162291e+16,
      "budget_used_percent": 23.449356131622913
    },
    {
      "type": "training",
      "description": "Training step 3947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:50",
      "total_flops_so_far": 2.345529718967501e+16,
      "budget_used_percent": 23.455297189675008
    },
    {
      "type": "training",
      "description": "Training step 3948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:50",
      "total_flops_so_far": 2.3461238247727104e+16,
      "budget_used_percent": 23.461238247727103
    },
    {
      "type": "training",
      "description": "Training step 3949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:50",
      "total_flops_so_far": 2.34671793057792e+16,
      "budget_used_percent": 23.4671793057792
    },
    {
      "type": "training",
      "description": "Training step 3950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:51",
      "total_flops_so_far": 2.3473120363831296e+16,
      "budget_used_percent": 23.473120363831296
    },
    {
      "type": "training",
      "description": "Training step 3951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:51",
      "total_flops_so_far": 2.347906142188339e+16,
      "budget_used_percent": 23.479061421883394
    },
    {
      "type": "training",
      "description": "Training step 3952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:52",
      "total_flops_so_far": 2.348500247993549e+16,
      "budget_used_percent": 23.48500247993549
    },
    {
      "type": "training",
      "description": "Training step 3953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:52",
      "total_flops_so_far": 2.3490943537987584e+16,
      "budget_used_percent": 23.490943537987587
    },
    {
      "type": "training",
      "description": "Training step 3954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:52",
      "total_flops_so_far": 2.349688459603968e+16,
      "budget_used_percent": 23.49688459603968
    },
    {
      "type": "training",
      "description": "Training step 3955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:53",
      "total_flops_so_far": 2.3502825654091776e+16,
      "budget_used_percent": 23.502825654091776
    },
    {
      "type": "training",
      "description": "Training step 3956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:53",
      "total_flops_so_far": 2.350876671214387e+16,
      "budget_used_percent": 23.50876671214387
    },
    {
      "type": "training",
      "description": "Training step 3957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:54",
      "total_flops_so_far": 2.351470777019597e+16,
      "budget_used_percent": 23.514707770195965
    },
    {
      "type": "training",
      "description": "Training step 3958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:54",
      "total_flops_so_far": 2.3520648828248064e+16,
      "budget_used_percent": 23.520648828248063
    },
    {
      "type": "training",
      "description": "Training step 3959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:54",
      "total_flops_so_far": 2.352658988630016e+16,
      "budget_used_percent": 23.526589886300158
    },
    {
      "type": "training",
      "description": "Training step 3960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:55",
      "total_flops_so_far": 2.3532530944352256e+16,
      "budget_used_percent": 23.532530944352256
    },
    {
      "type": "training",
      "description": "Training step 3961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:55",
      "total_flops_so_far": 2.353847200240435e+16,
      "budget_used_percent": 23.53847200240435
    },
    {
      "type": "training",
      "description": "Training step 3962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:56",
      "total_flops_so_far": 2.354441306045645e+16,
      "budget_used_percent": 23.54441306045645
    },
    {
      "type": "training",
      "description": "Training step 3963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:56",
      "total_flops_so_far": 2.3550354118508544e+16,
      "budget_used_percent": 23.550354118508544
    },
    {
      "type": "training",
      "description": "Training step 3964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:56",
      "total_flops_so_far": 2.355629517656064e+16,
      "budget_used_percent": 23.556295176560642
    },
    {
      "type": "training",
      "description": "Training step 3965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:57",
      "total_flops_so_far": 2.3562236234612736e+16,
      "budget_used_percent": 23.562236234612737
    },
    {
      "type": "training",
      "description": "Training step 3966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:57",
      "total_flops_so_far": 2.356817729266483e+16,
      "budget_used_percent": 23.56817729266483
    },
    {
      "type": "training",
      "description": "Training step 3967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:58",
      "total_flops_so_far": 2.357411835071693e+16,
      "budget_used_percent": 23.574118350716926
    },
    {
      "type": "training",
      "description": "Training step 3968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:58",
      "total_flops_so_far": 2.3580059408769024e+16,
      "budget_used_percent": 23.580059408769024
    },
    {
      "type": "training",
      "description": "Training step 3969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:58",
      "total_flops_so_far": 2.358600046682112e+16,
      "budget_used_percent": 23.58600046682112
    },
    {
      "type": "training",
      "description": "Training step 3970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:59",
      "total_flops_so_far": 2.3591941524873216e+16,
      "budget_used_percent": 23.591941524873217
    },
    {
      "type": "training",
      "description": "Training step 3971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:57:59",
      "total_flops_so_far": 2.359788258292531e+16,
      "budget_used_percent": 23.597882582925312
    },
    {
      "type": "training",
      "description": "Training step 3972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:00",
      "total_flops_so_far": 2.360382364097741e+16,
      "budget_used_percent": 23.60382364097741
    },
    {
      "type": "training",
      "description": "Training step 3973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:00",
      "total_flops_so_far": 2.3609764699029504e+16,
      "budget_used_percent": 23.609764699029505
    },
    {
      "type": "training",
      "description": "Training step 3974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:00",
      "total_flops_so_far": 2.36157057570816e+16,
      "budget_used_percent": 23.6157057570816
    },
    {
      "type": "training",
      "description": "Training step 3975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:01",
      "total_flops_so_far": 2.3621646815133696e+16,
      "budget_used_percent": 23.621646815133694
    },
    {
      "type": "training",
      "description": "Training step 3976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:01",
      "total_flops_so_far": 2.362758787318579e+16,
      "budget_used_percent": 23.627587873185792
    },
    {
      "type": "training",
      "description": "Training step 3977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:02",
      "total_flops_so_far": 2.363352893123789e+16,
      "budget_used_percent": 23.633528931237887
    },
    {
      "type": "training",
      "description": "Training step 3978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:02",
      "total_flops_so_far": 2.3639469989289984e+16,
      "budget_used_percent": 23.639469989289985
    },
    {
      "type": "training",
      "description": "Training step 3979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:02",
      "total_flops_so_far": 2.364541104734208e+16,
      "budget_used_percent": 23.64541104734208
    },
    {
      "type": "training",
      "description": "Training step 3980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:03",
      "total_flops_so_far": 2.3651352105394176e+16,
      "budget_used_percent": 23.651352105394178
    },
    {
      "type": "training",
      "description": "Training step 3981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:03",
      "total_flops_so_far": 2.365729316344627e+16,
      "budget_used_percent": 23.657293163446273
    },
    {
      "type": "training",
      "description": "Training step 3982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:04",
      "total_flops_so_far": 2.366323422149837e+16,
      "budget_used_percent": 23.66323422149837
    },
    {
      "type": "training",
      "description": "Training step 3983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:04",
      "total_flops_so_far": 2.3669175279550464e+16,
      "budget_used_percent": 23.669175279550466
    },
    {
      "type": "training",
      "description": "Training step 3984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:04",
      "total_flops_so_far": 2.367511633760256e+16,
      "budget_used_percent": 23.67511633760256
    },
    {
      "type": "training",
      "description": "Training step 3985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:05",
      "total_flops_so_far": 2.3681057395654656e+16,
      "budget_used_percent": 23.681057395654655
    },
    {
      "type": "training",
      "description": "Training step 3986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:05",
      "total_flops_so_far": 2.368699845370675e+16,
      "budget_used_percent": 23.68699845370675
    },
    {
      "type": "training",
      "description": "Training step 3987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:06",
      "total_flops_so_far": 2.369293951175885e+16,
      "budget_used_percent": 23.692939511758848
    },
    {
      "type": "training",
      "description": "Training step 3988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:06",
      "total_flops_so_far": 2.3698880569810944e+16,
      "budget_used_percent": 23.698880569810942
    },
    {
      "type": "training",
      "description": "Training step 3989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:06",
      "total_flops_so_far": 2.370482162786304e+16,
      "budget_used_percent": 23.70482162786304
    },
    {
      "type": "training",
      "description": "Training step 3990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:07",
      "total_flops_so_far": 2.3710762685915136e+16,
      "budget_used_percent": 23.710762685915135
    },
    {
      "type": "training",
      "description": "Training step 3991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:07",
      "total_flops_so_far": 2.371670374396723e+16,
      "budget_used_percent": 23.716703743967233
    },
    {
      "type": "training",
      "description": "Training step 3992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:08",
      "total_flops_so_far": 2.372264480201933e+16,
      "budget_used_percent": 23.722644802019328
    },
    {
      "type": "training",
      "description": "Training step 3993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:08",
      "total_flops_so_far": 2.3728585860071424e+16,
      "budget_used_percent": 23.728585860071423
    },
    {
      "type": "training",
      "description": "Training step 3994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:09",
      "total_flops_so_far": 2.373452691812352e+16,
      "budget_used_percent": 23.734526918123517
    },
    {
      "type": "training",
      "description": "Training step 3995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:09",
      "total_flops_so_far": 2.3740467976175616e+16,
      "budget_used_percent": 23.740467976175616
    },
    {
      "type": "training",
      "description": "Training step 3996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:10",
      "total_flops_so_far": 2.374640903422771e+16,
      "budget_used_percent": 23.74640903422771
    },
    {
      "type": "training",
      "description": "Training step 3997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:10",
      "total_flops_so_far": 2.375235009227981e+16,
      "budget_used_percent": 23.75235009227981
    },
    {
      "type": "training",
      "description": "Training step 3998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:10",
      "total_flops_so_far": 2.3758291150331904e+16,
      "budget_used_percent": 23.758291150331903
    },
    {
      "type": "training",
      "description": "Training step 3999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 18:58:11",
      "total_flops_so_far": 2.3764232208384e+16,
      "budget_used_percent": 23.764232208384
    },
    {
      "type": "training",
      "description": "Training step 4000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:34",
      "total_flops_so_far": 2.3770173266436096e+16,
      "budget_used_percent": 23.770173266436096
    },
    {
      "type": "training",
      "description": "Training step 4001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:35",
      "total_flops_so_far": 2.377611432448819e+16,
      "budget_used_percent": 23.776114324488194
    },
    {
      "type": "training",
      "description": "Training step 4002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:35",
      "total_flops_so_far": 2.378205538254029e+16,
      "budget_used_percent": 23.78205538254029
    },
    {
      "type": "training",
      "description": "Training step 4003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:36",
      "total_flops_so_far": 2.3787996440592384e+16,
      "budget_used_percent": 23.787996440592384
    },
    {
      "type": "training",
      "description": "Training step 4004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:36",
      "total_flops_so_far": 2.379393749864448e+16,
      "budget_used_percent": 23.79393749864448
    },
    {
      "type": "training",
      "description": "Training step 4005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:36",
      "total_flops_so_far": 2.3799878556696576e+16,
      "budget_used_percent": 23.799878556696576
    },
    {
      "type": "training",
      "description": "Training step 4006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:37",
      "total_flops_so_far": 2.380581961474867e+16,
      "budget_used_percent": 23.80581961474867
    },
    {
      "type": "training",
      "description": "Training step 4007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:37",
      "total_flops_so_far": 2.381176067280077e+16,
      "budget_used_percent": 23.81176067280077
    },
    {
      "type": "training",
      "description": "Training step 4008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:38",
      "total_flops_so_far": 2.3817701730852864e+16,
      "budget_used_percent": 23.817701730852864
    },
    {
      "type": "training",
      "description": "Training step 4009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:38",
      "total_flops_so_far": 2.382364278890496e+16,
      "budget_used_percent": 23.823642788904962
    },
    {
      "type": "training",
      "description": "Training step 4010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:38",
      "total_flops_so_far": 2.3829583846957056e+16,
      "budget_used_percent": 23.829583846957057
    },
    {
      "type": "training",
      "description": "Training step 4011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:39",
      "total_flops_so_far": 2.383552490500915e+16,
      "budget_used_percent": 23.83552490500915
    },
    {
      "type": "training",
      "description": "Training step 4012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:39",
      "total_flops_so_far": 2.384146596306125e+16,
      "budget_used_percent": 23.841465963061246
    },
    {
      "type": "training",
      "description": "Training step 4013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:40",
      "total_flops_so_far": 2.3847407021113344e+16,
      "budget_used_percent": 23.84740702111334
    },
    {
      "type": "training",
      "description": "Training step 4014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:40",
      "total_flops_so_far": 2.385334807916544e+16,
      "budget_used_percent": 23.85334807916544
    },
    {
      "type": "training",
      "description": "Training step 4015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:40",
      "total_flops_so_far": 2.3859289137217536e+16,
      "budget_used_percent": 23.859289137217534
    },
    {
      "type": "training",
      "description": "Training step 4016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:41",
      "total_flops_so_far": 2.386523019526963e+16,
      "budget_used_percent": 23.865230195269632
    },
    {
      "type": "training",
      "description": "Training step 4017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:41",
      "total_flops_so_far": 2.387117125332173e+16,
      "budget_used_percent": 23.871171253321727
    },
    {
      "type": "training",
      "description": "Training step 4018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:42",
      "total_flops_so_far": 2.3877112311373824e+16,
      "budget_used_percent": 23.877112311373825
    },
    {
      "type": "training",
      "description": "Training step 4019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:42",
      "total_flops_so_far": 2.388305336942592e+16,
      "budget_used_percent": 23.88305336942592
    },
    {
      "type": "training",
      "description": "Training step 4020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:42",
      "total_flops_so_far": 2.3888994427478016e+16,
      "budget_used_percent": 23.888994427478018
    },
    {
      "type": "training",
      "description": "Training step 4021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:43",
      "total_flops_so_far": 2.389493548553011e+16,
      "budget_used_percent": 23.894935485530112
    },
    {
      "type": "training",
      "description": "Training step 4022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:43",
      "total_flops_so_far": 2.390087654358221e+16,
      "budget_used_percent": 23.900876543582207
    },
    {
      "type": "training",
      "description": "Training step 4023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:44",
      "total_flops_so_far": 2.3906817601634304e+16,
      "budget_used_percent": 23.9068176016343
    },
    {
      "type": "training",
      "description": "Training step 4024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:44",
      "total_flops_so_far": 2.39127586596864e+16,
      "budget_used_percent": 23.9127586596864
    },
    {
      "type": "training",
      "description": "Training step 4025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:44",
      "total_flops_so_far": 2.3918699717738496e+16,
      "budget_used_percent": 23.918699717738495
    },
    {
      "type": "training",
      "description": "Training step 4026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:45",
      "total_flops_so_far": 2.392464077579059e+16,
      "budget_used_percent": 23.924640775790593
    },
    {
      "type": "training",
      "description": "Training step 4027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:45",
      "total_flops_so_far": 2.393058183384269e+16,
      "budget_used_percent": 23.930581833842687
    },
    {
      "type": "training",
      "description": "Training step 4028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:46",
      "total_flops_so_far": 2.3936522891894784e+16,
      "budget_used_percent": 23.936522891894786
    },
    {
      "type": "training",
      "description": "Training step 4029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:46",
      "total_flops_so_far": 2.394246394994688e+16,
      "budget_used_percent": 23.94246394994688
    },
    {
      "type": "training",
      "description": "Training step 4030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:46",
      "total_flops_so_far": 2.3948405007998976e+16,
      "budget_used_percent": 23.94840500799898
    },
    {
      "type": "training",
      "description": "Training step 4031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:47",
      "total_flops_so_far": 2.395434606605107e+16,
      "budget_used_percent": 23.954346066051073
    },
    {
      "type": "training",
      "description": "Training step 4032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:47",
      "total_flops_so_far": 2.396028712410317e+16,
      "budget_used_percent": 23.960287124103168
    },
    {
      "type": "training",
      "description": "Training step 4033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:48",
      "total_flops_so_far": 2.3966228182155264e+16,
      "budget_used_percent": 23.966228182155263
    },
    {
      "type": "training",
      "description": "Training step 4034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:48",
      "total_flops_so_far": 2.397216924020736e+16,
      "budget_used_percent": 23.97216924020736
    },
    {
      "type": "training",
      "description": "Training step 4035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:48",
      "total_flops_so_far": 2.3978110298259456e+16,
      "budget_used_percent": 23.978110298259455
    },
    {
      "type": "training",
      "description": "Training step 4036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:49",
      "total_flops_so_far": 2.398405135631155e+16,
      "budget_used_percent": 23.984051356311554
    },
    {
      "type": "training",
      "description": "Training step 4037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:49",
      "total_flops_so_far": 2.398999241436365e+16,
      "budget_used_percent": 23.98999241436365
    },
    {
      "type": "training",
      "description": "Training step 4038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:50",
      "total_flops_so_far": 2.3995933472415744e+16,
      "budget_used_percent": 23.995933472415746
    },
    {
      "type": "training",
      "description": "Training step 4039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:50",
      "total_flops_so_far": 2.400187453046784e+16,
      "budget_used_percent": 24.00187453046784
    },
    {
      "type": "training",
      "description": "Training step 4040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:50",
      "total_flops_so_far": 2.4007815588519936e+16,
      "budget_used_percent": 24.007815588519936
    },
    {
      "type": "training",
      "description": "Training step 4041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:51",
      "total_flops_so_far": 2.401375664657203e+16,
      "budget_used_percent": 24.01375664657203
    },
    {
      "type": "training",
      "description": "Training step 4042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:52",
      "total_flops_so_far": 2.401969770462413e+16,
      "budget_used_percent": 24.019697704624125
    },
    {
      "type": "training",
      "description": "Training step 4043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:52",
      "total_flops_so_far": 2.4025638762676224e+16,
      "budget_used_percent": 24.025638762676223
    },
    {
      "type": "training",
      "description": "Training step 4044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:52",
      "total_flops_so_far": 2.403157982072832e+16,
      "budget_used_percent": 24.031579820728318
    },
    {
      "type": "training",
      "description": "Training step 4045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:53",
      "total_flops_so_far": 2.4037520878780416e+16,
      "budget_used_percent": 24.037520878780416
    },
    {
      "type": "training",
      "description": "Training step 4046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:53",
      "total_flops_so_far": 2.404346193683251e+16,
      "budget_used_percent": 24.04346193683251
    },
    {
      "type": "training",
      "description": "Training step 4047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:54",
      "total_flops_so_far": 2.404940299488461e+16,
      "budget_used_percent": 24.04940299488461
    },
    {
      "type": "training",
      "description": "Training step 4048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:54",
      "total_flops_so_far": 2.4055344052936704e+16,
      "budget_used_percent": 24.055344052936704
    },
    {
      "type": "training",
      "description": "Training step 4049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:54",
      "total_flops_so_far": 2.40612851109888e+16,
      "budget_used_percent": 24.061285110988802
    },
    {
      "type": "training",
      "description": "Training step 4050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:55",
      "total_flops_so_far": 2.4067226169040896e+16,
      "budget_used_percent": 24.067226169040897
    },
    {
      "type": "training",
      "description": "Training step 4051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:55",
      "total_flops_so_far": 2.407316722709299e+16,
      "budget_used_percent": 24.07316722709299
    },
    {
      "type": "training",
      "description": "Training step 4052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:56",
      "total_flops_so_far": 2.407910828514509e+16,
      "budget_used_percent": 24.079108285145086
    },
    {
      "type": "training",
      "description": "Training step 4053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:56",
      "total_flops_so_far": 2.4085049343197184e+16,
      "budget_used_percent": 24.085049343197184
    },
    {
      "type": "training",
      "description": "Training step 4054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:56",
      "total_flops_so_far": 2.409099040124928e+16,
      "budget_used_percent": 24.09099040124928
    },
    {
      "type": "training",
      "description": "Training step 4055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:57",
      "total_flops_so_far": 2.4096931459301376e+16,
      "budget_used_percent": 24.096931459301377
    },
    {
      "type": "training",
      "description": "Training step 4056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:57",
      "total_flops_so_far": 2.410287251735347e+16,
      "budget_used_percent": 24.10287251735347
    },
    {
      "type": "training",
      "description": "Training step 4057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:58",
      "total_flops_so_far": 2.410881357540557e+16,
      "budget_used_percent": 24.10881357540557
    },
    {
      "type": "training",
      "description": "Training step 4058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:58",
      "total_flops_so_far": 2.4114754633457664e+16,
      "budget_used_percent": 24.114754633457665
    },
    {
      "type": "training",
      "description": "Training step 4059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:58",
      "total_flops_so_far": 2.412069569150976e+16,
      "budget_used_percent": 24.12069569150976
    },
    {
      "type": "training",
      "description": "Training step 4060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:59",
      "total_flops_so_far": 2.4126636749561856e+16,
      "budget_used_percent": 24.126636749561854
    },
    {
      "type": "training",
      "description": "Training step 4061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:00:59",
      "total_flops_so_far": 2.413257780761395e+16,
      "budget_used_percent": 24.132577807613952
    },
    {
      "type": "training",
      "description": "Training step 4062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:00",
      "total_flops_so_far": 2.413851886566605e+16,
      "budget_used_percent": 24.138518865666047
    },
    {
      "type": "training",
      "description": "Training step 4063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:00",
      "total_flops_so_far": 2.4144459923718144e+16,
      "budget_used_percent": 24.144459923718145
    },
    {
      "type": "training",
      "description": "Training step 4064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:00",
      "total_flops_so_far": 2.415040098177024e+16,
      "budget_used_percent": 24.15040098177024
    },
    {
      "type": "training",
      "description": "Training step 4065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:01",
      "total_flops_so_far": 2.4156342039822336e+16,
      "budget_used_percent": 24.156342039822338
    },
    {
      "type": "training",
      "description": "Training step 4066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:01",
      "total_flops_so_far": 2.416228309787443e+16,
      "budget_used_percent": 24.162283097874433
    },
    {
      "type": "training",
      "description": "Training step 4067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:02",
      "total_flops_so_far": 2.416822415592653e+16,
      "budget_used_percent": 24.16822415592653
    },
    {
      "type": "training",
      "description": "Training step 4068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:02",
      "total_flops_so_far": 2.4174165213978624e+16,
      "budget_used_percent": 24.174165213978625
    },
    {
      "type": "training",
      "description": "Training step 4069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:02",
      "total_flops_so_far": 2.418010627203072e+16,
      "budget_used_percent": 24.18010627203072
    },
    {
      "type": "training",
      "description": "Training step 4070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:03",
      "total_flops_so_far": 2.4186047330082816e+16,
      "budget_used_percent": 24.186047330082815
    },
    {
      "type": "training",
      "description": "Training step 4071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:03",
      "total_flops_so_far": 2.419198838813491e+16,
      "budget_used_percent": 24.19198838813491
    },
    {
      "type": "training",
      "description": "Training step 4072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:04",
      "total_flops_so_far": 2.419792944618701e+16,
      "budget_used_percent": 24.197929446187008
    },
    {
      "type": "training",
      "description": "Training step 4073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:04",
      "total_flops_so_far": 2.4203870504239104e+16,
      "budget_used_percent": 24.203870504239102
    },
    {
      "type": "training",
      "description": "Training step 4074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:04",
      "total_flops_so_far": 2.42098115622912e+16,
      "budget_used_percent": 24.2098115622912
    },
    {
      "type": "training",
      "description": "Training step 4075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:05",
      "total_flops_so_far": 2.4215752620343296e+16,
      "budget_used_percent": 24.215752620343295
    },
    {
      "type": "training",
      "description": "Training step 4076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:05",
      "total_flops_so_far": 2.422169367839539e+16,
      "budget_used_percent": 24.221693678395393
    },
    {
      "type": "training",
      "description": "Training step 4077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:06",
      "total_flops_so_far": 2.422763473644749e+16,
      "budget_used_percent": 24.227634736447488
    },
    {
      "type": "training",
      "description": "Training step 4078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:06",
      "total_flops_so_far": 2.4233575794499584e+16,
      "budget_used_percent": 24.233575794499586
    },
    {
      "type": "training",
      "description": "Training step 4079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:06",
      "total_flops_so_far": 2.423951685255168e+16,
      "budget_used_percent": 24.23951685255168
    },
    {
      "type": "training",
      "description": "Training step 4080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:07",
      "total_flops_so_far": 2.4245457910603776e+16,
      "budget_used_percent": 24.245457910603776
    },
    {
      "type": "training",
      "description": "Training step 4081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:07",
      "total_flops_so_far": 2.425139896865587e+16,
      "budget_used_percent": 24.25139896865587
    },
    {
      "type": "training",
      "description": "Training step 4082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:08",
      "total_flops_so_far": 2.425734002670797e+16,
      "budget_used_percent": 24.25734002670797
    },
    {
      "type": "training",
      "description": "Training step 4083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:08",
      "total_flops_so_far": 2.4263281084760064e+16,
      "budget_used_percent": 24.263281084760063
    },
    {
      "type": "training",
      "description": "Training step 4084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:08",
      "total_flops_so_far": 2.426922214281216e+16,
      "budget_used_percent": 24.26922214281216
    },
    {
      "type": "training",
      "description": "Training step 4085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:09",
      "total_flops_so_far": 2.4275163200864256e+16,
      "budget_used_percent": 24.275163200864256
    },
    {
      "type": "training",
      "description": "Training step 4086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:09",
      "total_flops_so_far": 2.428110425891635e+16,
      "budget_used_percent": 24.281104258916354
    },
    {
      "type": "training",
      "description": "Training step 4087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:10",
      "total_flops_so_far": 2.428704531696845e+16,
      "budget_used_percent": 24.28704531696845
    },
    {
      "type": "training",
      "description": "Training step 4088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:10",
      "total_flops_so_far": 2.4292986375020544e+16,
      "budget_used_percent": 24.292986375020543
    },
    {
      "type": "training",
      "description": "Training step 4089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:10",
      "total_flops_so_far": 2.429892743307264e+16,
      "budget_used_percent": 24.298927433072638
    },
    {
      "type": "training",
      "description": "Training step 4090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:11",
      "total_flops_so_far": 2.4304868491124736e+16,
      "budget_used_percent": 24.304868491124736
    },
    {
      "type": "training",
      "description": "Training step 4091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:11",
      "total_flops_so_far": 2.431080954917683e+16,
      "budget_used_percent": 24.31080954917683
    },
    {
      "type": "training",
      "description": "Training step 4092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:12",
      "total_flops_so_far": 2.431675060722893e+16,
      "budget_used_percent": 24.31675060722893
    },
    {
      "type": "training",
      "description": "Training step 4093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:12",
      "total_flops_so_far": 2.4322691665281024e+16,
      "budget_used_percent": 24.322691665281024
    },
    {
      "type": "training",
      "description": "Training step 4094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:13",
      "total_flops_so_far": 2.432863272333312e+16,
      "budget_used_percent": 24.328632723333122
    },
    {
      "type": "training",
      "description": "Training step 4095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:13",
      "total_flops_so_far": 2.4334573781385216e+16,
      "budget_used_percent": 24.334573781385217
    },
    {
      "type": "training",
      "description": "Training step 4096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:14",
      "total_flops_so_far": 2.434051483943731e+16,
      "budget_used_percent": 24.340514839437315
    },
    {
      "type": "training",
      "description": "Training step 4097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:14",
      "total_flops_so_far": 2.434645589748941e+16,
      "budget_used_percent": 24.34645589748941
    },
    {
      "type": "training",
      "description": "Training step 4098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:14",
      "total_flops_so_far": 2.4352396955541504e+16,
      "budget_used_percent": 24.352396955541504
    },
    {
      "type": "training",
      "description": "Training step 4099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:15",
      "total_flops_so_far": 2.43583380135936e+16,
      "budget_used_percent": 24.3583380135936
    },
    {
      "type": "training",
      "description": "Training step 4100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:15",
      "total_flops_so_far": 2.4364279071645696e+16,
      "budget_used_percent": 24.364279071645694
    },
    {
      "type": "training",
      "description": "Training step 4101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:16",
      "total_flops_so_far": 2.437022012969779e+16,
      "budget_used_percent": 24.370220129697792
    },
    {
      "type": "training",
      "description": "Training step 4102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:16",
      "total_flops_so_far": 2.437616118774989e+16,
      "budget_used_percent": 24.376161187749886
    },
    {
      "type": "training",
      "description": "Training step 4103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:16",
      "total_flops_so_far": 2.4382102245801984e+16,
      "budget_used_percent": 24.382102245801985
    },
    {
      "type": "training",
      "description": "Training step 4104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:17",
      "total_flops_so_far": 2.438804330385408e+16,
      "budget_used_percent": 24.38804330385408
    },
    {
      "type": "training",
      "description": "Training step 4105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:17",
      "total_flops_so_far": 2.4393984361906176e+16,
      "budget_used_percent": 24.393984361906178
    },
    {
      "type": "training",
      "description": "Training step 4106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:18",
      "total_flops_so_far": 2.439992541995827e+16,
      "budget_used_percent": 24.399925419958272
    },
    {
      "type": "training",
      "description": "Training step 4107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:18",
      "total_flops_so_far": 2.440586647801037e+16,
      "budget_used_percent": 24.405866478010367
    },
    {
      "type": "training",
      "description": "Training step 4108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:18",
      "total_flops_so_far": 2.4411807536062464e+16,
      "budget_used_percent": 24.41180753606246
    },
    {
      "type": "training",
      "description": "Training step 4109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:19",
      "total_flops_so_far": 2.441774859411456e+16,
      "budget_used_percent": 24.41774859411456
    },
    {
      "type": "training",
      "description": "Training step 4110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:19",
      "total_flops_so_far": 2.4423689652166656e+16,
      "budget_used_percent": 24.423689652166654
    },
    {
      "type": "training",
      "description": "Training step 4111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:20",
      "total_flops_so_far": 2.442963071021875e+16,
      "budget_used_percent": 24.429630710218753
    },
    {
      "type": "training",
      "description": "Training step 4112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:20",
      "total_flops_so_far": 2.443557176827085e+16,
      "budget_used_percent": 24.435571768270847
    },
    {
      "type": "training",
      "description": "Training step 4113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:20",
      "total_flops_so_far": 2.4441512826322944e+16,
      "budget_used_percent": 24.441512826322946
    },
    {
      "type": "training",
      "description": "Training step 4114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:21",
      "total_flops_so_far": 2.444745388437504e+16,
      "budget_used_percent": 24.44745388437504
    },
    {
      "type": "training",
      "description": "Training step 4115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:21",
      "total_flops_so_far": 2.4453394942427136e+16,
      "budget_used_percent": 24.45339494242714
    },
    {
      "type": "training",
      "description": "Training step 4116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:22",
      "total_flops_so_far": 2.445933600047923e+16,
      "budget_used_percent": 24.459336000479233
    },
    {
      "type": "training",
      "description": "Training step 4117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:22",
      "total_flops_so_far": 2.446527705853133e+16,
      "budget_used_percent": 24.465277058531328
    },
    {
      "type": "training",
      "description": "Training step 4118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:22",
      "total_flops_so_far": 2.4471218116583424e+16,
      "budget_used_percent": 24.471218116583422
    },
    {
      "type": "training",
      "description": "Training step 4119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:23",
      "total_flops_so_far": 2.447715917463552e+16,
      "budget_used_percent": 24.47715917463552
    },
    {
      "type": "training",
      "description": "Training step 4120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:23",
      "total_flops_so_far": 2.4483100232687616e+16,
      "budget_used_percent": 24.483100232687615
    },
    {
      "type": "training",
      "description": "Training step 4121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:24",
      "total_flops_so_far": 2.448904129073971e+16,
      "budget_used_percent": 24.489041290739713
    },
    {
      "type": "training",
      "description": "Training step 4122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:24",
      "total_flops_so_far": 2.449498234879181e+16,
      "budget_used_percent": 24.494982348791808
    },
    {
      "type": "training",
      "description": "Training step 4123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:24",
      "total_flops_so_far": 2.4500923406843904e+16,
      "budget_used_percent": 24.500923406843906
    },
    {
      "type": "training",
      "description": "Training step 4124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:25",
      "total_flops_so_far": 2.4506864464896e+16,
      "budget_used_percent": 24.506864464896
    },
    {
      "type": "training",
      "description": "Training step 4125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:25",
      "total_flops_so_far": 2.4512805522948096e+16,
      "budget_used_percent": 24.512805522948096
    },
    {
      "type": "training",
      "description": "Training step 4126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:26",
      "total_flops_so_far": 2.451874658100019e+16,
      "budget_used_percent": 24.51874658100019
    },
    {
      "type": "training",
      "description": "Training step 4127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:26",
      "total_flops_so_far": 2.452468763905229e+16,
      "budget_used_percent": 24.524687639052285
    },
    {
      "type": "training",
      "description": "Training step 4128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:26",
      "total_flops_so_far": 2.4530628697104384e+16,
      "budget_used_percent": 24.530628697104383
    },
    {
      "type": "training",
      "description": "Training step 4129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:27",
      "total_flops_so_far": 2.453656975515648e+16,
      "budget_used_percent": 24.536569755156478
    },
    {
      "type": "training",
      "description": "Training step 4130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:27",
      "total_flops_so_far": 2.4542510813208576e+16,
      "budget_used_percent": 24.542510813208576
    },
    {
      "type": "training",
      "description": "Training step 4131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:28",
      "total_flops_so_far": 2.454845187126067e+16,
      "budget_used_percent": 24.54845187126067
    },
    {
      "type": "training",
      "description": "Training step 4132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:28",
      "total_flops_so_far": 2.455439292931277e+16,
      "budget_used_percent": 24.55439292931277
    },
    {
      "type": "training",
      "description": "Training step 4133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:28",
      "total_flops_so_far": 2.4560333987364864e+16,
      "budget_used_percent": 24.560333987364864
    },
    {
      "type": "training",
      "description": "Training step 4134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:29",
      "total_flops_so_far": 2.456627504541696e+16,
      "budget_used_percent": 24.566275045416962
    },
    {
      "type": "training",
      "description": "Training step 4135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:29",
      "total_flops_so_far": 2.4572216103469056e+16,
      "budget_used_percent": 24.572216103469056
    },
    {
      "type": "training",
      "description": "Training step 4136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:30",
      "total_flops_so_far": 2.457815716152115e+16,
      "budget_used_percent": 24.57815716152115
    },
    {
      "type": "training",
      "description": "Training step 4137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:30",
      "total_flops_so_far": 2.458409821957325e+16,
      "budget_used_percent": 24.584098219573246
    },
    {
      "type": "training",
      "description": "Training step 4138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:30",
      "total_flops_so_far": 2.4590039277625344e+16,
      "budget_used_percent": 24.590039277625344
    },
    {
      "type": "training",
      "description": "Training step 4139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:31",
      "total_flops_so_far": 2.459598033567744e+16,
      "budget_used_percent": 24.59598033567744
    },
    {
      "type": "training",
      "description": "Training step 4140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:31",
      "total_flops_so_far": 2.4601921393729536e+16,
      "budget_used_percent": 24.601921393729537
    },
    {
      "type": "training",
      "description": "Training step 4141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:32",
      "total_flops_so_far": 2.460786245178163e+16,
      "budget_used_percent": 24.60786245178163
    },
    {
      "type": "training",
      "description": "Training step 4142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:32",
      "total_flops_so_far": 2.461380350983373e+16,
      "budget_used_percent": 24.61380350983373
    },
    {
      "type": "training",
      "description": "Training step 4143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:33",
      "total_flops_so_far": 2.4619744567885824e+16,
      "budget_used_percent": 24.619744567885824
    },
    {
      "type": "training",
      "description": "Training step 4144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:33",
      "total_flops_so_far": 2.462568562593792e+16,
      "budget_used_percent": 24.62568562593792
    },
    {
      "type": "training",
      "description": "Training step 4145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:34",
      "total_flops_so_far": 2.4631626683990016e+16,
      "budget_used_percent": 24.631626683990014
    },
    {
      "type": "training",
      "description": "Training step 4146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:34",
      "total_flops_so_far": 2.463756774204211e+16,
      "budget_used_percent": 24.637567742042112
    },
    {
      "type": "training",
      "description": "Training step 4147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:34",
      "total_flops_so_far": 2.464350880009421e+16,
      "budget_used_percent": 24.643508800094207
    },
    {
      "type": "training",
      "description": "Training step 4148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:35",
      "total_flops_so_far": 2.4649449858146304e+16,
      "budget_used_percent": 24.649449858146305
    },
    {
      "type": "training",
      "description": "Training step 4149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:35",
      "total_flops_so_far": 2.46553909161984e+16,
      "budget_used_percent": 24.6553909161984
    },
    {
      "type": "training",
      "description": "Training step 4150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:36",
      "total_flops_so_far": 2.4661331974250496e+16,
      "budget_used_percent": 24.661331974250498
    },
    {
      "type": "training",
      "description": "Training step 4151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:36",
      "total_flops_so_far": 2.466727303230259e+16,
      "budget_used_percent": 24.667273032302592
    },
    {
      "type": "training",
      "description": "Training step 4152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:36",
      "total_flops_so_far": 2.467321409035469e+16,
      "budget_used_percent": 24.67321409035469
    },
    {
      "type": "training",
      "description": "Training step 4153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:37",
      "total_flops_so_far": 2.4679155148406784e+16,
      "budget_used_percent": 24.679155148406785
    },
    {
      "type": "training",
      "description": "Training step 4154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:37",
      "total_flops_so_far": 2.468509620645888e+16,
      "budget_used_percent": 24.68509620645888
    },
    {
      "type": "training",
      "description": "Training step 4155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:38",
      "total_flops_so_far": 2.4691037264510976e+16,
      "budget_used_percent": 24.691037264510975
    },
    {
      "type": "training",
      "description": "Training step 4156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:38",
      "total_flops_so_far": 2.469697832256307e+16,
      "budget_used_percent": 24.69697832256307
    },
    {
      "type": "training",
      "description": "Training step 4157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:38",
      "total_flops_so_far": 2.470291938061517e+16,
      "budget_used_percent": 24.702919380615167
    },
    {
      "type": "training",
      "description": "Training step 4158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:39",
      "total_flops_so_far": 2.4708860438667264e+16,
      "budget_used_percent": 24.708860438667262
    },
    {
      "type": "training",
      "description": "Training step 4159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:39",
      "total_flops_so_far": 2.471480149671936e+16,
      "budget_used_percent": 24.71480149671936
    },
    {
      "type": "training",
      "description": "Training step 4160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:40",
      "total_flops_so_far": 2.4720742554771456e+16,
      "budget_used_percent": 24.720742554771455
    },
    {
      "type": "training",
      "description": "Training step 4161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:40",
      "total_flops_so_far": 2.472668361282355e+16,
      "budget_used_percent": 24.726683612823553
    },
    {
      "type": "training",
      "description": "Training step 4162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:40",
      "total_flops_so_far": 2.473262467087565e+16,
      "budget_used_percent": 24.732624670875648
    },
    {
      "type": "training",
      "description": "Training step 4163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:41",
      "total_flops_so_far": 2.4738565728927744e+16,
      "budget_used_percent": 24.738565728927746
    },
    {
      "type": "training",
      "description": "Training step 4164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:41",
      "total_flops_so_far": 2.474450678697984e+16,
      "budget_used_percent": 24.74450678697984
    },
    {
      "type": "training",
      "description": "Training step 4165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:42",
      "total_flops_so_far": 2.4750447845031936e+16,
      "budget_used_percent": 24.750447845031935
    },
    {
      "type": "training",
      "description": "Training step 4166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:42",
      "total_flops_so_far": 2.475638890308403e+16,
      "budget_used_percent": 24.75638890308403
    },
    {
      "type": "training",
      "description": "Training step 4167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:42",
      "total_flops_so_far": 2.476232996113613e+16,
      "budget_used_percent": 24.76232996113613
    },
    {
      "type": "training",
      "description": "Training step 4168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:43",
      "total_flops_so_far": 2.4768271019188224e+16,
      "budget_used_percent": 24.768271019188223
    },
    {
      "type": "training",
      "description": "Training step 4169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:43",
      "total_flops_so_far": 2.477421207724032e+16,
      "budget_used_percent": 24.77421207724032
    },
    {
      "type": "training",
      "description": "Training step 4170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:44",
      "total_flops_so_far": 2.4780153135292416e+16,
      "budget_used_percent": 24.780153135292416
    },
    {
      "type": "training",
      "description": "Training step 4171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:44",
      "total_flops_so_far": 2.478609419334451e+16,
      "budget_used_percent": 24.786094193344514
    },
    {
      "type": "training",
      "description": "Training step 4172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:44",
      "total_flops_so_far": 2.479203525139661e+16,
      "budget_used_percent": 24.79203525139661
    },
    {
      "type": "training",
      "description": "Training step 4173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:45",
      "total_flops_so_far": 2.4797976309448704e+16,
      "budget_used_percent": 24.797976309448703
    },
    {
      "type": "training",
      "description": "Training step 4174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:45",
      "total_flops_so_far": 2.48039173675008e+16,
      "budget_used_percent": 24.803917367500798
    },
    {
      "type": "training",
      "description": "Training step 4175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:46",
      "total_flops_so_far": 2.4809858425552896e+16,
      "budget_used_percent": 24.809858425552896
    },
    {
      "type": "training",
      "description": "Training step 4176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:46",
      "total_flops_so_far": 2.481579948360499e+16,
      "budget_used_percent": 24.81579948360499
    },
    {
      "type": "training",
      "description": "Training step 4177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:46",
      "total_flops_so_far": 2.482174054165709e+16,
      "budget_used_percent": 24.82174054165709
    },
    {
      "type": "training",
      "description": "Training step 4178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:47",
      "total_flops_so_far": 2.4827681599709184e+16,
      "budget_used_percent": 24.827681599709184
    },
    {
      "type": "training",
      "description": "Training step 4179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:47",
      "total_flops_so_far": 2.483362265776128e+16,
      "budget_used_percent": 24.833622657761282
    },
    {
      "type": "training",
      "description": "Training step 4180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:48",
      "total_flops_so_far": 2.4839563715813376e+16,
      "budget_used_percent": 24.839563715813377
    },
    {
      "type": "training",
      "description": "Training step 4181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:48",
      "total_flops_so_far": 2.484550477386547e+16,
      "budget_used_percent": 24.845504773865475
    },
    {
      "type": "training",
      "description": "Training step 4182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:49",
      "total_flops_so_far": 2.485144583191757e+16,
      "budget_used_percent": 24.85144583191757
    },
    {
      "type": "training",
      "description": "Training step 4183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:49",
      "total_flops_so_far": 2.4857386889969664e+16,
      "budget_used_percent": 24.857386889969664
    },
    {
      "type": "training",
      "description": "Training step 4184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:49",
      "total_flops_so_far": 2.486332794802176e+16,
      "budget_used_percent": 24.86332794802176
    },
    {
      "type": "training",
      "description": "Training step 4185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:50",
      "total_flops_so_far": 2.4869269006073856e+16,
      "budget_used_percent": 24.869269006073853
    },
    {
      "type": "training",
      "description": "Training step 4186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:50",
      "total_flops_so_far": 2.487521006412595e+16,
      "budget_used_percent": 24.87521006412595
    },
    {
      "type": "training",
      "description": "Training step 4187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:51",
      "total_flops_so_far": 2.488115112217805e+16,
      "budget_used_percent": 24.881151122178046
    },
    {
      "type": "training",
      "description": "Training step 4188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:51",
      "total_flops_so_far": 2.4887092180230144e+16,
      "budget_used_percent": 24.887092180230145
    },
    {
      "type": "training",
      "description": "Training step 4189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:51",
      "total_flops_so_far": 2.489303323828224e+16,
      "budget_used_percent": 24.89303323828224
    },
    {
      "type": "training",
      "description": "Training step 4190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:52",
      "total_flops_so_far": 2.4898974296334336e+16,
      "budget_used_percent": 24.898974296334337
    },
    {
      "type": "training",
      "description": "Training step 4191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:52",
      "total_flops_so_far": 2.490491535438643e+16,
      "budget_used_percent": 24.904915354386432
    },
    {
      "type": "training",
      "description": "Training step 4192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:53",
      "total_flops_so_far": 2.491085641243853e+16,
      "budget_used_percent": 24.910856412438527
    },
    {
      "type": "training",
      "description": "Training step 4193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:53",
      "total_flops_so_far": 2.4916797470490624e+16,
      "budget_used_percent": 24.91679747049062
    },
    {
      "type": "training",
      "description": "Training step 4194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:54",
      "total_flops_so_far": 2.492273852854272e+16,
      "budget_used_percent": 24.92273852854272
    },
    {
      "type": "training",
      "description": "Training step 4195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:54",
      "total_flops_so_far": 2.4928679586594816e+16,
      "budget_used_percent": 24.928679586594814
    },
    {
      "type": "training",
      "description": "Training step 4196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:54",
      "total_flops_so_far": 2.493462064464691e+16,
      "budget_used_percent": 24.934620644646913
    },
    {
      "type": "training",
      "description": "Training step 4197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:55",
      "total_flops_so_far": 2.494056170269901e+16,
      "budget_used_percent": 24.940561702699007
    },
    {
      "type": "training",
      "description": "Training step 4198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:55",
      "total_flops_so_far": 2.4946502760751104e+16,
      "budget_used_percent": 24.946502760751105
    },
    {
      "type": "training",
      "description": "Training step 4199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:56",
      "total_flops_so_far": 2.49524438188032e+16,
      "budget_used_percent": 24.9524438188032
    },
    {
      "type": "training",
      "description": "Training step 4200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:56",
      "total_flops_so_far": 2.4958384876855296e+16,
      "budget_used_percent": 24.9583848768553
    },
    {
      "type": "training",
      "description": "Training step 4201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:56",
      "total_flops_so_far": 2.496432593490739e+16,
      "budget_used_percent": 24.964325934907393
    },
    {
      "type": "training",
      "description": "Training step 4202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:57",
      "total_flops_so_far": 2.497026699295949e+16,
      "budget_used_percent": 24.970266992959488
    },
    {
      "type": "training",
      "description": "Training step 4203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:57",
      "total_flops_so_far": 2.4976208051011584e+16,
      "budget_used_percent": 24.976208051011582
    },
    {
      "type": "training",
      "description": "Training step 4204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:58",
      "total_flops_so_far": 2.498214910906368e+16,
      "budget_used_percent": 24.98214910906368
    },
    {
      "type": "training",
      "description": "Training step 4205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:58",
      "total_flops_so_far": 2.4988090167115776e+16,
      "budget_used_percent": 24.988090167115775
    },
    {
      "type": "training",
      "description": "Training step 4206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:59",
      "total_flops_so_far": 2.499403122516787e+16,
      "budget_used_percent": 24.994031225167873
    },
    {
      "type": "training",
      "description": "Training step 4207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:59",
      "total_flops_so_far": 2.499997228321997e+16,
      "budget_used_percent": 24.999972283219968
    },
    {
      "type": "training",
      "description": "Training step 4208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:01:59",
      "total_flops_so_far": 2.5005913341272064e+16,
      "budget_used_percent": 25.005913341272063
    },
    {
      "type": "training",
      "description": "Training step 4209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:00",
      "total_flops_so_far": 2.501185439932416e+16,
      "budget_used_percent": 25.011854399324164
    },
    {
      "type": "training",
      "description": "Training step 4210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:00",
      "total_flops_so_far": 2.5017795457376256e+16,
      "budget_used_percent": 25.01779545737626
    },
    {
      "type": "training",
      "description": "Training step 4211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:01",
      "total_flops_so_far": 2.502373651542835e+16,
      "budget_used_percent": 25.023736515428354
    },
    {
      "type": "training",
      "description": "Training step 4212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:01",
      "total_flops_so_far": 2.502967757348045e+16,
      "budget_used_percent": 25.02967757348045
    },
    {
      "type": "training",
      "description": "Training step 4213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:01",
      "total_flops_so_far": 2.5035618631532544e+16,
      "budget_used_percent": 25.035618631532543
    },
    {
      "type": "training",
      "description": "Training step 4214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:02",
      "total_flops_so_far": 2.504155968958464e+16,
      "budget_used_percent": 25.04155968958464
    },
    {
      "type": "training",
      "description": "Training step 4215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:02",
      "total_flops_so_far": 2.5047500747636736e+16,
      "budget_used_percent": 25.047500747636736
    },
    {
      "type": "training",
      "description": "Training step 4216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:02",
      "total_flops_so_far": 2.505344180568883e+16,
      "budget_used_percent": 25.05344180568883
    },
    {
      "type": "training",
      "description": "Training step 4217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:03",
      "total_flops_so_far": 2.505938286374093e+16,
      "budget_used_percent": 25.059382863740925
    },
    {
      "type": "training",
      "description": "Training step 4218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:03",
      "total_flops_so_far": 2.5065323921793024e+16,
      "budget_used_percent": 25.065323921793027
    },
    {
      "type": "training",
      "description": "Training step 4219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:04",
      "total_flops_so_far": 2.507126497984512e+16,
      "budget_used_percent": 25.07126497984512
    },
    {
      "type": "training",
      "description": "Training step 4220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:04",
      "total_flops_so_far": 2.5077206037897216e+16,
      "budget_used_percent": 25.077206037897216
    },
    {
      "type": "training",
      "description": "Training step 4221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:04",
      "total_flops_so_far": 2.508314709594931e+16,
      "budget_used_percent": 25.08314709594931
    },
    {
      "type": "training",
      "description": "Training step 4222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:05",
      "total_flops_so_far": 2.508908815400141e+16,
      "budget_used_percent": 25.08908815400141
    },
    {
      "type": "training",
      "description": "Training step 4223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:05",
      "total_flops_so_far": 2.5095029212053504e+16,
      "budget_used_percent": 25.095029212053504
    },
    {
      "type": "training",
      "description": "Training step 4224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:06",
      "total_flops_so_far": 2.51009702701056e+16,
      "budget_used_percent": 25.1009702701056
    },
    {
      "type": "training",
      "description": "Training step 4225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:06",
      "total_flops_so_far": 2.5106911328157696e+16,
      "budget_used_percent": 25.106911328157693
    },
    {
      "type": "training",
      "description": "Training step 4226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:06",
      "total_flops_so_far": 2.511285238620979e+16,
      "budget_used_percent": 25.112852386209795
    },
    {
      "type": "training",
      "description": "Training step 4227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:07",
      "total_flops_so_far": 2.511879344426189e+16,
      "budget_used_percent": 25.11879344426189
    },
    {
      "type": "training",
      "description": "Training step 4228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:07",
      "total_flops_so_far": 2.5124734502313984e+16,
      "budget_used_percent": 25.124734502313984
    },
    {
      "type": "training",
      "description": "Training step 4229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:08",
      "total_flops_so_far": 2.513067556036608e+16,
      "budget_used_percent": 25.13067556036608
    },
    {
      "type": "training",
      "description": "Training step 4230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:08",
      "total_flops_so_far": 2.5136616618418176e+16,
      "budget_used_percent": 25.136616618418177
    },
    {
      "type": "training",
      "description": "Training step 4231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:09",
      "total_flops_so_far": 2.514255767647027e+16,
      "budget_used_percent": 25.142557676470272
    },
    {
      "type": "training",
      "description": "Training step 4232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:09",
      "total_flops_so_far": 2.514849873452237e+16,
      "budget_used_percent": 25.148498734522367
    },
    {
      "type": "training",
      "description": "Training step 4233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:10",
      "total_flops_so_far": 2.5154439792574464e+16,
      "budget_used_percent": 25.15443979257446
    },
    {
      "type": "training",
      "description": "Training step 4234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:10",
      "total_flops_so_far": 2.516038085062656e+16,
      "budget_used_percent": 25.160380850626563
    },
    {
      "type": "training",
      "description": "Training step 4235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:10",
      "total_flops_so_far": 2.5166321908678656e+16,
      "budget_used_percent": 25.166321908678658
    },
    {
      "type": "training",
      "description": "Training step 4236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:11",
      "total_flops_so_far": 2.517226296673075e+16,
      "budget_used_percent": 25.172262966730752
    },
    {
      "type": "training",
      "description": "Training step 4237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:11",
      "total_flops_so_far": 2.517820402478285e+16,
      "budget_used_percent": 25.178204024782847
    },
    {
      "type": "training",
      "description": "Training step 4238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:12",
      "total_flops_so_far": 2.5184145082834944e+16,
      "budget_used_percent": 25.184145082834945
    },
    {
      "type": "training",
      "description": "Training step 4239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:12",
      "total_flops_so_far": 2.519008614088704e+16,
      "budget_used_percent": 25.19008614088704
    },
    {
      "type": "training",
      "description": "Training step 4240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:12",
      "total_flops_so_far": 2.5196027198939136e+16,
      "budget_used_percent": 25.196027198939134
    },
    {
      "type": "training",
      "description": "Training step 4241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:13",
      "total_flops_so_far": 2.520196825699123e+16,
      "budget_used_percent": 25.20196825699123
    },
    {
      "type": "training",
      "description": "Training step 4242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:13",
      "total_flops_so_far": 2.520790931504333e+16,
      "budget_used_percent": 25.207909315043324
    },
    {
      "type": "training",
      "description": "Training step 4243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:14",
      "total_flops_so_far": 2.5213850373095424e+16,
      "budget_used_percent": 25.213850373095426
    },
    {
      "type": "training",
      "description": "Training step 4244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:14",
      "total_flops_so_far": 2.521979143114752e+16,
      "budget_used_percent": 25.21979143114752
    },
    {
      "type": "training",
      "description": "Training step 4245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:14",
      "total_flops_so_far": 2.5225732489199616e+16,
      "budget_used_percent": 25.225732489199615
    },
    {
      "type": "training",
      "description": "Training step 4246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:15",
      "total_flops_so_far": 2.523167354725171e+16,
      "budget_used_percent": 25.23167354725171
    },
    {
      "type": "training",
      "description": "Training step 4247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:15",
      "total_flops_so_far": 2.523761460530381e+16,
      "budget_used_percent": 25.23761460530381
    },
    {
      "type": "training",
      "description": "Training step 4248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:16",
      "total_flops_so_far": 2.5243555663355904e+16,
      "budget_used_percent": 25.243555663355906
    },
    {
      "type": "training",
      "description": "Training step 4249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:16",
      "total_flops_so_far": 2.5249496721408e+16,
      "budget_used_percent": 25.249496721408
    },
    {
      "type": "training",
      "description": "Training step 4250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:17",
      "total_flops_so_far": 2.5255437779460096e+16,
      "budget_used_percent": 25.255437779460095
    },
    {
      "type": "training",
      "description": "Training step 4251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:17",
      "total_flops_so_far": 2.526137883751219e+16,
      "budget_used_percent": 25.261378837512193
    },
    {
      "type": "training",
      "description": "Training step 4252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:17",
      "total_flops_so_far": 2.526731989556429e+16,
      "budget_used_percent": 25.267319895564288
    },
    {
      "type": "training",
      "description": "Training step 4253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:18",
      "total_flops_so_far": 2.5273260953616384e+16,
      "budget_used_percent": 25.273260953616383
    },
    {
      "type": "training",
      "description": "Training step 4254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:18",
      "total_flops_so_far": 2.527920201166848e+16,
      "budget_used_percent": 25.279202011668477
    },
    {
      "type": "training",
      "description": "Training step 4255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:19",
      "total_flops_so_far": 2.5285143069720576e+16,
      "budget_used_percent": 25.28514306972058
    },
    {
      "type": "training",
      "description": "Training step 4256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:19",
      "total_flops_so_far": 2.529108412777267e+16,
      "budget_used_percent": 25.291084127772674
    },
    {
      "type": "training",
      "description": "Training step 4257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:19",
      "total_flops_so_far": 2.529702518582477e+16,
      "budget_used_percent": 25.29702518582477
    },
    {
      "type": "training",
      "description": "Training step 4258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:20",
      "total_flops_so_far": 2.5302966243876864e+16,
      "budget_used_percent": 25.302966243876863
    },
    {
      "type": "training",
      "description": "Training step 4259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:20",
      "total_flops_so_far": 2.530890730192896e+16,
      "budget_used_percent": 25.30890730192896
    },
    {
      "type": "training",
      "description": "Training step 4260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:21",
      "total_flops_so_far": 2.5314848359981056e+16,
      "budget_used_percent": 25.314848359981056
    },
    {
      "type": "training",
      "description": "Training step 4261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:21",
      "total_flops_so_far": 2.532078941803315e+16,
      "budget_used_percent": 25.32078941803315
    },
    {
      "type": "training",
      "description": "Training step 4262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:22",
      "total_flops_so_far": 2.532673047608525e+16,
      "budget_used_percent": 25.326730476085245
    },
    {
      "type": "training",
      "description": "Training step 4263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:22",
      "total_flops_so_far": 2.5332671534137344e+16,
      "budget_used_percent": 25.332671534137347
    },
    {
      "type": "training",
      "description": "Training step 4264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:23",
      "total_flops_so_far": 2.533861259218944e+16,
      "budget_used_percent": 25.338612592189442
    },
    {
      "type": "training",
      "description": "Training step 4265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:23",
      "total_flops_so_far": 2.5344553650241536e+16,
      "budget_used_percent": 25.344553650241537
    },
    {
      "type": "training",
      "description": "Training step 4266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:23",
      "total_flops_so_far": 2.535049470829363e+16,
      "budget_used_percent": 25.35049470829363
    },
    {
      "type": "training",
      "description": "Training step 4267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:24",
      "total_flops_so_far": 2.535643576634573e+16,
      "budget_used_percent": 25.35643576634573
    },
    {
      "type": "training",
      "description": "Training step 4268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:24",
      "total_flops_so_far": 2.5362376824397824e+16,
      "budget_used_percent": 25.362376824397824
    },
    {
      "type": "training",
      "description": "Training step 4269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:25",
      "total_flops_so_far": 2.536831788244992e+16,
      "budget_used_percent": 25.36831788244992
    },
    {
      "type": "training",
      "description": "Training step 4270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:25",
      "total_flops_so_far": 2.5374258940502016e+16,
      "budget_used_percent": 25.374258940502013
    },
    {
      "type": "training",
      "description": "Training step 4271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:25",
      "total_flops_so_far": 2.538019999855411e+16,
      "budget_used_percent": 25.380199998554108
    },
    {
      "type": "training",
      "description": "Training step 4272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:26",
      "total_flops_so_far": 2.538614105660621e+16,
      "budget_used_percent": 25.38614105660621
    },
    {
      "type": "training",
      "description": "Training step 4273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:26",
      "total_flops_so_far": 2.5392082114658304e+16,
      "budget_used_percent": 25.392082114658304
    },
    {
      "type": "training",
      "description": "Training step 4274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:27",
      "total_flops_so_far": 2.53980231727104e+16,
      "budget_used_percent": 25.3980231727104
    },
    {
      "type": "training",
      "description": "Training step 4275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:27",
      "total_flops_so_far": 2.5403964230762496e+16,
      "budget_used_percent": 25.403964230762494
    },
    {
      "type": "training",
      "description": "Training step 4276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:27",
      "total_flops_so_far": 2.540990528881459e+16,
      "budget_used_percent": 25.409905288814592
    },
    {
      "type": "training",
      "description": "Training step 4277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:28",
      "total_flops_so_far": 2.541584634686669e+16,
      "budget_used_percent": 25.415846346866687
    },
    {
      "type": "training",
      "description": "Training step 4278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:28",
      "total_flops_so_far": 2.5421787404918784e+16,
      "budget_used_percent": 25.42178740491878
    },
    {
      "type": "training",
      "description": "Training step 4279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:29",
      "total_flops_so_far": 2.542772846297088e+16,
      "budget_used_percent": 25.427728462970876
    },
    {
      "type": "training",
      "description": "Training step 4280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:29",
      "total_flops_so_far": 2.5433669521022976e+16,
      "budget_used_percent": 25.433669521022978
    },
    {
      "type": "training",
      "description": "Training step 4281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:29",
      "total_flops_so_far": 2.543961057907507e+16,
      "budget_used_percent": 25.439610579075072
    },
    {
      "type": "training",
      "description": "Training step 4282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:30",
      "total_flops_so_far": 2.544555163712717e+16,
      "budget_used_percent": 25.445551637127167
    },
    {
      "type": "training",
      "description": "Training step 4283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:30",
      "total_flops_so_far": 2.5451492695179264e+16,
      "budget_used_percent": 25.45149269517926
    },
    {
      "type": "training",
      "description": "Training step 4284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:31",
      "total_flops_so_far": 2.545743375323136e+16,
      "budget_used_percent": 25.457433753231363
    },
    {
      "type": "training",
      "description": "Training step 4285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:31",
      "total_flops_so_far": 2.5463374811283456e+16,
      "budget_used_percent": 25.463374811283458
    },
    {
      "type": "training",
      "description": "Training step 4286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:32",
      "total_flops_so_far": 2.546931586933555e+16,
      "budget_used_percent": 25.469315869335553
    },
    {
      "type": "training",
      "description": "Training step 4287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:32",
      "total_flops_so_far": 2.547525692738765e+16,
      "budget_used_percent": 25.475256927387647
    },
    {
      "type": "training",
      "description": "Training step 4288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:33",
      "total_flops_so_far": 2.5481197985439744e+16,
      "budget_used_percent": 25.481197985439746
    },
    {
      "type": "training",
      "description": "Training step 4289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:33",
      "total_flops_so_far": 2.548713904349184e+16,
      "budget_used_percent": 25.48713904349184
    },
    {
      "type": "training",
      "description": "Training step 4290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:33",
      "total_flops_so_far": 2.5493080101543936e+16,
      "budget_used_percent": 25.493080101543935
    },
    {
      "type": "training",
      "description": "Training step 4291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:34",
      "total_flops_so_far": 2.549902115959603e+16,
      "budget_used_percent": 25.49902115959603
    },
    {
      "type": "training",
      "description": "Training step 4292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:34",
      "total_flops_so_far": 2.550496221764813e+16,
      "budget_used_percent": 25.50496221764813
    },
    {
      "type": "training",
      "description": "Training step 4293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:35",
      "total_flops_so_far": 2.5510903275700224e+16,
      "budget_used_percent": 25.510903275700226
    },
    {
      "type": "training",
      "description": "Training step 4294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:35",
      "total_flops_so_far": 2.551684433375232e+16,
      "budget_used_percent": 25.51684433375232
    },
    {
      "type": "training",
      "description": "Training step 4295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:35",
      "total_flops_so_far": 2.5522785391804416e+16,
      "budget_used_percent": 25.522785391804415
    },
    {
      "type": "training",
      "description": "Training step 4296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:36",
      "total_flops_so_far": 2.552872644985651e+16,
      "budget_used_percent": 25.528726449856514
    },
    {
      "type": "training",
      "description": "Training step 4297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:36",
      "total_flops_so_far": 2.553466750790861e+16,
      "budget_used_percent": 25.53466750790861
    },
    {
      "type": "training",
      "description": "Training step 4298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:37",
      "total_flops_so_far": 2.5540608565960704e+16,
      "budget_used_percent": 25.540608565960703
    },
    {
      "type": "training",
      "description": "Training step 4299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:37",
      "total_flops_so_far": 2.55465496240128e+16,
      "budget_used_percent": 25.546549624012798
    },
    {
      "type": "training",
      "description": "Training step 4300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:37",
      "total_flops_so_far": 2.5552490682064896e+16,
      "budget_used_percent": 25.552490682064892
    },
    {
      "type": "training",
      "description": "Training step 4301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:38",
      "total_flops_so_far": 2.555843174011699e+16,
      "budget_used_percent": 25.558431740116994
    },
    {
      "type": "training",
      "description": "Training step 4302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:38",
      "total_flops_so_far": 2.556437279816909e+16,
      "budget_used_percent": 25.56437279816909
    },
    {
      "type": "training",
      "description": "Training step 4303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:39",
      "total_flops_so_far": 2.5570313856221184e+16,
      "budget_used_percent": 25.570313856221183
    },
    {
      "type": "training",
      "description": "Training step 4304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:39",
      "total_flops_so_far": 2.557625491427328e+16,
      "budget_used_percent": 25.576254914273278
    },
    {
      "type": "training",
      "description": "Training step 4305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:39",
      "total_flops_so_far": 2.5582195972325376e+16,
      "budget_used_percent": 25.582195972325376
    },
    {
      "type": "training",
      "description": "Training step 4306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:40",
      "total_flops_so_far": 2.558813703037747e+16,
      "budget_used_percent": 25.58813703037747
    },
    {
      "type": "training",
      "description": "Training step 4307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:40",
      "total_flops_so_far": 2.559407808842957e+16,
      "budget_used_percent": 25.594078088429566
    },
    {
      "type": "training",
      "description": "Training step 4308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:41",
      "total_flops_so_far": 2.5600019146481664e+16,
      "budget_used_percent": 25.60001914648166
    },
    {
      "type": "training",
      "description": "Training step 4309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:41",
      "total_flops_so_far": 2.560596020453376e+16,
      "budget_used_percent": 25.605960204533762
    },
    {
      "type": "training",
      "description": "Training step 4310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:42",
      "total_flops_so_far": 2.5611901262585856e+16,
      "budget_used_percent": 25.611901262585857
    },
    {
      "type": "training",
      "description": "Training step 4311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:42",
      "total_flops_so_far": 2.561784232063795e+16,
      "budget_used_percent": 25.61784232063795
    },
    {
      "type": "training",
      "description": "Training step 4312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:43",
      "total_flops_so_far": 2.562378337869005e+16,
      "budget_used_percent": 25.623783378690046
    },
    {
      "type": "training",
      "description": "Training step 4313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:43",
      "total_flops_so_far": 2.5629724436742144e+16,
      "budget_used_percent": 25.629724436742148
    },
    {
      "type": "training",
      "description": "Training step 4314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:43",
      "total_flops_so_far": 2.563566549479424e+16,
      "budget_used_percent": 25.635665494794242
    },
    {
      "type": "training",
      "description": "Training step 4315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:44",
      "total_flops_so_far": 2.5641606552846336e+16,
      "budget_used_percent": 25.641606552846337
    },
    {
      "type": "training",
      "description": "Training step 4316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:44",
      "total_flops_so_far": 2.564754761089843e+16,
      "budget_used_percent": 25.64754761089843
    },
    {
      "type": "training",
      "description": "Training step 4317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:45",
      "total_flops_so_far": 2.565348866895053e+16,
      "budget_used_percent": 25.65348866895053
    },
    {
      "type": "training",
      "description": "Training step 4318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:45",
      "total_flops_so_far": 2.5659429727002624e+16,
      "budget_used_percent": 25.659429727002625
    },
    {
      "type": "training",
      "description": "Training step 4319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:45",
      "total_flops_so_far": 2.566537078505472e+16,
      "budget_used_percent": 25.66537078505472
    },
    {
      "type": "training",
      "description": "Training step 4320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:46",
      "total_flops_so_far": 2.5671311843106816e+16,
      "budget_used_percent": 25.671311843106814
    },
    {
      "type": "training",
      "description": "Training step 4321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:46",
      "total_flops_so_far": 2.567725290115891e+16,
      "budget_used_percent": 25.677252901158916
    },
    {
      "type": "training",
      "description": "Training step 4322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:47",
      "total_flops_so_far": 2.568319395921101e+16,
      "budget_used_percent": 25.68319395921101
    },
    {
      "type": "training",
      "description": "Training step 4323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:47",
      "total_flops_so_far": 2.5689135017263104e+16,
      "budget_used_percent": 25.689135017263105
    },
    {
      "type": "training",
      "description": "Training step 4324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:48",
      "total_flops_so_far": 2.56950760753152e+16,
      "budget_used_percent": 25.6950760753152
    },
    {
      "type": "training",
      "description": "Training step 4325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:48",
      "total_flops_so_far": 2.5701017133367296e+16,
      "budget_used_percent": 25.701017133367298
    },
    {
      "type": "training",
      "description": "Training step 4326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:48",
      "total_flops_so_far": 2.570695819141939e+16,
      "budget_used_percent": 25.706958191419393
    },
    {
      "type": "training",
      "description": "Training step 4327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:49",
      "total_flops_so_far": 2.571289924947149e+16,
      "budget_used_percent": 25.712899249471487
    },
    {
      "type": "training",
      "description": "Training step 4328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:49",
      "total_flops_so_far": 2.5718840307523584e+16,
      "budget_used_percent": 25.718840307523582
    },
    {
      "type": "training",
      "description": "Training step 4329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:50",
      "total_flops_so_far": 2.572478136557568e+16,
      "budget_used_percent": 25.724781365575677
    },
    {
      "type": "training",
      "description": "Training step 4330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:50",
      "total_flops_so_far": 2.5730722423627776e+16,
      "budget_used_percent": 25.73072242362778
    },
    {
      "type": "training",
      "description": "Training step 4331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:50",
      "total_flops_so_far": 2.573666348167987e+16,
      "budget_used_percent": 25.736663481679873
    },
    {
      "type": "training",
      "description": "Training step 4332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:51",
      "total_flops_so_far": 2.574260453973197e+16,
      "budget_used_percent": 25.742604539731968
    },
    {
      "type": "training",
      "description": "Training step 4333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:51",
      "total_flops_so_far": 2.5748545597784064e+16,
      "budget_used_percent": 25.748545597784062
    },
    {
      "type": "training",
      "description": "Training step 4334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:52",
      "total_flops_so_far": 2.575448665583616e+16,
      "budget_used_percent": 25.75448665583616
    },
    {
      "type": "training",
      "description": "Training step 4335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:52",
      "total_flops_so_far": 2.5760427713888256e+16,
      "budget_used_percent": 25.760427713888255
    },
    {
      "type": "training",
      "description": "Training step 4336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:52",
      "total_flops_so_far": 2.576636877194035e+16,
      "budget_used_percent": 25.76636877194035
    },
    {
      "type": "training",
      "description": "Training step 4337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:53",
      "total_flops_so_far": 2.577230982999245e+16,
      "budget_used_percent": 25.772309829992444
    },
    {
      "type": "training",
      "description": "Training step 4338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:53",
      "total_flops_so_far": 2.5778250888044544e+16,
      "budget_used_percent": 25.778250888044546
    },
    {
      "type": "training",
      "description": "Training step 4339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:54",
      "total_flops_so_far": 2.578419194609664e+16,
      "budget_used_percent": 25.78419194609664
    },
    {
      "type": "training",
      "description": "Training step 4340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:54",
      "total_flops_so_far": 2.5790133004148736e+16,
      "budget_used_percent": 25.790133004148736
    },
    {
      "type": "training",
      "description": "Training step 4341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:54",
      "total_flops_so_far": 2.579607406220083e+16,
      "budget_used_percent": 25.79607406220083
    },
    {
      "type": "training",
      "description": "Training step 4342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:55",
      "total_flops_so_far": 2.580201512025293e+16,
      "budget_used_percent": 25.802015120252932
    },
    {
      "type": "training",
      "description": "Training step 4343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:55",
      "total_flops_so_far": 2.5807956178305024e+16,
      "budget_used_percent": 25.807956178305027
    },
    {
      "type": "training",
      "description": "Training step 4344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:56",
      "total_flops_so_far": 2.581389723635712e+16,
      "budget_used_percent": 25.81389723635712
    },
    {
      "type": "training",
      "description": "Training step 4345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:56",
      "total_flops_so_far": 2.5819838294409216e+16,
      "budget_used_percent": 25.819838294409216
    },
    {
      "type": "training",
      "description": "Training step 4346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:56",
      "total_flops_so_far": 2.582577935246131e+16,
      "budget_used_percent": 25.825779352461314
    },
    {
      "type": "training",
      "description": "Training step 4347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:57",
      "total_flops_so_far": 2.583172041051341e+16,
      "budget_used_percent": 25.83172041051341
    },
    {
      "type": "training",
      "description": "Training step 4348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:57",
      "total_flops_so_far": 2.5837661468565504e+16,
      "budget_used_percent": 25.837661468565503
    },
    {
      "type": "training",
      "description": "Training step 4349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:58",
      "total_flops_so_far": 2.58436025266176e+16,
      "budget_used_percent": 25.843602526617598
    },
    {
      "type": "training",
      "description": "Training step 4350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:58",
      "total_flops_so_far": 2.5849543584669696e+16,
      "budget_used_percent": 25.8495435846697
    },
    {
      "type": "training",
      "description": "Training step 4351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:59",
      "total_flops_so_far": 2.585548464272179e+16,
      "budget_used_percent": 25.855484642721795
    },
    {
      "type": "training",
      "description": "Training step 4352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:59",
      "total_flops_so_far": 2.586142570077389e+16,
      "budget_used_percent": 25.86142570077389
    },
    {
      "type": "training",
      "description": "Training step 4353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:02:59",
      "total_flops_so_far": 2.5867366758825984e+16,
      "budget_used_percent": 25.867366758825984
    },
    {
      "type": "training",
      "description": "Training step 4354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:00",
      "total_flops_so_far": 2.587330781687808e+16,
      "budget_used_percent": 25.87330781687808
    },
    {
      "type": "training",
      "description": "Training step 4355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:00",
      "total_flops_so_far": 2.5879248874930176e+16,
      "budget_used_percent": 25.879248874930177
    },
    {
      "type": "training",
      "description": "Training step 4356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:01",
      "total_flops_so_far": 2.588518993298227e+16,
      "budget_used_percent": 25.88518993298227
    },
    {
      "type": "training",
      "description": "Training step 4357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:01",
      "total_flops_so_far": 2.589113099103437e+16,
      "budget_used_percent": 25.891130991034366
    },
    {
      "type": "training",
      "description": "Training step 4358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:01",
      "total_flops_so_far": 2.5897072049086464e+16,
      "budget_used_percent": 25.89707204908646
    },
    {
      "type": "training",
      "description": "Training step 4359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:02",
      "total_flops_so_far": 2.590301310713856e+16,
      "budget_used_percent": 25.903013107138563
    },
    {
      "type": "training",
      "description": "Training step 4360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:02",
      "total_flops_so_far": 2.5908954165190656e+16,
      "budget_used_percent": 25.908954165190657
    },
    {
      "type": "training",
      "description": "Training step 4361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:03",
      "total_flops_so_far": 2.591489522324275e+16,
      "budget_used_percent": 25.914895223242752
    },
    {
      "type": "training",
      "description": "Training step 4362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:03",
      "total_flops_so_far": 2.592083628129485e+16,
      "budget_used_percent": 25.920836281294847
    },
    {
      "type": "training",
      "description": "Training step 4363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:03",
      "total_flops_so_far": 2.5926777339346944e+16,
      "budget_used_percent": 25.926777339346945
    },
    {
      "type": "training",
      "description": "Training step 4364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:04",
      "total_flops_so_far": 2.593271839739904e+16,
      "budget_used_percent": 25.93271839739904
    },
    {
      "type": "training",
      "description": "Training step 4365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:04",
      "total_flops_so_far": 2.5938659455451136e+16,
      "budget_used_percent": 25.938659455451134
    },
    {
      "type": "training",
      "description": "Training step 4366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:05",
      "total_flops_so_far": 2.594460051350323e+16,
      "budget_used_percent": 25.94460051350323
    },
    {
      "type": "training",
      "description": "Training step 4367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:05",
      "total_flops_so_far": 2.595054157155533e+16,
      "budget_used_percent": 25.95054157155533
    },
    {
      "type": "training",
      "description": "Training step 4368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:05",
      "total_flops_so_far": 2.5956482629607424e+16,
      "budget_used_percent": 25.956482629607425
    },
    {
      "type": "training",
      "description": "Training step 4369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:06",
      "total_flops_so_far": 2.596242368765952e+16,
      "budget_used_percent": 25.96242368765952
    },
    {
      "type": "training",
      "description": "Training step 4370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:06",
      "total_flops_so_far": 2.5968364745711616e+16,
      "budget_used_percent": 25.968364745711614
    },
    {
      "type": "training",
      "description": "Training step 4371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:07",
      "total_flops_so_far": 2.597430580376371e+16,
      "budget_used_percent": 25.974305803763713
    },
    {
      "type": "training",
      "description": "Training step 4372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:07",
      "total_flops_so_far": 2.598024686181581e+16,
      "budget_used_percent": 25.980246861815807
    },
    {
      "type": "training",
      "description": "Training step 4373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:07",
      "total_flops_so_far": 2.5986187919867904e+16,
      "budget_used_percent": 25.986187919867902
    },
    {
      "type": "training",
      "description": "Training step 4374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:08",
      "total_flops_so_far": 2.599212897792e+16,
      "budget_used_percent": 25.992128977919997
    },
    {
      "type": "training",
      "description": "Training step 4375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:08",
      "total_flops_so_far": 2.5998070035972096e+16,
      "budget_used_percent": 25.9980700359721
    },
    {
      "type": "training",
      "description": "Training step 4376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:09",
      "total_flops_so_far": 2.600401109402419e+16,
      "budget_used_percent": 26.004011094024193
    },
    {
      "type": "training",
      "description": "Training step 4377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:09",
      "total_flops_so_far": 2.600995215207629e+16,
      "budget_used_percent": 26.009952152076288
    },
    {
      "type": "training",
      "description": "Training step 4378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:10",
      "total_flops_so_far": 2.6015893210128384e+16,
      "budget_used_percent": 26.015893210128382
    },
    {
      "type": "training",
      "description": "Training step 4379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:10",
      "total_flops_so_far": 2.602183426818048e+16,
      "budget_used_percent": 26.021834268180484
    },
    {
      "type": "training",
      "description": "Training step 4380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:10",
      "total_flops_so_far": 2.6027775326232576e+16,
      "budget_used_percent": 26.02777532623258
    },
    {
      "type": "training",
      "description": "Training step 4381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:11",
      "total_flops_so_far": 2.603371638428467e+16,
      "budget_used_percent": 26.033716384284673
    },
    {
      "type": "training",
      "description": "Training step 4382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:11",
      "total_flops_so_far": 2.603965744233677e+16,
      "budget_used_percent": 26.039657442336768
    },
    {
      "type": "training",
      "description": "Training step 4383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:12",
      "total_flops_so_far": 2.6045598500388864e+16,
      "budget_used_percent": 26.045598500388863
    },
    {
      "type": "training",
      "description": "Training step 4384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:12",
      "total_flops_so_far": 2.605153955844096e+16,
      "budget_used_percent": 26.05153955844096
    },
    {
      "type": "training",
      "description": "Training step 4385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:12",
      "total_flops_so_far": 2.6057480616493056e+16,
      "budget_used_percent": 26.057480616493056
    },
    {
      "type": "training",
      "description": "Training step 4386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:13",
      "total_flops_so_far": 2.606342167454515e+16,
      "budget_used_percent": 26.06342167454515
    },
    {
      "type": "training",
      "description": "Training step 4387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:13",
      "total_flops_so_far": 2.606936273259725e+16,
      "budget_used_percent": 26.069362732597245
    },
    {
      "type": "training",
      "description": "Training step 4388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:14",
      "total_flops_so_far": 2.6075303790649344e+16,
      "budget_used_percent": 26.075303790649347
    },
    {
      "type": "training",
      "description": "Training step 4389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:14",
      "total_flops_so_far": 2.608124484870144e+16,
      "budget_used_percent": 26.08124484870144
    },
    {
      "type": "training",
      "description": "Training step 4390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:14",
      "total_flops_so_far": 2.6087185906753536e+16,
      "budget_used_percent": 26.087185906753536
    },
    {
      "type": "training",
      "description": "Training step 4391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:15",
      "total_flops_so_far": 2.609312696480563e+16,
      "budget_used_percent": 26.09312696480563
    },
    {
      "type": "training",
      "description": "Training step 4392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:15",
      "total_flops_so_far": 2.609906802285773e+16,
      "budget_used_percent": 26.09906802285773
    },
    {
      "type": "training",
      "description": "Training step 4393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:16",
      "total_flops_so_far": 2.6105009080909824e+16,
      "budget_used_percent": 26.105009080909824
    },
    {
      "type": "training",
      "description": "Training step 4394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:16",
      "total_flops_so_far": 2.611095013896192e+16,
      "budget_used_percent": 26.11095013896192
    },
    {
      "type": "training",
      "description": "Training step 4395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:16",
      "total_flops_so_far": 2.6116891197014016e+16,
      "budget_used_percent": 26.116891197014013
    },
    {
      "type": "training",
      "description": "Training step 4396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:17",
      "total_flops_so_far": 2.612283225506611e+16,
      "budget_used_percent": 26.122832255066115
    },
    {
      "type": "training",
      "description": "Training step 4397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:17",
      "total_flops_so_far": 2.612877331311821e+16,
      "budget_used_percent": 26.12877331311821
    },
    {
      "type": "training",
      "description": "Training step 4398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:18",
      "total_flops_so_far": 2.6134714371170304e+16,
      "budget_used_percent": 26.134714371170304
    },
    {
      "type": "training",
      "description": "Training step 4399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:18",
      "total_flops_so_far": 2.61406554292224e+16,
      "budget_used_percent": 26.1406554292224
    },
    {
      "type": "training",
      "description": "Training step 4400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:19",
      "total_flops_so_far": 2.6146596487274496e+16,
      "budget_used_percent": 26.146596487274497
    },
    {
      "type": "training",
      "description": "Training step 4401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:19",
      "total_flops_so_far": 2.615253754532659e+16,
      "budget_used_percent": 26.15253754532659
    },
    {
      "type": "training",
      "description": "Training step 4402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:19",
      "total_flops_so_far": 2.615847860337869e+16,
      "budget_used_percent": 26.158478603378686
    },
    {
      "type": "training",
      "description": "Training step 4403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:20",
      "total_flops_so_far": 2.6164419661430784e+16,
      "budget_used_percent": 26.16441966143078
    },
    {
      "type": "training",
      "description": "Training step 4404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:20",
      "total_flops_so_far": 2.617036071948288e+16,
      "budget_used_percent": 26.170360719482883
    },
    {
      "type": "training",
      "description": "Training step 4405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:21",
      "total_flops_so_far": 2.6176301777534976e+16,
      "budget_used_percent": 26.176301777534977
    },
    {
      "type": "training",
      "description": "Training step 4406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:21",
      "total_flops_so_far": 2.618224283558707e+16,
      "budget_used_percent": 26.182242835587072
    },
    {
      "type": "training",
      "description": "Training step 4407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:21",
      "total_flops_so_far": 2.618818389363917e+16,
      "budget_used_percent": 26.188183893639167
    },
    {
      "type": "training",
      "description": "Training step 4408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:22",
      "total_flops_so_far": 2.6194124951691264e+16,
      "budget_used_percent": 26.194124951691265
    },
    {
      "type": "training",
      "description": "Training step 4409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:22",
      "total_flops_so_far": 2.620006600974336e+16,
      "budget_used_percent": 26.20006600974336
    },
    {
      "type": "training",
      "description": "Training step 4410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:23",
      "total_flops_so_far": 2.6206007067795456e+16,
      "budget_used_percent": 26.206007067795454
    },
    {
      "type": "training",
      "description": "Training step 4411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:23",
      "total_flops_so_far": 2.621194812584755e+16,
      "budget_used_percent": 26.21194812584755
    },
    {
      "type": "training",
      "description": "Training step 4412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:23",
      "total_flops_so_far": 2.621788918389965e+16,
      "budget_used_percent": 26.217889183899644
    },
    {
      "type": "training",
      "description": "Training step 4413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:24",
      "total_flops_so_far": 2.6223830241951744e+16,
      "budget_used_percent": 26.223830241951745
    },
    {
      "type": "training",
      "description": "Training step 4414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:24",
      "total_flops_so_far": 2.622977130000384e+16,
      "budget_used_percent": 26.22977130000384
    },
    {
      "type": "training",
      "description": "Training step 4415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:25",
      "total_flops_so_far": 2.6235712358055936e+16,
      "budget_used_percent": 26.235712358055935
    },
    {
      "type": "training",
      "description": "Training step 4416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:25",
      "total_flops_so_far": 2.624165341610803e+16,
      "budget_used_percent": 26.24165341610803
    },
    {
      "type": "training",
      "description": "Training step 4417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:26",
      "total_flops_so_far": 2.624759447416013e+16,
      "budget_used_percent": 26.24759447416013
    },
    {
      "type": "training",
      "description": "Training step 4418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:26",
      "total_flops_so_far": 2.6253535532212224e+16,
      "budget_used_percent": 26.253535532212226
    },
    {
      "type": "training",
      "description": "Training step 4419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:26",
      "total_flops_so_far": 2.625947659026432e+16,
      "budget_used_percent": 26.25947659026432
    },
    {
      "type": "training",
      "description": "Training step 4420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:27",
      "total_flops_so_far": 2.6265417648316416e+16,
      "budget_used_percent": 26.265417648316415
    },
    {
      "type": "training",
      "description": "Training step 4421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:27",
      "total_flops_so_far": 2.627135870636851e+16,
      "budget_used_percent": 26.271358706368513
    },
    {
      "type": "training",
      "description": "Training step 4422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:28",
      "total_flops_so_far": 2.627729976442061e+16,
      "budget_used_percent": 26.277299764420608
    },
    {
      "type": "training",
      "description": "Training step 4423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:28",
      "total_flops_so_far": 2.6283240822472704e+16,
      "budget_used_percent": 26.283240822472703
    },
    {
      "type": "training",
      "description": "Training step 4424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:28",
      "total_flops_so_far": 2.62891818805248e+16,
      "budget_used_percent": 26.289181880524797
    },
    {
      "type": "training",
      "description": "Training step 4425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:29",
      "total_flops_so_far": 2.6295122938576896e+16,
      "budget_used_percent": 26.2951229385769
    },
    {
      "type": "training",
      "description": "Training step 4426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:29",
      "total_flops_so_far": 2.630106399662899e+16,
      "budget_used_percent": 26.301063996628994
    },
    {
      "type": "training",
      "description": "Training step 4427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:30",
      "total_flops_so_far": 2.630700505468109e+16,
      "budget_used_percent": 26.30700505468109
    },
    {
      "type": "training",
      "description": "Training step 4428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:30",
      "total_flops_so_far": 2.6312946112733184e+16,
      "budget_used_percent": 26.312946112733183
    },
    {
      "type": "training",
      "description": "Training step 4429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:30",
      "total_flops_so_far": 2.631888717078528e+16,
      "budget_used_percent": 26.31888717078528
    },
    {
      "type": "training",
      "description": "Training step 4430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:31",
      "total_flops_so_far": 2.6324828228837376e+16,
      "budget_used_percent": 26.324828228837376
    },
    {
      "type": "training",
      "description": "Training step 4431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:31",
      "total_flops_so_far": 2.633076928688947e+16,
      "budget_used_percent": 26.33076928688947
    },
    {
      "type": "training",
      "description": "Training step 4432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:32",
      "total_flops_so_far": 2.633671034494157e+16,
      "budget_used_percent": 26.336710344941565
    },
    {
      "type": "training",
      "description": "Training step 4433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:32",
      "total_flops_so_far": 2.6342651402993664e+16,
      "budget_used_percent": 26.342651402993667
    },
    {
      "type": "training",
      "description": "Training step 4434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:32",
      "total_flops_so_far": 2.634859246104576e+16,
      "budget_used_percent": 26.34859246104576
    },
    {
      "type": "training",
      "description": "Training step 4435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:33",
      "total_flops_so_far": 2.6354533519097856e+16,
      "budget_used_percent": 26.354533519097856
    },
    {
      "type": "training",
      "description": "Training step 4436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:33",
      "total_flops_so_far": 2.636047457714995e+16,
      "budget_used_percent": 26.36047457714995
    },
    {
      "type": "training",
      "description": "Training step 4437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:34",
      "total_flops_so_far": 2.636641563520205e+16,
      "budget_used_percent": 26.36641563520205
    },
    {
      "type": "training",
      "description": "Training step 4438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:34",
      "total_flops_so_far": 2.6372356693254144e+16,
      "budget_used_percent": 26.372356693254144
    },
    {
      "type": "training",
      "description": "Training step 4439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:35",
      "total_flops_so_far": 2.637829775130624e+16,
      "budget_used_percent": 26.37829775130624
    },
    {
      "type": "training",
      "description": "Training step 4440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:35",
      "total_flops_so_far": 2.6384238809358336e+16,
      "budget_used_percent": 26.384238809358333
    },
    {
      "type": "training",
      "description": "Training step 4441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:35",
      "total_flops_so_far": 2.639017986741043e+16,
      "budget_used_percent": 26.390179867410428
    },
    {
      "type": "training",
      "description": "Training step 4442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:36",
      "total_flops_so_far": 2.639612092546253e+16,
      "budget_used_percent": 26.39612092546253
    },
    {
      "type": "training",
      "description": "Training step 4443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:36",
      "total_flops_so_far": 2.6402061983514624e+16,
      "budget_used_percent": 26.402061983514624
    },
    {
      "type": "training",
      "description": "Training step 4444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:37",
      "total_flops_so_far": 2.640800304156672e+16,
      "budget_used_percent": 26.40800304156672
    },
    {
      "type": "training",
      "description": "Training step 4445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:37",
      "total_flops_so_far": 2.6413944099618816e+16,
      "budget_used_percent": 26.413944099618814
    },
    {
      "type": "training",
      "description": "Training step 4446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:37",
      "total_flops_so_far": 2.641988515767091e+16,
      "budget_used_percent": 26.419885157670915
    },
    {
      "type": "training",
      "description": "Training step 4447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:38",
      "total_flops_so_far": 2.642582621572301e+16,
      "budget_used_percent": 26.42582621572301
    },
    {
      "type": "training",
      "description": "Training step 4448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:38",
      "total_flops_so_far": 2.6431767273775104e+16,
      "budget_used_percent": 26.431767273775105
    },
    {
      "type": "training",
      "description": "Training step 4449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:39",
      "total_flops_so_far": 2.64377083318272e+16,
      "budget_used_percent": 26.4377083318272
    },
    {
      "type": "training",
      "description": "Training step 4450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:39",
      "total_flops_so_far": 2.6443649389879296e+16,
      "budget_used_percent": 26.443649389879297
    },
    {
      "type": "training",
      "description": "Training step 4451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:39",
      "total_flops_so_far": 2.644959044793139e+16,
      "budget_used_percent": 26.449590447931392
    },
    {
      "type": "training",
      "description": "Training step 4452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:40",
      "total_flops_so_far": 2.645553150598349e+16,
      "budget_used_percent": 26.455531505983487
    },
    {
      "type": "training",
      "description": "Training step 4453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:40",
      "total_flops_so_far": 2.6461472564035584e+16,
      "budget_used_percent": 26.46147256403558
    },
    {
      "type": "training",
      "description": "Training step 4454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:41",
      "total_flops_so_far": 2.646741362208768e+16,
      "budget_used_percent": 26.467413622087683
    },
    {
      "type": "training",
      "description": "Training step 4455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:41",
      "total_flops_so_far": 2.6473354680139776e+16,
      "budget_used_percent": 26.473354680139778
    },
    {
      "type": "training",
      "description": "Training step 4456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:41",
      "total_flops_so_far": 2.647929573819187e+16,
      "budget_used_percent": 26.479295738191873
    },
    {
      "type": "training",
      "description": "Training step 4457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:42",
      "total_flops_so_far": 2.648523679624397e+16,
      "budget_used_percent": 26.485236796243967
    },
    {
      "type": "training",
      "description": "Training step 4458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:42",
      "total_flops_so_far": 2.6491177854296064e+16,
      "budget_used_percent": 26.491177854296065
    },
    {
      "type": "training",
      "description": "Training step 4459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:43",
      "total_flops_so_far": 2.649711891234816e+16,
      "budget_used_percent": 26.49711891234816
    },
    {
      "type": "training",
      "description": "Training step 4460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:43",
      "total_flops_so_far": 2.6503059970400256e+16,
      "budget_used_percent": 26.503059970400255
    },
    {
      "type": "training",
      "description": "Training step 4461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:44",
      "total_flops_so_far": 2.650900102845235e+16,
      "budget_used_percent": 26.50900102845235
    },
    {
      "type": "training",
      "description": "Training step 4462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:44",
      "total_flops_so_far": 2.651494208650445e+16,
      "budget_used_percent": 26.51494208650445
    },
    {
      "type": "training",
      "description": "Training step 4463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:44",
      "total_flops_so_far": 2.6520883144556544e+16,
      "budget_used_percent": 26.520883144556546
    },
    {
      "type": "training",
      "description": "Training step 4464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:45",
      "total_flops_so_far": 2.652682420260864e+16,
      "budget_used_percent": 26.52682420260864
    },
    {
      "type": "training",
      "description": "Training step 4465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:45",
      "total_flops_so_far": 2.6532765260660736e+16,
      "budget_used_percent": 26.532765260660735
    },
    {
      "type": "training",
      "description": "Training step 4466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:46",
      "total_flops_so_far": 2.653870631871283e+16,
      "budget_used_percent": 26.538706318712833
    },
    {
      "type": "training",
      "description": "Training step 4467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:46",
      "total_flops_so_far": 2.654464737676493e+16,
      "budget_used_percent": 26.544647376764928
    },
    {
      "type": "training",
      "description": "Training step 4468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:46",
      "total_flops_so_far": 2.6550588434817024e+16,
      "budget_used_percent": 26.550588434817023
    },
    {
      "type": "training",
      "description": "Training step 4469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:47",
      "total_flops_so_far": 2.655652949286912e+16,
      "budget_used_percent": 26.556529492869117
    },
    {
      "type": "training",
      "description": "Training step 4470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:47",
      "total_flops_so_far": 2.6562470550921216e+16,
      "budget_used_percent": 26.562470550921212
    },
    {
      "type": "training",
      "description": "Training step 4471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:48",
      "total_flops_so_far": 2.656841160897331e+16,
      "budget_used_percent": 26.568411608973314
    },
    {
      "type": "training",
      "description": "Training step 4472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:48",
      "total_flops_so_far": 2.657435266702541e+16,
      "budget_used_percent": 26.57435266702541
    },
    {
      "type": "training",
      "description": "Training step 4473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:48",
      "total_flops_so_far": 2.6580293725077504e+16,
      "budget_used_percent": 26.580293725077503
    },
    {
      "type": "training",
      "description": "Training step 4474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:49",
      "total_flops_so_far": 2.65862347831296e+16,
      "budget_used_percent": 26.586234783129598
    },
    {
      "type": "training",
      "description": "Training step 4475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:49",
      "total_flops_so_far": 2.6592175841181696e+16,
      "budget_used_percent": 26.5921758411817
    },
    {
      "type": "training",
      "description": "Training step 4476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:50",
      "total_flops_so_far": 2.659811689923379e+16,
      "budget_used_percent": 26.598116899233794
    },
    {
      "type": "training",
      "description": "Training step 4477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:50",
      "total_flops_so_far": 2.660405795728589e+16,
      "budget_used_percent": 26.60405795728589
    },
    {
      "type": "training",
      "description": "Training step 4478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:51",
      "total_flops_so_far": 2.6609999015337984e+16,
      "budget_used_percent": 26.609999015337984
    },
    {
      "type": "training",
      "description": "Training step 4479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:51",
      "total_flops_so_far": 2.661594007339008e+16,
      "budget_used_percent": 26.61594007339008
    },
    {
      "type": "training",
      "description": "Training step 4480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:51",
      "total_flops_so_far": 2.6621881131442176e+16,
      "budget_used_percent": 26.621881131442176
    },
    {
      "type": "training",
      "description": "Training step 4481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:52",
      "total_flops_so_far": 2.662782218949427e+16,
      "budget_used_percent": 26.62782218949427
    },
    {
      "type": "training",
      "description": "Training step 4482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:52",
      "total_flops_so_far": 2.663376324754637e+16,
      "budget_used_percent": 26.633763247546366
    },
    {
      "type": "training",
      "description": "Training step 4483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:53",
      "total_flops_so_far": 2.6639704305598464e+16,
      "budget_used_percent": 26.639704305598467
    },
    {
      "type": "training",
      "description": "Training step 4484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:53",
      "total_flops_so_far": 2.664564536365056e+16,
      "budget_used_percent": 26.645645363650562
    },
    {
      "type": "training",
      "description": "Training step 4485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:53",
      "total_flops_so_far": 2.6651586421702656e+16,
      "budget_used_percent": 26.651586421702657
    },
    {
      "type": "training",
      "description": "Training step 4486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:54",
      "total_flops_so_far": 2.665752747975475e+16,
      "budget_used_percent": 26.65752747975475
    },
    {
      "type": "training",
      "description": "Training step 4487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:54",
      "total_flops_so_far": 2.666346853780685e+16,
      "budget_used_percent": 26.66346853780685
    },
    {
      "type": "training",
      "description": "Training step 4488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:55",
      "total_flops_so_far": 2.6669409595858944e+16,
      "budget_used_percent": 26.669409595858944
    },
    {
      "type": "training",
      "description": "Training step 4489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:55",
      "total_flops_so_far": 2.667535065391104e+16,
      "budget_used_percent": 26.67535065391104
    },
    {
      "type": "training",
      "description": "Training step 4490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:55",
      "total_flops_so_far": 2.6681291711963136e+16,
      "budget_used_percent": 26.681291711963134
    },
    {
      "type": "training",
      "description": "Training step 4491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:56",
      "total_flops_so_far": 2.668723277001523e+16,
      "budget_used_percent": 26.687232770015235
    },
    {
      "type": "training",
      "description": "Training step 4492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:56",
      "total_flops_so_far": 2.669317382806733e+16,
      "budget_used_percent": 26.69317382806733
    },
    {
      "type": "training",
      "description": "Training step 4493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:57",
      "total_flops_so_far": 2.6699114886119424e+16,
      "budget_used_percent": 26.699114886119425
    },
    {
      "type": "training",
      "description": "Training step 4494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:57",
      "total_flops_so_far": 2.670505594417152e+16,
      "budget_used_percent": 26.70505594417152
    },
    {
      "type": "training",
      "description": "Training step 4495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:57",
      "total_flops_so_far": 2.6710997002223616e+16,
      "budget_used_percent": 26.710997002223618
    },
    {
      "type": "training",
      "description": "Training step 4496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:58",
      "total_flops_so_far": 2.671693806027571e+16,
      "budget_used_percent": 26.716938060275712
    },
    {
      "type": "training",
      "description": "Training step 4497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:58",
      "total_flops_so_far": 2.672287911832781e+16,
      "budget_used_percent": 26.722879118327807
    },
    {
      "type": "training",
      "description": "Training step 4498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:59",
      "total_flops_so_far": 2.6728820176379904e+16,
      "budget_used_percent": 26.7288201763799
    },
    {
      "type": "training",
      "description": "Training step 4499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:03:59",
      "total_flops_so_far": 2.6734761234432e+16,
      "budget_used_percent": 26.734761234431996
    },
    {
      "type": "training",
      "description": "Training step 4500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:23",
      "total_flops_so_far": 2.6740702292484096e+16,
      "budget_used_percent": 26.740702292484098
    },
    {
      "type": "training",
      "description": "Training step 4501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:24",
      "total_flops_so_far": 2.674664335053619e+16,
      "budget_used_percent": 26.746643350536193
    },
    {
      "type": "training",
      "description": "Training step 4502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:24",
      "total_flops_so_far": 2.675258440858829e+16,
      "budget_used_percent": 26.752584408588287
    },
    {
      "type": "training",
      "description": "Training step 4503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:24",
      "total_flops_so_far": 2.6758525466640384e+16,
      "budget_used_percent": 26.758525466640382
    },
    {
      "type": "training",
      "description": "Training step 4504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:25",
      "total_flops_so_far": 2.676446652469248e+16,
      "budget_used_percent": 26.76446652469248
    },
    {
      "type": "training",
      "description": "Training step 4505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:25",
      "total_flops_so_far": 2.6770407582744576e+16,
      "budget_used_percent": 26.770407582744575
    },
    {
      "type": "training",
      "description": "Training step 4506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:26",
      "total_flops_so_far": 2.677634864079667e+16,
      "budget_used_percent": 26.77634864079667
    },
    {
      "type": "training",
      "description": "Training step 4507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:26",
      "total_flops_so_far": 2.678228969884877e+16,
      "budget_used_percent": 26.782289698848764
    },
    {
      "type": "training",
      "description": "Training step 4508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:27",
      "total_flops_so_far": 2.6788230756900864e+16,
      "budget_used_percent": 26.788230756900866
    },
    {
      "type": "training",
      "description": "Training step 4509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:27",
      "total_flops_so_far": 2.679417181495296e+16,
      "budget_used_percent": 26.79417181495296
    },
    {
      "type": "training",
      "description": "Training step 4510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:28",
      "total_flops_so_far": 2.6800112873005056e+16,
      "budget_used_percent": 26.800112873005055
    },
    {
      "type": "training",
      "description": "Training step 4511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:28",
      "total_flops_so_far": 2.680605393105715e+16,
      "budget_used_percent": 26.80605393105715
    },
    {
      "type": "training",
      "description": "Training step 4512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:28",
      "total_flops_so_far": 2.681199498910925e+16,
      "budget_used_percent": 26.81199498910925
    },
    {
      "type": "training",
      "description": "Training step 4513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:29",
      "total_flops_so_far": 2.6817936047161344e+16,
      "budget_used_percent": 26.817936047161346
    },
    {
      "type": "training",
      "description": "Training step 4514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:29",
      "total_flops_so_far": 2.682387710521344e+16,
      "budget_used_percent": 26.82387710521344
    },
    {
      "type": "training",
      "description": "Training step 4515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:30",
      "total_flops_so_far": 2.6829818163265536e+16,
      "budget_used_percent": 26.829818163265536
    },
    {
      "type": "training",
      "description": "Training step 4516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:30",
      "total_flops_so_far": 2.683575922131763e+16,
      "budget_used_percent": 26.835759221317634
    },
    {
      "type": "training",
      "description": "Training step 4517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:30",
      "total_flops_so_far": 2.684170027936973e+16,
      "budget_used_percent": 26.84170027936973
    },
    {
      "type": "training",
      "description": "Training step 4518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:31",
      "total_flops_so_far": 2.6847641337421824e+16,
      "budget_used_percent": 26.847641337421823
    },
    {
      "type": "training",
      "description": "Training step 4519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:31",
      "total_flops_so_far": 2.685358239547392e+16,
      "budget_used_percent": 26.853582395473918
    },
    {
      "type": "training",
      "description": "Training step 4520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:32",
      "total_flops_so_far": 2.6859523453526016e+16,
      "budget_used_percent": 26.85952345352602
    },
    {
      "type": "training",
      "description": "Training step 4521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:32",
      "total_flops_so_far": 2.686546451157811e+16,
      "budget_used_percent": 26.865464511578114
    },
    {
      "type": "training",
      "description": "Training step 4522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:32",
      "total_flops_so_far": 2.687140556963021e+16,
      "budget_used_percent": 26.87140556963021
    },
    {
      "type": "training",
      "description": "Training step 4523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:33",
      "total_flops_so_far": 2.6877346627682304e+16,
      "budget_used_percent": 26.877346627682304
    },
    {
      "type": "training",
      "description": "Training step 4524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:33",
      "total_flops_so_far": 2.68832876857344e+16,
      "budget_used_percent": 26.883287685734402
    },
    {
      "type": "training",
      "description": "Training step 4525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:34",
      "total_flops_so_far": 2.6889228743786496e+16,
      "budget_used_percent": 26.889228743786497
    },
    {
      "type": "training",
      "description": "Training step 4526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:34",
      "total_flops_so_far": 2.689516980183859e+16,
      "budget_used_percent": 26.89516980183859
    },
    {
      "type": "training",
      "description": "Training step 4527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:35",
      "total_flops_so_far": 2.690111085989069e+16,
      "budget_used_percent": 26.901110859890686
    },
    {
      "type": "training",
      "description": "Training step 4528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:35",
      "total_flops_so_far": 2.6907051917942784e+16,
      "budget_used_percent": 26.90705191794278
    },
    {
      "type": "training",
      "description": "Training step 4529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:35",
      "total_flops_so_far": 2.691299297599488e+16,
      "budget_used_percent": 26.912992975994882
    },
    {
      "type": "training",
      "description": "Training step 4530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:36",
      "total_flops_so_far": 2.6918934034046976e+16,
      "budget_used_percent": 26.918934034046977
    },
    {
      "type": "training",
      "description": "Training step 4531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:36",
      "total_flops_so_far": 2.692487509209907e+16,
      "budget_used_percent": 26.92487509209907
    },
    {
      "type": "training",
      "description": "Training step 4532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:37",
      "total_flops_so_far": 2.693081615015117e+16,
      "budget_used_percent": 26.930816150151166
    },
    {
      "type": "training",
      "description": "Training step 4533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:37",
      "total_flops_so_far": 2.6936757208203264e+16,
      "budget_used_percent": 26.936757208203264
    },
    {
      "type": "training",
      "description": "Training step 4534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:37",
      "total_flops_so_far": 2.694269826625536e+16,
      "budget_used_percent": 26.94269826625536
    },
    {
      "type": "training",
      "description": "Training step 4535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:38",
      "total_flops_so_far": 2.6948639324307456e+16,
      "budget_used_percent": 26.948639324307454
    },
    {
      "type": "training",
      "description": "Training step 4536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:38",
      "total_flops_so_far": 2.695458038235955e+16,
      "budget_used_percent": 26.95458038235955
    },
    {
      "type": "training",
      "description": "Training step 4537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:39",
      "total_flops_so_far": 2.696052144041165e+16,
      "budget_used_percent": 26.96052144041165
    },
    {
      "type": "training",
      "description": "Training step 4538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:39",
      "total_flops_so_far": 2.6966462498463744e+16,
      "budget_used_percent": 26.966462498463745
    },
    {
      "type": "training",
      "description": "Training step 4539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:39",
      "total_flops_so_far": 2.697240355651584e+16,
      "budget_used_percent": 26.97240355651584
    },
    {
      "type": "training",
      "description": "Training step 4540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:40",
      "total_flops_so_far": 2.6978344614567936e+16,
      "budget_used_percent": 26.978344614567934
    },
    {
      "type": "training",
      "description": "Training step 4541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:40",
      "total_flops_so_far": 2.698428567262003e+16,
      "budget_used_percent": 26.984285672620032
    },
    {
      "type": "training",
      "description": "Training step 4542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:41",
      "total_flops_so_far": 2.699022673067213e+16,
      "budget_used_percent": 26.990226730672127
    },
    {
      "type": "training",
      "description": "Training step 4543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:41",
      "total_flops_so_far": 2.6996167788724224e+16,
      "budget_used_percent": 26.99616778872422
    },
    {
      "type": "training",
      "description": "Training step 4544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:42",
      "total_flops_so_far": 2.700210884677632e+16,
      "budget_used_percent": 27.002108846776316
    },
    {
      "type": "training",
      "description": "Training step 4545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:42",
      "total_flops_so_far": 2.7008049904828416e+16,
      "budget_used_percent": 27.008049904828418
    },
    {
      "type": "training",
      "description": "Training step 4546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:42",
      "total_flops_so_far": 2.701399096288051e+16,
      "budget_used_percent": 27.013990962880513
    },
    {
      "type": "training",
      "description": "Training step 4547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:43",
      "total_flops_so_far": 2.701993202093261e+16,
      "budget_used_percent": 27.019932020932607
    },
    {
      "type": "training",
      "description": "Training step 4548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:43",
      "total_flops_so_far": 2.7025873078984704e+16,
      "budget_used_percent": 27.025873078984702
    },
    {
      "type": "training",
      "description": "Training step 4549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:44",
      "total_flops_so_far": 2.70318141370368e+16,
      "budget_used_percent": 27.031814137036804
    },
    {
      "type": "training",
      "description": "Training step 4550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:44",
      "total_flops_so_far": 2.7037755195088896e+16,
      "budget_used_percent": 27.0377551950889
    },
    {
      "type": "training",
      "description": "Training step 4551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:44",
      "total_flops_so_far": 2.704369625314099e+16,
      "budget_used_percent": 27.043696253140993
    },
    {
      "type": "training",
      "description": "Training step 4552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:45",
      "total_flops_so_far": 2.704963731119309e+16,
      "budget_used_percent": 27.049637311193088
    },
    {
      "type": "training",
      "description": "Training step 4553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:45",
      "total_flops_so_far": 2.7055578369245184e+16,
      "budget_used_percent": 27.055578369245183
    },
    {
      "type": "training",
      "description": "Training step 4554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:46",
      "total_flops_so_far": 2.706151942729728e+16,
      "budget_used_percent": 27.06151942729728
    },
    {
      "type": "training",
      "description": "Training step 4555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:46",
      "total_flops_so_far": 2.7067460485349376e+16,
      "budget_used_percent": 27.067460485349375
    },
    {
      "type": "training",
      "description": "Training step 4556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:46",
      "total_flops_so_far": 2.707340154340147e+16,
      "budget_used_percent": 27.07340154340147
    },
    {
      "type": "training",
      "description": "Training step 4557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:47",
      "total_flops_so_far": 2.707934260145357e+16,
      "budget_used_percent": 27.079342601453565
    },
    {
      "type": "training",
      "description": "Training step 4558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:47",
      "total_flops_so_far": 2.7085283659505664e+16,
      "budget_used_percent": 27.085283659505667
    },
    {
      "type": "training",
      "description": "Training step 4559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:48",
      "total_flops_so_far": 2.709122471755776e+16,
      "budget_used_percent": 27.09122471755776
    },
    {
      "type": "training",
      "description": "Training step 4560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:48",
      "total_flops_so_far": 2.7097165775609856e+16,
      "budget_used_percent": 27.097165775609856
    },
    {
      "type": "training",
      "description": "Training step 4561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:49",
      "total_flops_so_far": 2.710310683366195e+16,
      "budget_used_percent": 27.10310683366195
    },
    {
      "type": "training",
      "description": "Training step 4562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:49",
      "total_flops_so_far": 2.710904789171405e+16,
      "budget_used_percent": 27.10904789171405
    },
    {
      "type": "training",
      "description": "Training step 4563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:49",
      "total_flops_so_far": 2.7114988949766144e+16,
      "budget_used_percent": 27.114988949766143
    },
    {
      "type": "training",
      "description": "Training step 4564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:50",
      "total_flops_so_far": 2.712093000781824e+16,
      "budget_used_percent": 27.120930007818238
    },
    {
      "type": "training",
      "description": "Training step 4565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:50",
      "total_flops_so_far": 2.7126871065870336e+16,
      "budget_used_percent": 27.126871065870333
    },
    {
      "type": "training",
      "description": "Training step 4566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:50",
      "total_flops_so_far": 2.713281212392243e+16,
      "budget_used_percent": 27.132812123922434
    },
    {
      "type": "training",
      "description": "Training step 4567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:51",
      "total_flops_so_far": 2.713875318197453e+16,
      "budget_used_percent": 27.13875318197453
    },
    {
      "type": "training",
      "description": "Training step 4568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:51",
      "total_flops_so_far": 2.7144694240026624e+16,
      "budget_used_percent": 27.144694240026624
    },
    {
      "type": "training",
      "description": "Training step 4569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:52",
      "total_flops_so_far": 2.715063529807872e+16,
      "budget_used_percent": 27.15063529807872
    },
    {
      "type": "training",
      "description": "Training step 4570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:52",
      "total_flops_so_far": 2.7156576356130816e+16,
      "budget_used_percent": 27.156576356130817
    },
    {
      "type": "training",
      "description": "Training step 4571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:53",
      "total_flops_so_far": 2.716251741418291e+16,
      "budget_used_percent": 27.16251741418291
    },
    {
      "type": "training",
      "description": "Training step 4572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:53",
      "total_flops_so_far": 2.716845847223501e+16,
      "budget_used_percent": 27.168458472235006
    },
    {
      "type": "training",
      "description": "Training step 4573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:53",
      "total_flops_so_far": 2.7174399530287104e+16,
      "budget_used_percent": 27.1743995302871
    },
    {
      "type": "training",
      "description": "Training step 4574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:54",
      "total_flops_so_far": 2.71803405883392e+16,
      "budget_used_percent": 27.180340588339202
    },
    {
      "type": "training",
      "description": "Training step 4575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:54",
      "total_flops_so_far": 2.7186281646391296e+16,
      "budget_used_percent": 27.186281646391297
    },
    {
      "type": "training",
      "description": "Training step 4576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:55",
      "total_flops_so_far": 2.719222270444339e+16,
      "budget_used_percent": 27.19222270444339
    },
    {
      "type": "training",
      "description": "Training step 4577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:55",
      "total_flops_so_far": 2.719816376249549e+16,
      "budget_used_percent": 27.198163762495486
    },
    {
      "type": "training",
      "description": "Training step 4578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:55",
      "total_flops_so_far": 2.7204104820547584e+16,
      "budget_used_percent": 27.204104820547588
    },
    {
      "type": "training",
      "description": "Training step 4579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:56",
      "total_flops_so_far": 2.721004587859968e+16,
      "budget_used_percent": 27.210045878599683
    },
    {
      "type": "training",
      "description": "Training step 4580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:56",
      "total_flops_so_far": 2.7215986936651776e+16,
      "budget_used_percent": 27.215986936651777
    },
    {
      "type": "training",
      "description": "Training step 4581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:57",
      "total_flops_so_far": 2.722192799470387e+16,
      "budget_used_percent": 27.221927994703872
    },
    {
      "type": "training",
      "description": "Training step 4582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:57",
      "total_flops_so_far": 2.722786905275597e+16,
      "budget_used_percent": 27.227869052755967
    },
    {
      "type": "training",
      "description": "Training step 4583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:57",
      "total_flops_so_far": 2.7233810110808064e+16,
      "budget_used_percent": 27.233810110808065
    },
    {
      "type": "training",
      "description": "Training step 4584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:58",
      "total_flops_so_far": 2.723975116886016e+16,
      "budget_used_percent": 27.23975116886016
    },
    {
      "type": "training",
      "description": "Training step 4585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:58",
      "total_flops_so_far": 2.7245692226912256e+16,
      "budget_used_percent": 27.245692226912254
    },
    {
      "type": "training",
      "description": "Training step 4586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:59",
      "total_flops_so_far": 2.725163328496435e+16,
      "budget_used_percent": 27.25163328496435
    },
    {
      "type": "training",
      "description": "Training step 4587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:06:59",
      "total_flops_so_far": 2.725757434301645e+16,
      "budget_used_percent": 27.25757434301645
    },
    {
      "type": "training",
      "description": "Training step 4588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:00",
      "total_flops_so_far": 2.7263515401068544e+16,
      "budget_used_percent": 27.263515401068545
    },
    {
      "type": "training",
      "description": "Training step 4589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:00",
      "total_flops_so_far": 2.726945645912064e+16,
      "budget_used_percent": 27.26945645912064
    },
    {
      "type": "training",
      "description": "Training step 4590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:00",
      "total_flops_so_far": 2.7275397517172736e+16,
      "budget_used_percent": 27.275397517172735
    },
    {
      "type": "training",
      "description": "Training step 4591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:01",
      "total_flops_so_far": 2.728133857522483e+16,
      "budget_used_percent": 27.281338575224833
    },
    {
      "type": "training",
      "description": "Training step 4592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:02",
      "total_flops_so_far": 2.728727963327693e+16,
      "budget_used_percent": 27.287279633276928
    },
    {
      "type": "training",
      "description": "Training step 4593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:02",
      "total_flops_so_far": 2.7293220691329024e+16,
      "budget_used_percent": 27.293220691329022
    },
    {
      "type": "training",
      "description": "Training step 4594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:02",
      "total_flops_so_far": 2.729916174938112e+16,
      "budget_used_percent": 27.299161749381117
    },
    {
      "type": "training",
      "description": "Training step 4595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:03",
      "total_flops_so_far": 2.7305102807433216e+16,
      "budget_used_percent": 27.30510280743322
    },
    {
      "type": "training",
      "description": "Training step 4596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:03",
      "total_flops_so_far": 2.731104386548531e+16,
      "budget_used_percent": 27.311043865485313
    },
    {
      "type": "training",
      "description": "Training step 4597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:04",
      "total_flops_so_far": 2.731698492353741e+16,
      "budget_used_percent": 27.316984923537408
    },
    {
      "type": "training",
      "description": "Training step 4598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:04",
      "total_flops_so_far": 2.7322925981589504e+16,
      "budget_used_percent": 27.322925981589503
    },
    {
      "type": "training",
      "description": "Training step 4599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:04",
      "total_flops_so_far": 2.73288670396416e+16,
      "budget_used_percent": 27.3288670396416
    },
    {
      "type": "training",
      "description": "Training step 4600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:05",
      "total_flops_so_far": 2.7334808097693696e+16,
      "budget_used_percent": 27.334808097693696
    },
    {
      "type": "training",
      "description": "Training step 4601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:05",
      "total_flops_so_far": 2.734074915574579e+16,
      "budget_used_percent": 27.34074915574579
    },
    {
      "type": "training",
      "description": "Training step 4602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:06",
      "total_flops_so_far": 2.734669021379789e+16,
      "budget_used_percent": 27.346690213797885
    },
    {
      "type": "training",
      "description": "Training step 4603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:06",
      "total_flops_so_far": 2.7352631271849984e+16,
      "budget_used_percent": 27.352631271849987
    },
    {
      "type": "training",
      "description": "Training step 4604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:07",
      "total_flops_so_far": 2.735857232990208e+16,
      "budget_used_percent": 27.35857232990208
    },
    {
      "type": "training",
      "description": "Training step 4605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:07",
      "total_flops_so_far": 2.7364513387954176e+16,
      "budget_used_percent": 27.364513387954176
    },
    {
      "type": "training",
      "description": "Training step 4606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:07",
      "total_flops_so_far": 2.737045444600627e+16,
      "budget_used_percent": 27.37045444600627
    },
    {
      "type": "training",
      "description": "Training step 4607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:08",
      "total_flops_so_far": 2.737639550405837e+16,
      "budget_used_percent": 27.376395504058372
    },
    {
      "type": "training",
      "description": "Training step 4608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:08",
      "total_flops_so_far": 2.7382336562110464e+16,
      "budget_used_percent": 27.382336562110467
    },
    {
      "type": "training",
      "description": "Training step 4609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:09",
      "total_flops_so_far": 2.738827762016256e+16,
      "budget_used_percent": 27.38827762016256
    },
    {
      "type": "training",
      "description": "Training step 4610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:09",
      "total_flops_so_far": 2.7394218678214656e+16,
      "budget_used_percent": 27.394218678214656
    },
    {
      "type": "training",
      "description": "Training step 4611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:09",
      "total_flops_so_far": 2.740015973626675e+16,
      "budget_used_percent": 27.40015973626675
    },
    {
      "type": "training",
      "description": "Training step 4612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:10",
      "total_flops_so_far": 2.740610079431885e+16,
      "budget_used_percent": 27.40610079431885
    },
    {
      "type": "training",
      "description": "Training step 4613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:10",
      "total_flops_so_far": 2.7412041852370944e+16,
      "budget_used_percent": 27.412041852370944
    },
    {
      "type": "training",
      "description": "Training step 4614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:11",
      "total_flops_so_far": 2.741798291042304e+16,
      "budget_used_percent": 27.41798291042304
    },
    {
      "type": "training",
      "description": "Training step 4615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:11",
      "total_flops_so_far": 2.7423923968475136e+16,
      "budget_used_percent": 27.423923968475133
    },
    {
      "type": "training",
      "description": "Training step 4616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:11",
      "total_flops_so_far": 2.742986502652723e+16,
      "budget_used_percent": 27.429865026527235
    },
    {
      "type": "training",
      "description": "Training step 4617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:12",
      "total_flops_so_far": 2.743580608457933e+16,
      "budget_used_percent": 27.43580608457933
    },
    {
      "type": "training",
      "description": "Training step 4618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:12",
      "total_flops_so_far": 2.7441747142631424e+16,
      "budget_used_percent": 27.441747142631424
    },
    {
      "type": "training",
      "description": "Training step 4619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:13",
      "total_flops_so_far": 2.744768820068352e+16,
      "budget_used_percent": 27.44768820068352
    },
    {
      "type": "training",
      "description": "Training step 4620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:13",
      "total_flops_so_far": 2.7453629258735616e+16,
      "budget_used_percent": 27.453629258735617
    },
    {
      "type": "training",
      "description": "Training step 4621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:14",
      "total_flops_so_far": 2.745957031678771e+16,
      "budget_used_percent": 27.459570316787712
    },
    {
      "type": "training",
      "description": "Training step 4622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:14",
      "total_flops_so_far": 2.746551137483981e+16,
      "budget_used_percent": 27.465511374839807
    },
    {
      "type": "training",
      "description": "Training step 4623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:14",
      "total_flops_so_far": 2.7471452432891904e+16,
      "budget_used_percent": 27.4714524328919
    },
    {
      "type": "training",
      "description": "Training step 4624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:15",
      "total_flops_so_far": 2.7477393490944e+16,
      "budget_used_percent": 27.477393490944003
    },
    {
      "type": "training",
      "description": "Training step 4625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:15",
      "total_flops_so_far": 2.7483334548996096e+16,
      "budget_used_percent": 27.483334548996098
    },
    {
      "type": "training",
      "description": "Training step 4626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:16",
      "total_flops_so_far": 2.748927560704819e+16,
      "budget_used_percent": 27.489275607048192
    },
    {
      "type": "training",
      "description": "Training step 4627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:16",
      "total_flops_so_far": 2.749521666510029e+16,
      "budget_used_percent": 27.495216665100287
    },
    {
      "type": "training",
      "description": "Training step 4628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:17",
      "total_flops_so_far": 2.7501157723152384e+16,
      "budget_used_percent": 27.501157723152385
    },
    {
      "type": "training",
      "description": "Training step 4629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:17",
      "total_flops_so_far": 2.750709878120448e+16,
      "budget_used_percent": 27.50709878120448
    },
    {
      "type": "training",
      "description": "Training step 4630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:18",
      "total_flops_so_far": 2.7513039839256576e+16,
      "budget_used_percent": 27.513039839256574
    },
    {
      "type": "training",
      "description": "Training step 4631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:18",
      "total_flops_so_far": 2.751898089730867e+16,
      "budget_used_percent": 27.51898089730867
    },
    {
      "type": "training",
      "description": "Training step 4632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:18",
      "total_flops_so_far": 2.752492195536077e+16,
      "budget_used_percent": 27.52492195536077
    },
    {
      "type": "training",
      "description": "Training step 4633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:19",
      "total_flops_so_far": 2.7530863013412864e+16,
      "budget_used_percent": 27.530863013412866
    },
    {
      "type": "training",
      "description": "Training step 4634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:19",
      "total_flops_so_far": 2.753680407146496e+16,
      "budget_used_percent": 27.53680407146496
    },
    {
      "type": "training",
      "description": "Training step 4635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:20",
      "total_flops_so_far": 2.7542745129517056e+16,
      "budget_used_percent": 27.542745129517055
    },
    {
      "type": "training",
      "description": "Training step 4636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:20",
      "total_flops_so_far": 2.754868618756915e+16,
      "budget_used_percent": 27.548686187569153
    },
    {
      "type": "training",
      "description": "Training step 4637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:20",
      "total_flops_so_far": 2.755462724562125e+16,
      "budget_used_percent": 27.554627245621248
    },
    {
      "type": "training",
      "description": "Training step 4638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:21",
      "total_flops_so_far": 2.7560568303673344e+16,
      "budget_used_percent": 27.560568303673342
    },
    {
      "type": "training",
      "description": "Training step 4639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:21",
      "total_flops_so_far": 2.756650936172544e+16,
      "budget_used_percent": 27.566509361725437
    },
    {
      "type": "training",
      "description": "Training step 4640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:22",
      "total_flops_so_far": 2.7572450419777536e+16,
      "budget_used_percent": 27.57245041977753
    },
    {
      "type": "training",
      "description": "Training step 4641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:22",
      "total_flops_so_far": 2.757839147782963e+16,
      "budget_used_percent": 27.578391477829634
    },
    {
      "type": "training",
      "description": "Training step 4642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:23",
      "total_flops_so_far": 2.758433253588173e+16,
      "budget_used_percent": 27.584332535881728
    },
    {
      "type": "training",
      "description": "Training step 4643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:23",
      "total_flops_so_far": 2.7590273593933824e+16,
      "budget_used_percent": 27.590273593933823
    },
    {
      "type": "training",
      "description": "Training step 4644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:23",
      "total_flops_so_far": 2.759621465198592e+16,
      "budget_used_percent": 27.596214651985917
    },
    {
      "type": "training",
      "description": "Training step 4645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:24",
      "total_flops_so_far": 2.7602155710038016e+16,
      "budget_used_percent": 27.60215571003802
    },
    {
      "type": "training",
      "description": "Training step 4646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:24",
      "total_flops_so_far": 2.760809676809011e+16,
      "budget_used_percent": 27.608096768090114
    },
    {
      "type": "training",
      "description": "Training step 4647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:25",
      "total_flops_so_far": 2.761403782614221e+16,
      "budget_used_percent": 27.61403782614221
    },
    {
      "type": "training",
      "description": "Training step 4648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:25",
      "total_flops_so_far": 2.7619978884194304e+16,
      "budget_used_percent": 27.619978884194303
    },
    {
      "type": "training",
      "description": "Training step 4649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:25",
      "total_flops_so_far": 2.76259199422464e+16,
      "budget_used_percent": 27.6259199422464
    },
    {
      "type": "training",
      "description": "Training step 4650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:26",
      "total_flops_so_far": 2.7631861000298496e+16,
      "budget_used_percent": 27.631861000298496
    },
    {
      "type": "training",
      "description": "Training step 4651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:26",
      "total_flops_so_far": 2.763780205835059e+16,
      "budget_used_percent": 27.63780205835059
    },
    {
      "type": "training",
      "description": "Training step 4652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:27",
      "total_flops_so_far": 2.764374311640269e+16,
      "budget_used_percent": 27.643743116402685
    },
    {
      "type": "training",
      "description": "Training step 4653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:27",
      "total_flops_so_far": 2.7649684174454784e+16,
      "budget_used_percent": 27.649684174454787
    },
    {
      "type": "training",
      "description": "Training step 4654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:28",
      "total_flops_so_far": 2.765562523250688e+16,
      "budget_used_percent": 27.655625232506882
    },
    {
      "type": "training",
      "description": "Training step 4655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:28",
      "total_flops_so_far": 2.7661566290558976e+16,
      "budget_used_percent": 27.661566290558977
    },
    {
      "type": "training",
      "description": "Training step 4656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:28",
      "total_flops_so_far": 2.766750734861107e+16,
      "budget_used_percent": 27.66750734861107
    },
    {
      "type": "training",
      "description": "Training step 4657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:29",
      "total_flops_so_far": 2.767344840666317e+16,
      "budget_used_percent": 27.67344840666317
    },
    {
      "type": "training",
      "description": "Training step 4658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:29",
      "total_flops_so_far": 2.7679389464715264e+16,
      "budget_used_percent": 27.679389464715264
    },
    {
      "type": "training",
      "description": "Training step 4659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:30",
      "total_flops_so_far": 2.768533052276736e+16,
      "budget_used_percent": 27.68533052276736
    },
    {
      "type": "training",
      "description": "Training step 4660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:30",
      "total_flops_so_far": 2.7691271580819456e+16,
      "budget_used_percent": 27.691271580819453
    },
    {
      "type": "training",
      "description": "Training step 4661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:30",
      "total_flops_so_far": 2.769721263887155e+16,
      "budget_used_percent": 27.697212638871555
    },
    {
      "type": "training",
      "description": "Training step 4662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:31",
      "total_flops_so_far": 2.770315369692365e+16,
      "budget_used_percent": 27.70315369692365
    },
    {
      "type": "training",
      "description": "Training step 4663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:31",
      "total_flops_so_far": 2.7709094754975744e+16,
      "budget_used_percent": 27.709094754975744
    },
    {
      "type": "training",
      "description": "Training step 4664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:32",
      "total_flops_so_far": 2.771503581302784e+16,
      "budget_used_percent": 27.71503581302784
    },
    {
      "type": "training",
      "description": "Training step 4665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:32",
      "total_flops_so_far": 2.7720976871079936e+16,
      "budget_used_percent": 27.720976871079937
    },
    {
      "type": "training",
      "description": "Training step 4666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:33",
      "total_flops_so_far": 2.772691792913203e+16,
      "budget_used_percent": 27.726917929132032
    },
    {
      "type": "training",
      "description": "Training step 4667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:33",
      "total_flops_so_far": 2.773285898718413e+16,
      "budget_used_percent": 27.732858987184127
    },
    {
      "type": "training",
      "description": "Training step 4668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:34",
      "total_flops_so_far": 2.7738800045236224e+16,
      "budget_used_percent": 27.73880004523622
    },
    {
      "type": "training",
      "description": "Training step 4669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:34",
      "total_flops_so_far": 2.774474110328832e+16,
      "budget_used_percent": 27.744741103288316
    },
    {
      "type": "training",
      "description": "Training step 4670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:34",
      "total_flops_so_far": 2.7750682161340416e+16,
      "budget_used_percent": 27.750682161340418
    },
    {
      "type": "training",
      "description": "Training step 4671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:35",
      "total_flops_so_far": 2.775662321939251e+16,
      "budget_used_percent": 27.756623219392512
    },
    {
      "type": "training",
      "description": "Training step 4672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:35",
      "total_flops_so_far": 2.776256427744461e+16,
      "budget_used_percent": 27.762564277444607
    },
    {
      "type": "training",
      "description": "Training step 4673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:36",
      "total_flops_so_far": 2.7768505335496704e+16,
      "budget_used_percent": 27.7685053354967
    },
    {
      "type": "training",
      "description": "Training step 4674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:36",
      "total_flops_so_far": 2.77744463935488e+16,
      "budget_used_percent": 27.7744463935488
    },
    {
      "type": "training",
      "description": "Training step 4675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:37",
      "total_flops_so_far": 2.7780387451600896e+16,
      "budget_used_percent": 27.780387451600895
    },
    {
      "type": "training",
      "description": "Training step 4676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:37",
      "total_flops_so_far": 2.778632850965299e+16,
      "budget_used_percent": 27.78632850965299
    },
    {
      "type": "training",
      "description": "Training step 4677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:37",
      "total_flops_so_far": 2.779226956770509e+16,
      "budget_used_percent": 27.792269567705084
    },
    {
      "type": "training",
      "description": "Training step 4678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:38",
      "total_flops_so_far": 2.7798210625757184e+16,
      "budget_used_percent": 27.798210625757186
    },
    {
      "type": "training",
      "description": "Training step 4679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:38",
      "total_flops_so_far": 2.780415168380928e+16,
      "budget_used_percent": 27.80415168380928
    },
    {
      "type": "training",
      "description": "Training step 4680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:39",
      "total_flops_so_far": 2.7810092741861376e+16,
      "budget_used_percent": 27.810092741861375
    },
    {
      "type": "training",
      "description": "Training step 4681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:39",
      "total_flops_so_far": 2.781603379991347e+16,
      "budget_used_percent": 27.81603379991347
    },
    {
      "type": "training",
      "description": "Training step 4682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:39",
      "total_flops_so_far": 2.782197485796557e+16,
      "budget_used_percent": 27.82197485796557
    },
    {
      "type": "training",
      "description": "Training step 4683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:40",
      "total_flops_so_far": 2.7827915916017664e+16,
      "budget_used_percent": 27.827915916017666
    },
    {
      "type": "training",
      "description": "Training step 4684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:40",
      "total_flops_so_far": 2.783385697406976e+16,
      "budget_used_percent": 27.83385697406976
    },
    {
      "type": "training",
      "description": "Training step 4685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:41",
      "total_flops_so_far": 2.7839798032121856e+16,
      "budget_used_percent": 27.839798032121855
    },
    {
      "type": "training",
      "description": "Training step 4686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:41",
      "total_flops_so_far": 2.784573909017395e+16,
      "budget_used_percent": 27.845739090173954
    },
    {
      "type": "training",
      "description": "Training step 4687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:42",
      "total_flops_so_far": 2.785168014822605e+16,
      "budget_used_percent": 27.85168014822605
    },
    {
      "type": "training",
      "description": "Training step 4688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:42",
      "total_flops_so_far": 2.7857621206278144e+16,
      "budget_used_percent": 27.857621206278143
    },
    {
      "type": "training",
      "description": "Training step 4689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:42",
      "total_flops_so_far": 2.786356226433024e+16,
      "budget_used_percent": 27.863562264330238
    },
    {
      "type": "training",
      "description": "Training step 4690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:43",
      "total_flops_so_far": 2.7869503322382336e+16,
      "budget_used_percent": 27.86950332238234
    },
    {
      "type": "training",
      "description": "Training step 4691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:43",
      "total_flops_so_far": 2.787544438043443e+16,
      "budget_used_percent": 27.875444380434434
    },
    {
      "type": "training",
      "description": "Training step 4692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:44",
      "total_flops_so_far": 2.788138543848653e+16,
      "budget_used_percent": 27.88138543848653
    },
    {
      "type": "training",
      "description": "Training step 4693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:44",
      "total_flops_so_far": 2.7887326496538624e+16,
      "budget_used_percent": 27.887326496538623
    },
    {
      "type": "training",
      "description": "Training step 4694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:44",
      "total_flops_so_far": 2.789326755459072e+16,
      "budget_used_percent": 27.89326755459072
    },
    {
      "type": "training",
      "description": "Training step 4695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:45",
      "total_flops_so_far": 2.7899208612642816e+16,
      "budget_used_percent": 27.899208612642816
    },
    {
      "type": "training",
      "description": "Training step 4696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:45",
      "total_flops_so_far": 2.790514967069491e+16,
      "budget_used_percent": 27.90514967069491
    },
    {
      "type": "training",
      "description": "Training step 4697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:46",
      "total_flops_so_far": 2.791109072874701e+16,
      "budget_used_percent": 27.911090728747006
    },
    {
      "type": "training",
      "description": "Training step 4698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:46",
      "total_flops_so_far": 2.7917031786799104e+16,
      "budget_used_percent": 27.9170317867991
    },
    {
      "type": "training",
      "description": "Training step 4699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:47",
      "total_flops_so_far": 2.79229728448512e+16,
      "budget_used_percent": 27.922972844851202
    },
    {
      "type": "training",
      "description": "Training step 4700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:47",
      "total_flops_so_far": 2.7928913902903296e+16,
      "budget_used_percent": 27.928913902903297
    },
    {
      "type": "training",
      "description": "Training step 4701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:47",
      "total_flops_so_far": 2.793485496095539e+16,
      "budget_used_percent": 27.93485496095539
    },
    {
      "type": "training",
      "description": "Training step 4702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:48",
      "total_flops_so_far": 2.794079601900749e+16,
      "budget_used_percent": 27.940796019007486
    },
    {
      "type": "training",
      "description": "Training step 4703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:49",
      "total_flops_so_far": 2.7946737077059584e+16,
      "budget_used_percent": 27.946737077059584
    },
    {
      "type": "training",
      "description": "Training step 4704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:49",
      "total_flops_so_far": 2.795267813511168e+16,
      "budget_used_percent": 27.95267813511168
    },
    {
      "type": "training",
      "description": "Training step 4705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:49",
      "total_flops_so_far": 2.7958619193163776e+16,
      "budget_used_percent": 27.958619193163774
    },
    {
      "type": "training",
      "description": "Training step 4706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:50",
      "total_flops_so_far": 2.796456025121587e+16,
      "budget_used_percent": 27.964560251215868
    },
    {
      "type": "training",
      "description": "Training step 4707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:50",
      "total_flops_so_far": 2.797050130926797e+16,
      "budget_used_percent": 27.97050130926797
    },
    {
      "type": "training",
      "description": "Training step 4708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:51",
      "total_flops_so_far": 2.7976442367320064e+16,
      "budget_used_percent": 27.976442367320065
    },
    {
      "type": "training",
      "description": "Training step 4709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:51",
      "total_flops_so_far": 2.798238342537216e+16,
      "budget_used_percent": 27.98238342537216
    },
    {
      "type": "training",
      "description": "Training step 4710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:51",
      "total_flops_so_far": 2.7988324483424256e+16,
      "budget_used_percent": 27.988324483424254
    },
    {
      "type": "training",
      "description": "Training step 4711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:52",
      "total_flops_so_far": 2.799426554147635e+16,
      "budget_used_percent": 27.994265541476356
    },
    {
      "type": "training",
      "description": "Training step 4712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:52",
      "total_flops_so_far": 2.800020659952845e+16,
      "budget_used_percent": 28.00020659952845
    },
    {
      "type": "training",
      "description": "Training step 4713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:53",
      "total_flops_so_far": 2.8006147657580544e+16,
      "budget_used_percent": 28.006147657580545
    },
    {
      "type": "training",
      "description": "Training step 4714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:53",
      "total_flops_so_far": 2.801208871563264e+16,
      "budget_used_percent": 28.01208871563264
    },
    {
      "type": "training",
      "description": "Training step 4715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:54",
      "total_flops_so_far": 2.8018029773684736e+16,
      "budget_used_percent": 28.018029773684738
    },
    {
      "type": "training",
      "description": "Training step 4716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:54",
      "total_flops_so_far": 2.802397083173683e+16,
      "budget_used_percent": 28.023970831736833
    },
    {
      "type": "training",
      "description": "Training step 4717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:54",
      "total_flops_so_far": 2.802991188978893e+16,
      "budget_used_percent": 28.029911889788927
    },
    {
      "type": "training",
      "description": "Training step 4718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:55",
      "total_flops_so_far": 2.8035852947841024e+16,
      "budget_used_percent": 28.035852947841022
    },
    {
      "type": "training",
      "description": "Training step 4719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:55",
      "total_flops_so_far": 2.804179400589312e+16,
      "budget_used_percent": 28.041794005893124
    },
    {
      "type": "training",
      "description": "Training step 4720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:56",
      "total_flops_so_far": 2.8047735063945216e+16,
      "budget_used_percent": 28.04773506394522
    },
    {
      "type": "training",
      "description": "Training step 4721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:56",
      "total_flops_so_far": 2.805367612199731e+16,
      "budget_used_percent": 28.053676121997313
    },
    {
      "type": "training",
      "description": "Training step 4722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:56",
      "total_flops_so_far": 2.805961718004941e+16,
      "budget_used_percent": 28.059617180049408
    },
    {
      "type": "training",
      "description": "Training step 4723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:57",
      "total_flops_so_far": 2.8065558238101504e+16,
      "budget_used_percent": 28.065558238101506
    },
    {
      "type": "training",
      "description": "Training step 4724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:57",
      "total_flops_so_far": 2.80714992961536e+16,
      "budget_used_percent": 28.0714992961536
    },
    {
      "type": "training",
      "description": "Training step 4725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:58",
      "total_flops_so_far": 2.8077440354205696e+16,
      "budget_used_percent": 28.077440354205695
    },
    {
      "type": "training",
      "description": "Training step 4726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:58",
      "total_flops_so_far": 2.808338141225779e+16,
      "budget_used_percent": 28.08338141225779
    },
    {
      "type": "training",
      "description": "Training step 4727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:58",
      "total_flops_so_far": 2.808932247030989e+16,
      "budget_used_percent": 28.089322470309884
    },
    {
      "type": "training",
      "description": "Training step 4728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:59",
      "total_flops_so_far": 2.8095263528361984e+16,
      "budget_used_percent": 28.095263528361986
    },
    {
      "type": "training",
      "description": "Training step 4729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:07:59",
      "total_flops_so_far": 2.810120458641408e+16,
      "budget_used_percent": 28.10120458641408
    },
    {
      "type": "training",
      "description": "Training step 4730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:00",
      "total_flops_so_far": 2.8107145644466176e+16,
      "budget_used_percent": 28.107145644466176
    },
    {
      "type": "training",
      "description": "Training step 4731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:00",
      "total_flops_so_far": 2.811308670251827e+16,
      "budget_used_percent": 28.11308670251827
    },
    {
      "type": "training",
      "description": "Training step 4732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:01",
      "total_flops_so_far": 2.811902776057037e+16,
      "budget_used_percent": 28.11902776057037
    },
    {
      "type": "training",
      "description": "Training step 4733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:01",
      "total_flops_so_far": 2.8124968818622464e+16,
      "budget_used_percent": 28.124968818622463
    },
    {
      "type": "training",
      "description": "Training step 4734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:01",
      "total_flops_so_far": 2.813090987667456e+16,
      "budget_used_percent": 28.130909876674558
    },
    {
      "type": "training",
      "description": "Training step 4735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:02",
      "total_flops_so_far": 2.8136850934726656e+16,
      "budget_used_percent": 28.136850934726652
    },
    {
      "type": "training",
      "description": "Training step 4736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:02",
      "total_flops_so_far": 2.814279199277875e+16,
      "budget_used_percent": 28.142791992778754
    },
    {
      "type": "training",
      "description": "Training step 4737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:03",
      "total_flops_so_far": 2.814873305083085e+16,
      "budget_used_percent": 28.14873305083085
    },
    {
      "type": "training",
      "description": "Training step 4738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:03",
      "total_flops_so_far": 2.8154674108882944e+16,
      "budget_used_percent": 28.154674108882944
    },
    {
      "type": "training",
      "description": "Training step 4739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:04",
      "total_flops_so_far": 2.816061516693504e+16,
      "budget_used_percent": 28.160615166935038
    },
    {
      "type": "training",
      "description": "Training step 4740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:04",
      "total_flops_so_far": 2.8166556224987136e+16,
      "budget_used_percent": 28.16655622498714
    },
    {
      "type": "training",
      "description": "Training step 4741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:05",
      "total_flops_so_far": 2.817249728303923e+16,
      "budget_used_percent": 28.172497283039235
    },
    {
      "type": "training",
      "description": "Training step 4742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:05",
      "total_flops_so_far": 2.817843834109133e+16,
      "budget_used_percent": 28.17843834109133
    },
    {
      "type": "training",
      "description": "Training step 4743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:05",
      "total_flops_so_far": 2.8184379399143424e+16,
      "budget_used_percent": 28.184379399143424
    },
    {
      "type": "training",
      "description": "Training step 4744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:06",
      "total_flops_so_far": 2.819032045719552e+16,
      "budget_used_percent": 28.190320457195522
    },
    {
      "type": "training",
      "description": "Training step 4745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:06",
      "total_flops_so_far": 2.8196261515247616e+16,
      "budget_used_percent": 28.196261515247617
    },
    {
      "type": "training",
      "description": "Training step 4746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:07",
      "total_flops_so_far": 2.820220257329971e+16,
      "budget_used_percent": 28.20220257329971
    },
    {
      "type": "training",
      "description": "Training step 4747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:07",
      "total_flops_so_far": 2.820814363135181e+16,
      "budget_used_percent": 28.208143631351806
    },
    {
      "type": "training",
      "description": "Training step 4748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:08",
      "total_flops_so_far": 2.8214084689403904e+16,
      "budget_used_percent": 28.214084689403908
    },
    {
      "type": "training",
      "description": "Training step 4749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:08",
      "total_flops_so_far": 2.8220025747456e+16,
      "budget_used_percent": 28.220025747456003
    },
    {
      "type": "training",
      "description": "Training step 4750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:08",
      "total_flops_so_far": 2.8225966805508096e+16,
      "budget_used_percent": 28.225966805508097
    },
    {
      "type": "training",
      "description": "Training step 4751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:09",
      "total_flops_so_far": 2.823190786356019e+16,
      "budget_used_percent": 28.231907863560192
    },
    {
      "type": "training",
      "description": "Training step 4752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:09",
      "total_flops_so_far": 2.823784892161229e+16,
      "budget_used_percent": 28.23784892161229
    },
    {
      "type": "training",
      "description": "Training step 4753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:10",
      "total_flops_so_far": 2.8243789979664384e+16,
      "budget_used_percent": 28.243789979664385
    },
    {
      "type": "training",
      "description": "Training step 4754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:10",
      "total_flops_so_far": 2.824973103771648e+16,
      "budget_used_percent": 28.24973103771648
    },
    {
      "type": "training",
      "description": "Training step 4755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:10",
      "total_flops_so_far": 2.8255672095768576e+16,
      "budget_used_percent": 28.255672095768574
    },
    {
      "type": "training",
      "description": "Training step 4756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:11",
      "total_flops_so_far": 2.826161315382067e+16,
      "budget_used_percent": 28.26161315382067
    },
    {
      "type": "training",
      "description": "Training step 4757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:11",
      "total_flops_so_far": 2.826755421187277e+16,
      "budget_used_percent": 28.26755421187277
    },
    {
      "type": "training",
      "description": "Training step 4758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:12",
      "total_flops_so_far": 2.8273495269924864e+16,
      "budget_used_percent": 28.273495269924865
    },
    {
      "type": "training",
      "description": "Training step 4759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:12",
      "total_flops_so_far": 2.827943632797696e+16,
      "budget_used_percent": 28.27943632797696
    },
    {
      "type": "training",
      "description": "Training step 4760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:13",
      "total_flops_so_far": 2.8285377386029056e+16,
      "budget_used_percent": 28.285377386029054
    },
    {
      "type": "training",
      "description": "Training step 4761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:13",
      "total_flops_so_far": 2.829131844408115e+16,
      "budget_used_percent": 28.291318444081153
    },
    {
      "type": "training",
      "description": "Training step 4762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:13",
      "total_flops_so_far": 2.829725950213325e+16,
      "budget_used_percent": 28.297259502133247
    },
    {
      "type": "training",
      "description": "Training step 4763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:14",
      "total_flops_so_far": 2.8303200560185344e+16,
      "budget_used_percent": 28.303200560185342
    },
    {
      "type": "training",
      "description": "Training step 4764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:14",
      "total_flops_so_far": 2.830914161823744e+16,
      "budget_used_percent": 28.309141618237437
    },
    {
      "type": "training",
      "description": "Training step 4765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:15",
      "total_flops_so_far": 2.8315082676289536e+16,
      "budget_used_percent": 28.31508267628954
    },
    {
      "type": "training",
      "description": "Training step 4766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:15",
      "total_flops_so_far": 2.832102373434163e+16,
      "budget_used_percent": 28.321023734341633
    },
    {
      "type": "training",
      "description": "Training step 4767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:16",
      "total_flops_so_far": 2.832696479239373e+16,
      "budget_used_percent": 28.326964792393728
    },
    {
      "type": "training",
      "description": "Training step 4768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:16",
      "total_flops_so_far": 2.8332905850445824e+16,
      "budget_used_percent": 28.332905850445822
    },
    {
      "type": "training",
      "description": "Training step 4769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:16",
      "total_flops_so_far": 2.833884690849792e+16,
      "budget_used_percent": 28.33884690849792
    },
    {
      "type": "training",
      "description": "Training step 4770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:17",
      "total_flops_so_far": 2.8344787966550016e+16,
      "budget_used_percent": 28.344787966550015
    },
    {
      "type": "training",
      "description": "Training step 4771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:17",
      "total_flops_so_far": 2.835072902460211e+16,
      "budget_used_percent": 28.35072902460211
    },
    {
      "type": "training",
      "description": "Training step 4772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:18",
      "total_flops_so_far": 2.835667008265421e+16,
      "budget_used_percent": 28.356670082654205
    },
    {
      "type": "training",
      "description": "Training step 4773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:18",
      "total_flops_so_far": 2.8362611140706304e+16,
      "budget_used_percent": 28.362611140706306
    },
    {
      "type": "training",
      "description": "Training step 4774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:18",
      "total_flops_so_far": 2.83685521987584e+16,
      "budget_used_percent": 28.3685521987584
    },
    {
      "type": "training",
      "description": "Training step 4775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:19",
      "total_flops_so_far": 2.8374493256810496e+16,
      "budget_used_percent": 28.374493256810496
    },
    {
      "type": "training",
      "description": "Training step 4776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:20",
      "total_flops_so_far": 2.838043431486259e+16,
      "budget_used_percent": 28.38043431486259
    },
    {
      "type": "training",
      "description": "Training step 4777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:20",
      "total_flops_so_far": 2.838637537291469e+16,
      "budget_used_percent": 28.386375372914692
    },
    {
      "type": "training",
      "description": "Training step 4778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:20",
      "total_flops_so_far": 2.8392316430966784e+16,
      "budget_used_percent": 28.392316430966787
    },
    {
      "type": "training",
      "description": "Training step 4779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:21",
      "total_flops_so_far": 2.839825748901888e+16,
      "budget_used_percent": 28.39825748901888
    },
    {
      "type": "training",
      "description": "Training step 4780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:21",
      "total_flops_so_far": 2.8404198547070976e+16,
      "budget_used_percent": 28.404198547070976
    },
    {
      "type": "training",
      "description": "Training step 4781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:22",
      "total_flops_so_far": 2.841013960512307e+16,
      "budget_used_percent": 28.41013960512307
    },
    {
      "type": "training",
      "description": "Training step 4782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:22",
      "total_flops_so_far": 2.841608066317517e+16,
      "budget_used_percent": 28.41608066317517
    },
    {
      "type": "training",
      "description": "Training step 4783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:22",
      "total_flops_so_far": 2.8422021721227264e+16,
      "budget_used_percent": 28.422021721227264
    },
    {
      "type": "training",
      "description": "Training step 4784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:23",
      "total_flops_so_far": 2.842796277927936e+16,
      "budget_used_percent": 28.42796277927936
    },
    {
      "type": "training",
      "description": "Training step 4785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:23",
      "total_flops_so_far": 2.8433903837331456e+16,
      "budget_used_percent": 28.433903837331453
    },
    {
      "type": "training",
      "description": "Training step 4786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:24",
      "total_flops_so_far": 2.843984489538355e+16,
      "budget_used_percent": 28.439844895383555
    },
    {
      "type": "training",
      "description": "Training step 4787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:24",
      "total_flops_so_far": 2.844578595343565e+16,
      "budget_used_percent": 28.44578595343565
    },
    {
      "type": "training",
      "description": "Training step 4788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:25",
      "total_flops_so_far": 2.8451727011487744e+16,
      "budget_used_percent": 28.451727011487744
    },
    {
      "type": "training",
      "description": "Training step 4789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:25",
      "total_flops_so_far": 2.845766806953984e+16,
      "budget_used_percent": 28.45766806953984
    },
    {
      "type": "training",
      "description": "Training step 4790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:25",
      "total_flops_so_far": 2.8463609127591936e+16,
      "budget_used_percent": 28.463609127591937
    },
    {
      "type": "training",
      "description": "Training step 4791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:26",
      "total_flops_so_far": 2.846955018564403e+16,
      "budget_used_percent": 28.46955018564403
    },
    {
      "type": "training",
      "description": "Training step 4792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:26",
      "total_flops_so_far": 2.847549124369613e+16,
      "budget_used_percent": 28.475491243696126
    },
    {
      "type": "training",
      "description": "Training step 4793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:27",
      "total_flops_so_far": 2.8481432301748224e+16,
      "budget_used_percent": 28.48143230174822
    },
    {
      "type": "training",
      "description": "Training step 4794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:27",
      "total_flops_so_far": 2.848737335980032e+16,
      "budget_used_percent": 28.487373359800323
    },
    {
      "type": "training",
      "description": "Training step 4795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:28",
      "total_flops_so_far": 2.8493314417852416e+16,
      "budget_used_percent": 28.493314417852417
    },
    {
      "type": "training",
      "description": "Training step 4796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:28",
      "total_flops_so_far": 2.849925547590451e+16,
      "budget_used_percent": 28.499255475904512
    },
    {
      "type": "training",
      "description": "Training step 4797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:28",
      "total_flops_so_far": 2.850519653395661e+16,
      "budget_used_percent": 28.505196533956607
    },
    {
      "type": "training",
      "description": "Training step 4798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:29",
      "total_flops_so_far": 2.8511137592008704e+16,
      "budget_used_percent": 28.511137592008705
    },
    {
      "type": "training",
      "description": "Training step 4799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:29",
      "total_flops_so_far": 2.85170786500608e+16,
      "budget_used_percent": 28.5170786500608
    },
    {
      "type": "training",
      "description": "Training step 4800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:30",
      "total_flops_so_far": 2.8523019708112896e+16,
      "budget_used_percent": 28.523019708112894
    },
    {
      "type": "training",
      "description": "Training step 4801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:30",
      "total_flops_so_far": 2.852896076616499e+16,
      "budget_used_percent": 28.52896076616499
    },
    {
      "type": "training",
      "description": "Training step 4802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:30",
      "total_flops_so_far": 2.853490182421709e+16,
      "budget_used_percent": 28.53490182421709
    },
    {
      "type": "training",
      "description": "Training step 4803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:31",
      "total_flops_so_far": 2.8540842882269184e+16,
      "budget_used_percent": 28.540842882269185
    },
    {
      "type": "training",
      "description": "Training step 4804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:31",
      "total_flops_so_far": 2.854678394032128e+16,
      "budget_used_percent": 28.54678394032128
    },
    {
      "type": "training",
      "description": "Training step 4805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:32",
      "total_flops_so_far": 2.8552724998373376e+16,
      "budget_used_percent": 28.552724998373375
    },
    {
      "type": "training",
      "description": "Training step 4806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:32",
      "total_flops_so_far": 2.855866605642547e+16,
      "budget_used_percent": 28.558666056425473
    },
    {
      "type": "training",
      "description": "Training step 4807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:33",
      "total_flops_so_far": 2.856460711447757e+16,
      "budget_used_percent": 28.564607114477568
    },
    {
      "type": "training",
      "description": "Training step 4808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:33",
      "total_flops_so_far": 2.8570548172529664e+16,
      "budget_used_percent": 28.570548172529662
    },
    {
      "type": "training",
      "description": "Training step 4809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:33",
      "total_flops_so_far": 2.857648923058176e+16,
      "budget_used_percent": 28.576489230581757
    },
    {
      "type": "training",
      "description": "Training step 4810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:34",
      "total_flops_so_far": 2.8582430288633856e+16,
      "budget_used_percent": 28.58243028863385
    },
    {
      "type": "training",
      "description": "Training step 4811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:34",
      "total_flops_so_far": 2.858837134668595e+16,
      "budget_used_percent": 28.588371346685953
    },
    {
      "type": "training",
      "description": "Training step 4812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:35",
      "total_flops_so_far": 2.859431240473805e+16,
      "budget_used_percent": 28.594312404738048
    },
    {
      "type": "training",
      "description": "Training step 4813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:35",
      "total_flops_so_far": 2.8600253462790144e+16,
      "budget_used_percent": 28.600253462790143
    },
    {
      "type": "training",
      "description": "Training step 4814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:36",
      "total_flops_so_far": 2.860619452084224e+16,
      "budget_used_percent": 28.606194520842237
    },
    {
      "type": "training",
      "description": "Training step 4815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:36",
      "total_flops_so_far": 2.8612135578894336e+16,
      "budget_used_percent": 28.61213557889434
    },
    {
      "type": "training",
      "description": "Training step 4816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:37",
      "total_flops_so_far": 2.861807663694643e+16,
      "budget_used_percent": 28.618076636946434
    },
    {
      "type": "training",
      "description": "Training step 4817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:37",
      "total_flops_so_far": 2.862401769499853e+16,
      "budget_used_percent": 28.62401769499853
    },
    {
      "type": "training",
      "description": "Training step 4818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:37",
      "total_flops_so_far": 2.8629958753050624e+16,
      "budget_used_percent": 28.629958753050623
    },
    {
      "type": "training",
      "description": "Training step 4819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:38",
      "total_flops_so_far": 2.863589981110272e+16,
      "budget_used_percent": 28.63589981110272
    },
    {
      "type": "training",
      "description": "Training step 4820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:38",
      "total_flops_so_far": 2.8641840869154816e+16,
      "budget_used_percent": 28.641840869154816
    },
    {
      "type": "training",
      "description": "Training step 4821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:39",
      "total_flops_so_far": 2.864778192720691e+16,
      "budget_used_percent": 28.64778192720691
    },
    {
      "type": "training",
      "description": "Training step 4822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:39",
      "total_flops_so_far": 2.865372298525901e+16,
      "budget_used_percent": 28.653722985259005
    },
    {
      "type": "training",
      "description": "Training step 4823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:40",
      "total_flops_so_far": 2.8659664043311104e+16,
      "budget_used_percent": 28.659664043311107
    },
    {
      "type": "training",
      "description": "Training step 4824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:40",
      "total_flops_so_far": 2.86656051013632e+16,
      "budget_used_percent": 28.6656051013632
    },
    {
      "type": "training",
      "description": "Training step 4825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:40",
      "total_flops_so_far": 2.8671546159415296e+16,
      "budget_used_percent": 28.671546159415296
    },
    {
      "type": "training",
      "description": "Training step 4826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:41",
      "total_flops_so_far": 2.867748721746739e+16,
      "budget_used_percent": 28.67748721746739
    },
    {
      "type": "training",
      "description": "Training step 4827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:41",
      "total_flops_so_far": 2.868342827551949e+16,
      "budget_used_percent": 28.68342827551949
    },
    {
      "type": "training",
      "description": "Training step 4828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:42",
      "total_flops_so_far": 2.8689369333571584e+16,
      "budget_used_percent": 28.689369333571584
    },
    {
      "type": "training",
      "description": "Training step 4829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:42",
      "total_flops_so_far": 2.869531039162368e+16,
      "budget_used_percent": 28.69531039162368
    },
    {
      "type": "training",
      "description": "Training step 4830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:43",
      "total_flops_so_far": 2.8701251449675776e+16,
      "budget_used_percent": 28.701251449675773
    },
    {
      "type": "training",
      "description": "Training step 4831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:43",
      "total_flops_so_far": 2.870719250772787e+16,
      "budget_used_percent": 28.707192507727875
    },
    {
      "type": "training",
      "description": "Training step 4832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:43",
      "total_flops_so_far": 2.871313356577997e+16,
      "budget_used_percent": 28.71313356577997
    },
    {
      "type": "training",
      "description": "Training step 4833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:44",
      "total_flops_so_far": 2.8719074623832064e+16,
      "budget_used_percent": 28.719074623832064
    },
    {
      "type": "training",
      "description": "Training step 4834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:44",
      "total_flops_so_far": 2.872501568188416e+16,
      "budget_used_percent": 28.72501568188416
    },
    {
      "type": "training",
      "description": "Training step 4835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:45",
      "total_flops_so_far": 2.8730956739936256e+16,
      "budget_used_percent": 28.730956739936257
    },
    {
      "type": "training",
      "description": "Training step 4836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:45",
      "total_flops_so_far": 2.873689779798835e+16,
      "budget_used_percent": 28.73689779798835
    },
    {
      "type": "training",
      "description": "Training step 4837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:45",
      "total_flops_so_far": 2.874283885604045e+16,
      "budget_used_percent": 28.742838856040446
    },
    {
      "type": "training",
      "description": "Training step 4838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:46",
      "total_flops_so_far": 2.8748779914092544e+16,
      "budget_used_percent": 28.74877991409254
    },
    {
      "type": "training",
      "description": "Training step 4839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:46",
      "total_flops_so_far": 2.875472097214464e+16,
      "budget_used_percent": 28.754720972144636
    },
    {
      "type": "training",
      "description": "Training step 4840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:47",
      "total_flops_so_far": 2.8760662030196736e+16,
      "budget_used_percent": 28.760662030196738
    },
    {
      "type": "training",
      "description": "Training step 4841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:47",
      "total_flops_so_far": 2.876660308824883e+16,
      "budget_used_percent": 28.766603088248832
    },
    {
      "type": "training",
      "description": "Training step 4842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:48",
      "total_flops_so_far": 2.877254414630093e+16,
      "budget_used_percent": 28.772544146300927
    },
    {
      "type": "training",
      "description": "Training step 4843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:48",
      "total_flops_so_far": 2.8778485204353024e+16,
      "budget_used_percent": 28.77848520435302
    },
    {
      "type": "training",
      "description": "Training step 4844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:48",
      "total_flops_so_far": 2.878442626240512e+16,
      "budget_used_percent": 28.784426262405123
    },
    {
      "type": "training",
      "description": "Training step 4845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:49",
      "total_flops_so_far": 2.8790367320457216e+16,
      "budget_used_percent": 28.790367320457218
    },
    {
      "type": "training",
      "description": "Training step 4846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:49",
      "total_flops_so_far": 2.879630837850931e+16,
      "budget_used_percent": 28.796308378509313
    },
    {
      "type": "training",
      "description": "Training step 4847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:50",
      "total_flops_so_far": 2.880224943656141e+16,
      "budget_used_percent": 28.802249436561407
    },
    {
      "type": "training",
      "description": "Training step 4848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:50",
      "total_flops_so_far": 2.8808190494613504e+16,
      "budget_used_percent": 28.808190494613505
    },
    {
      "type": "training",
      "description": "Training step 4849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:50",
      "total_flops_so_far": 2.88141315526656e+16,
      "budget_used_percent": 28.8141315526656
    },
    {
      "type": "training",
      "description": "Training step 4850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:51",
      "total_flops_so_far": 2.8820072610717696e+16,
      "budget_used_percent": 28.820072610717695
    },
    {
      "type": "training",
      "description": "Training step 4851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:52",
      "total_flops_so_far": 2.882601366876979e+16,
      "budget_used_percent": 28.82601366876979
    },
    {
      "type": "training",
      "description": "Training step 4852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:52",
      "total_flops_so_far": 2.883195472682189e+16,
      "budget_used_percent": 28.83195472682189
    },
    {
      "type": "training",
      "description": "Training step 4853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:52",
      "total_flops_so_far": 2.8837895784873984e+16,
      "budget_used_percent": 28.837895784873986
    },
    {
      "type": "training",
      "description": "Training step 4854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:53",
      "total_flops_so_far": 2.884383684292608e+16,
      "budget_used_percent": 28.84383684292608
    },
    {
      "type": "training",
      "description": "Training step 4855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:53",
      "total_flops_so_far": 2.8849777900978176e+16,
      "budget_used_percent": 28.849777900978175
    },
    {
      "type": "training",
      "description": "Training step 4856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:54",
      "total_flops_so_far": 2.885571895903027e+16,
      "budget_used_percent": 28.855718959030273
    },
    {
      "type": "training",
      "description": "Training step 4857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:54",
      "total_flops_so_far": 2.886166001708237e+16,
      "budget_used_percent": 28.861660017082368
    },
    {
      "type": "training",
      "description": "Training step 4858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:55",
      "total_flops_so_far": 2.8867601075134464e+16,
      "budget_used_percent": 28.867601075134463
    },
    {
      "type": "training",
      "description": "Training step 4859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:55",
      "total_flops_so_far": 2.887354213318656e+16,
      "budget_used_percent": 28.873542133186557
    },
    {
      "type": "training",
      "description": "Training step 4860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:55",
      "total_flops_so_far": 2.8879483191238656e+16,
      "budget_used_percent": 28.87948319123866
    },
    {
      "type": "training",
      "description": "Training step 4861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:56",
      "total_flops_so_far": 2.888542424929075e+16,
      "budget_used_percent": 28.885424249290754
    },
    {
      "type": "training",
      "description": "Training step 4862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:56",
      "total_flops_so_far": 2.889136530734285e+16,
      "budget_used_percent": 28.89136530734285
    },
    {
      "type": "training",
      "description": "Training step 4863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:57",
      "total_flops_so_far": 2.8897306365394944e+16,
      "budget_used_percent": 28.897306365394943
    },
    {
      "type": "training",
      "description": "Training step 4864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:57",
      "total_flops_so_far": 2.890324742344704e+16,
      "budget_used_percent": 28.90324742344704
    },
    {
      "type": "training",
      "description": "Training step 4865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:58",
      "total_flops_so_far": 2.8909188481499136e+16,
      "budget_used_percent": 28.909188481499136
    },
    {
      "type": "training",
      "description": "Training step 4866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:58",
      "total_flops_so_far": 2.891512953955123e+16,
      "budget_used_percent": 28.91512953955123
    },
    {
      "type": "training",
      "description": "Training step 4867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:58",
      "total_flops_so_far": 2.892107059760333e+16,
      "budget_used_percent": 28.921070597603325
    },
    {
      "type": "training",
      "description": "Training step 4868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:59",
      "total_flops_so_far": 2.8927011655655424e+16,
      "budget_used_percent": 28.92701165565542
    },
    {
      "type": "training",
      "description": "Training step 4869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:08:59",
      "total_flops_so_far": 2.893295271370752e+16,
      "budget_used_percent": 28.93295271370752
    },
    {
      "type": "training",
      "description": "Training step 4870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:00",
      "total_flops_so_far": 2.8938893771759616e+16,
      "budget_used_percent": 28.938893771759616
    },
    {
      "type": "training",
      "description": "Training step 4871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:00",
      "total_flops_so_far": 2.894483482981171e+16,
      "budget_used_percent": 28.94483482981171
    },
    {
      "type": "training",
      "description": "Training step 4872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:00",
      "total_flops_so_far": 2.895077588786381e+16,
      "budget_used_percent": 28.950775887863806
    },
    {
      "type": "training",
      "description": "Training step 4873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:01",
      "total_flops_so_far": 2.8956716945915904e+16,
      "budget_used_percent": 28.956716945915908
    },
    {
      "type": "training",
      "description": "Training step 4874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:01",
      "total_flops_so_far": 2.8962658003968e+16,
      "budget_used_percent": 28.962658003968002
    },
    {
      "type": "training",
      "description": "Training step 4875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:02",
      "total_flops_so_far": 2.8968599062020096e+16,
      "budget_used_percent": 28.968599062020097
    },
    {
      "type": "training",
      "description": "Training step 4876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:02",
      "total_flops_so_far": 2.897454012007219e+16,
      "budget_used_percent": 28.97454012007219
    },
    {
      "type": "training",
      "description": "Training step 4877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:03",
      "total_flops_so_far": 2.898048117812429e+16,
      "budget_used_percent": 28.98048117812429
    },
    {
      "type": "training",
      "description": "Training step 4878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:03",
      "total_flops_so_far": 2.8986422236176384e+16,
      "budget_used_percent": 28.986422236176384
    },
    {
      "type": "training",
      "description": "Training step 4879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:03",
      "total_flops_so_far": 2.899236329422848e+16,
      "budget_used_percent": 28.99236329422848
    },
    {
      "type": "training",
      "description": "Training step 4880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:04",
      "total_flops_so_far": 2.8998304352280576e+16,
      "budget_used_percent": 28.998304352280574
    },
    {
      "type": "training",
      "description": "Training step 4881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:04",
      "total_flops_so_far": 2.900424541033267e+16,
      "budget_used_percent": 29.004245410332675
    },
    {
      "type": "training",
      "description": "Training step 4882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:05",
      "total_flops_so_far": 2.901018646838477e+16,
      "budget_used_percent": 29.01018646838477
    },
    {
      "type": "training",
      "description": "Training step 4883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:05",
      "total_flops_so_far": 2.9016127526436864e+16,
      "budget_used_percent": 29.016127526436865
    },
    {
      "type": "training",
      "description": "Training step 4884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:05",
      "total_flops_so_far": 2.902206858448896e+16,
      "budget_used_percent": 29.02206858448896
    },
    {
      "type": "training",
      "description": "Training step 4885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:06",
      "total_flops_so_far": 2.9028009642541056e+16,
      "budget_used_percent": 29.028009642541058
    },
    {
      "type": "training",
      "description": "Training step 4886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:06",
      "total_flops_so_far": 2.903395070059315e+16,
      "budget_used_percent": 29.033950700593152
    },
    {
      "type": "training",
      "description": "Training step 4887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:07",
      "total_flops_so_far": 2.903989175864525e+16,
      "budget_used_percent": 29.039891758645247
    },
    {
      "type": "training",
      "description": "Training step 4888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:07",
      "total_flops_so_far": 2.9045832816697344e+16,
      "budget_used_percent": 29.04583281669734
    },
    {
      "type": "training",
      "description": "Training step 4889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:08",
      "total_flops_so_far": 2.905177387474944e+16,
      "budget_used_percent": 29.051773874749443
    },
    {
      "type": "training",
      "description": "Training step 4890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:08",
      "total_flops_so_far": 2.9057714932801536e+16,
      "budget_used_percent": 29.057714932801538
    },
    {
      "type": "training",
      "description": "Training step 4891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:09",
      "total_flops_so_far": 2.906365599085363e+16,
      "budget_used_percent": 29.063655990853633
    },
    {
      "type": "training",
      "description": "Training step 4892",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:09",
      "total_flops_so_far": 2.906959704890573e+16,
      "budget_used_percent": 29.069597048905727
    },
    {
      "type": "training",
      "description": "Training step 4893",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:10",
      "total_flops_so_far": 2.9075538106957824e+16,
      "budget_used_percent": 29.075538106957826
    },
    {
      "type": "training",
      "description": "Training step 4894",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:10",
      "total_flops_so_far": 2.908147916500992e+16,
      "budget_used_percent": 29.08147916500992
    },
    {
      "type": "training",
      "description": "Training step 4895",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:10",
      "total_flops_so_far": 2.9087420223062016e+16,
      "budget_used_percent": 29.087420223062015
    },
    {
      "type": "training",
      "description": "Training step 4896",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:11",
      "total_flops_so_far": 2.909336128111411e+16,
      "budget_used_percent": 29.09336128111411
    },
    {
      "type": "training",
      "description": "Training step 4897",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:11",
      "total_flops_so_far": 2.909930233916621e+16,
      "budget_used_percent": 29.099302339166204
    },
    {
      "type": "training",
      "description": "Training step 4898",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:12",
      "total_flops_so_far": 2.9105243397218304e+16,
      "budget_used_percent": 29.105243397218306
    },
    {
      "type": "training",
      "description": "Training step 4899",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:12",
      "total_flops_so_far": 2.91111844552704e+16,
      "budget_used_percent": 29.1111844552704
    },
    {
      "type": "training",
      "description": "Training step 4900",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:13",
      "total_flops_so_far": 2.9117125513322496e+16,
      "budget_used_percent": 29.117125513322495
    },
    {
      "type": "training",
      "description": "Training step 4901",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:13",
      "total_flops_so_far": 2.912306657137459e+16,
      "budget_used_percent": 29.12306657137459
    },
    {
      "type": "training",
      "description": "Training step 4902",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:13",
      "total_flops_so_far": 2.912900762942669e+16,
      "budget_used_percent": 29.129007629426688
    },
    {
      "type": "training",
      "description": "Training step 4903",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:14",
      "total_flops_so_far": 2.9134948687478784e+16,
      "budget_used_percent": 29.134948687478783
    },
    {
      "type": "training",
      "description": "Training step 4904",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:14",
      "total_flops_so_far": 2.914088974553088e+16,
      "budget_used_percent": 29.140889745530878
    },
    {
      "type": "training",
      "description": "Training step 4905",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:15",
      "total_flops_so_far": 2.9146830803582976e+16,
      "budget_used_percent": 29.146830803582972
    },
    {
      "type": "training",
      "description": "Training step 4906",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:15",
      "total_flops_so_far": 2.915277186163507e+16,
      "budget_used_percent": 29.152771861635074
    },
    {
      "type": "training",
      "description": "Training step 4907",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:15",
      "total_flops_so_far": 2.915871291968717e+16,
      "budget_used_percent": 29.15871291968717
    },
    {
      "type": "training",
      "description": "Training step 4908",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:16",
      "total_flops_so_far": 2.9164653977739264e+16,
      "budget_used_percent": 29.164653977739263
    },
    {
      "type": "training",
      "description": "Training step 4909",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:16",
      "total_flops_so_far": 2.917059503579136e+16,
      "budget_used_percent": 29.170595035791358
    },
    {
      "type": "training",
      "description": "Training step 4910",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:17",
      "total_flops_so_far": 2.9176536093843456e+16,
      "budget_used_percent": 29.17653609384346
    },
    {
      "type": "training",
      "description": "Training step 4911",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:17",
      "total_flops_so_far": 2.918247715189555e+16,
      "budget_used_percent": 29.182477151895554
    },
    {
      "type": "training",
      "description": "Training step 4912",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:18",
      "total_flops_so_far": 2.918841820994765e+16,
      "budget_used_percent": 29.18841820994765
    },
    {
      "type": "training",
      "description": "Training step 4913",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:18",
      "total_flops_so_far": 2.9194359267999744e+16,
      "budget_used_percent": 29.194359267999744
    },
    {
      "type": "training",
      "description": "Training step 4914",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:18",
      "total_flops_so_far": 2.920030032605184e+16,
      "budget_used_percent": 29.200300326051842
    },
    {
      "type": "training",
      "description": "Training step 4915",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:19",
      "total_flops_so_far": 2.9206241384103936e+16,
      "budget_used_percent": 29.206241384103937
    },
    {
      "type": "training",
      "description": "Training step 4916",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:19",
      "total_flops_so_far": 2.921218244215603e+16,
      "budget_used_percent": 29.21218244215603
    },
    {
      "type": "training",
      "description": "Training step 4917",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:20",
      "total_flops_so_far": 2.921812350020813e+16,
      "budget_used_percent": 29.218123500208126
    },
    {
      "type": "training",
      "description": "Training step 4918",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:20",
      "total_flops_so_far": 2.9224064558260224e+16,
      "budget_used_percent": 29.224064558260228
    },
    {
      "type": "training",
      "description": "Training step 4919",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:20",
      "total_flops_so_far": 2.923000561631232e+16,
      "budget_used_percent": 29.230005616312322
    },
    {
      "type": "training",
      "description": "Training step 4920",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:21",
      "total_flops_so_far": 2.9235946674364416e+16,
      "budget_used_percent": 29.235946674364417
    },
    {
      "type": "training",
      "description": "Training step 4921",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:22",
      "total_flops_so_far": 2.924188773241651e+16,
      "budget_used_percent": 29.24188773241651
    },
    {
      "type": "training",
      "description": "Training step 4922",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:22",
      "total_flops_so_far": 2.924782879046861e+16,
      "budget_used_percent": 29.24782879046861
    },
    {
      "type": "training",
      "description": "Training step 4923",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:22",
      "total_flops_so_far": 2.9253769848520704e+16,
      "budget_used_percent": 29.253769848520704
    },
    {
      "type": "training",
      "description": "Training step 4924",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:23",
      "total_flops_so_far": 2.92597109065728e+16,
      "budget_used_percent": 29.2597109065728
    },
    {
      "type": "training",
      "description": "Training step 4925",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:23",
      "total_flops_so_far": 2.9265651964624896e+16,
      "budget_used_percent": 29.265651964624894
    },
    {
      "type": "training",
      "description": "Training step 4926",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:24",
      "total_flops_so_far": 2.927159302267699e+16,
      "budget_used_percent": 29.27159302267699
    },
    {
      "type": "training",
      "description": "Training step 4927",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:24",
      "total_flops_so_far": 2.927753408072909e+16,
      "budget_used_percent": 29.27753408072909
    },
    {
      "type": "training",
      "description": "Training step 4928",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:25",
      "total_flops_so_far": 2.9283475138781184e+16,
      "budget_used_percent": 29.283475138781185
    },
    {
      "type": "training",
      "description": "Training step 4929",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:25",
      "total_flops_so_far": 2.928941619683328e+16,
      "budget_used_percent": 29.28941619683328
    },
    {
      "type": "training",
      "description": "Training step 4930",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:25",
      "total_flops_so_far": 2.9295357254885376e+16,
      "budget_used_percent": 29.295357254885374
    },
    {
      "type": "training",
      "description": "Training step 4931",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:26",
      "total_flops_so_far": 2.930129831293747e+16,
      "budget_used_percent": 29.301298312937472
    },
    {
      "type": "training",
      "description": "Training step 4932",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:26",
      "total_flops_so_far": 2.930723937098957e+16,
      "budget_used_percent": 29.307239370989567
    },
    {
      "type": "training",
      "description": "Training step 4933",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:27",
      "total_flops_so_far": 2.9313180429041664e+16,
      "budget_used_percent": 29.31318042904166
    },
    {
      "type": "training",
      "description": "Training step 4934",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:27",
      "total_flops_so_far": 2.931912148709376e+16,
      "budget_used_percent": 29.319121487093756
    },
    {
      "type": "training",
      "description": "Training step 4935",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:27",
      "total_flops_so_far": 2.9325062545145856e+16,
      "budget_used_percent": 29.325062545145858
    },
    {
      "type": "training",
      "description": "Training step 4936",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:28",
      "total_flops_so_far": 2.933100360319795e+16,
      "budget_used_percent": 29.331003603197953
    },
    {
      "type": "training",
      "description": "Training step 4937",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:28",
      "total_flops_so_far": 2.933694466125005e+16,
      "budget_used_percent": 29.336944661250048
    },
    {
      "type": "training",
      "description": "Training step 4938",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:29",
      "total_flops_so_far": 2.9342885719302144e+16,
      "budget_used_percent": 29.342885719302142
    },
    {
      "type": "training",
      "description": "Training step 4939",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:29",
      "total_flops_so_far": 2.934882677735424e+16,
      "budget_used_percent": 29.34882677735424
    },
    {
      "type": "training",
      "description": "Training step 4940",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:30",
      "total_flops_so_far": 2.9354767835406336e+16,
      "budget_used_percent": 29.354767835406335
    },
    {
      "type": "training",
      "description": "Training step 4941",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:30",
      "total_flops_so_far": 2.936070889345843e+16,
      "budget_used_percent": 29.36070889345843
    },
    {
      "type": "training",
      "description": "Training step 4942",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:30",
      "total_flops_so_far": 2.936664995151053e+16,
      "budget_used_percent": 29.366649951510524
    },
    {
      "type": "training",
      "description": "Training step 4943",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:31",
      "total_flops_so_far": 2.9372591009562624e+16,
      "budget_used_percent": 29.372591009562626
    },
    {
      "type": "training",
      "description": "Training step 4944",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:31",
      "total_flops_so_far": 2.937853206761472e+16,
      "budget_used_percent": 29.37853206761472
    },
    {
      "type": "training",
      "description": "Training step 4945",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:32",
      "total_flops_so_far": 2.9384473125666816e+16,
      "budget_used_percent": 29.384473125666815
    },
    {
      "type": "training",
      "description": "Training step 4946",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:32",
      "total_flops_so_far": 2.939041418371891e+16,
      "budget_used_percent": 29.39041418371891
    },
    {
      "type": "training",
      "description": "Training step 4947",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:33",
      "total_flops_so_far": 2.939635524177101e+16,
      "budget_used_percent": 29.396355241771012
    },
    {
      "type": "training",
      "description": "Training step 4948",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:33",
      "total_flops_so_far": 2.9402296299823104e+16,
      "budget_used_percent": 29.402296299823107
    },
    {
      "type": "training",
      "description": "Training step 4949",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:33",
      "total_flops_so_far": 2.94082373578752e+16,
      "budget_used_percent": 29.4082373578752
    },
    {
      "type": "training",
      "description": "Training step 4950",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:34",
      "total_flops_so_far": 2.9414178415927296e+16,
      "budget_used_percent": 29.414178415927296
    },
    {
      "type": "training",
      "description": "Training step 4951",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:34",
      "total_flops_so_far": 2.942011947397939e+16,
      "budget_used_percent": 29.420119473979394
    },
    {
      "type": "training",
      "description": "Training step 4952",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:35",
      "total_flops_so_far": 2.942606053203149e+16,
      "budget_used_percent": 29.42606053203149
    },
    {
      "type": "training",
      "description": "Training step 4953",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:35",
      "total_flops_so_far": 2.9432001590083584e+16,
      "budget_used_percent": 29.432001590083583
    },
    {
      "type": "training",
      "description": "Training step 4954",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:35",
      "total_flops_so_far": 2.943794264813568e+16,
      "budget_used_percent": 29.437942648135678
    },
    {
      "type": "training",
      "description": "Training step 4955",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:36",
      "total_flops_so_far": 2.9443883706187776e+16,
      "budget_used_percent": 29.443883706187773
    },
    {
      "type": "training",
      "description": "Training step 4956",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:37",
      "total_flops_so_far": 2.944982476423987e+16,
      "budget_used_percent": 29.449824764239874
    },
    {
      "type": "training",
      "description": "Training step 4957",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:37",
      "total_flops_so_far": 2.945576582229197e+16,
      "budget_used_percent": 29.45576582229197
    },
    {
      "type": "training",
      "description": "Training step 4958",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:37",
      "total_flops_so_far": 2.9461706880344064e+16,
      "budget_used_percent": 29.461706880344064
    },
    {
      "type": "training",
      "description": "Training step 4959",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:38",
      "total_flops_so_far": 2.946764793839616e+16,
      "budget_used_percent": 29.46764793839616
    },
    {
      "type": "training",
      "description": "Training step 4960",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:38",
      "total_flops_so_far": 2.9473588996448256e+16,
      "budget_used_percent": 29.473588996448257
    },
    {
      "type": "training",
      "description": "Training step 4961",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:39",
      "total_flops_so_far": 2.947953005450035e+16,
      "budget_used_percent": 29.47953005450035
    },
    {
      "type": "training",
      "description": "Training step 4962",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:39",
      "total_flops_so_far": 2.948547111255245e+16,
      "budget_used_percent": 29.485471112552446
    },
    {
      "type": "training",
      "description": "Training step 4963",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:40",
      "total_flops_so_far": 2.9491412170604544e+16,
      "budget_used_percent": 29.49141217060454
    },
    {
      "type": "training",
      "description": "Training step 4964",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:40",
      "total_flops_so_far": 2.949735322865664e+16,
      "budget_used_percent": 29.497353228656642
    },
    {
      "type": "training",
      "description": "Training step 4965",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:40",
      "total_flops_so_far": 2.9503294286708736e+16,
      "budget_used_percent": 29.503294286708737
    },
    {
      "type": "training",
      "description": "Training step 4966",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:41",
      "total_flops_so_far": 2.950923534476083e+16,
      "budget_used_percent": 29.50923534476083
    },
    {
      "type": "training",
      "description": "Training step 4967",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:41",
      "total_flops_so_far": 2.951517640281293e+16,
      "budget_used_percent": 29.515176402812926
    },
    {
      "type": "training",
      "description": "Training step 4968",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:42",
      "total_flops_so_far": 2.9521117460865024e+16,
      "budget_used_percent": 29.521117460865025
    },
    {
      "type": "training",
      "description": "Training step 4969",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:42",
      "total_flops_so_far": 2.952705851891712e+16,
      "budget_used_percent": 29.52705851891712
    },
    {
      "type": "training",
      "description": "Training step 4970",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:43",
      "total_flops_so_far": 2.9532999576969216e+16,
      "budget_used_percent": 29.532999576969214
    },
    {
      "type": "training",
      "description": "Training step 4971",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:43",
      "total_flops_so_far": 2.953894063502131e+16,
      "budget_used_percent": 29.53894063502131
    },
    {
      "type": "training",
      "description": "Training step 4972",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:43",
      "total_flops_so_far": 2.954488169307341e+16,
      "budget_used_percent": 29.54488169307341
    },
    {
      "type": "training",
      "description": "Training step 4973",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:44",
      "total_flops_so_far": 2.9550822751125504e+16,
      "budget_used_percent": 29.550822751125505
    },
    {
      "type": "training",
      "description": "Training step 4974",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:44",
      "total_flops_so_far": 2.95567638091776e+16,
      "budget_used_percent": 29.5567638091776
    },
    {
      "type": "training",
      "description": "Training step 4975",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:45",
      "total_flops_so_far": 2.9562704867229696e+16,
      "budget_used_percent": 29.562704867229694
    },
    {
      "type": "training",
      "description": "Training step 4976",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:45",
      "total_flops_so_far": 2.956864592528179e+16,
      "budget_used_percent": 29.568645925281796
    },
    {
      "type": "training",
      "description": "Training step 4977",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:46",
      "total_flops_so_far": 2.957458698333389e+16,
      "budget_used_percent": 29.57458698333389
    },
    {
      "type": "training",
      "description": "Training step 4978",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:46",
      "total_flops_so_far": 2.9580528041385984e+16,
      "budget_used_percent": 29.580528041385985
    },
    {
      "type": "training",
      "description": "Training step 4979",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:46",
      "total_flops_so_far": 2.958646909943808e+16,
      "budget_used_percent": 29.58646909943808
    },
    {
      "type": "training",
      "description": "Training step 4980",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:47",
      "total_flops_so_far": 2.9592410157490176e+16,
      "budget_used_percent": 29.592410157490175
    },
    {
      "type": "training",
      "description": "Training step 4981",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:47",
      "total_flops_so_far": 2.959835121554227e+16,
      "budget_used_percent": 29.598351215542273
    },
    {
      "type": "training",
      "description": "Training step 4982",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:48",
      "total_flops_so_far": 2.960429227359437e+16,
      "budget_used_percent": 29.604292273594368
    },
    {
      "type": "training",
      "description": "Training step 4983",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:48",
      "total_flops_so_far": 2.9610233331646464e+16,
      "budget_used_percent": 29.610233331646462
    },
    {
      "type": "training",
      "description": "Training step 4984",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:48",
      "total_flops_so_far": 2.961617438969856e+16,
      "budget_used_percent": 29.616174389698557
    },
    {
      "type": "training",
      "description": "Training step 4985",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:49",
      "total_flops_so_far": 2.9622115447750656e+16,
      "budget_used_percent": 29.62211544775066
    },
    {
      "type": "training",
      "description": "Training step 4986",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:49",
      "total_flops_so_far": 2.962805650580275e+16,
      "budget_used_percent": 29.628056505802753
    },
    {
      "type": "training",
      "description": "Training step 4987",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:50",
      "total_flops_so_far": 2.963399756385485e+16,
      "budget_used_percent": 29.633997563854848
    },
    {
      "type": "training",
      "description": "Training step 4988",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:50",
      "total_flops_so_far": 2.9639938621906944e+16,
      "budget_used_percent": 29.639938621906943
    },
    {
      "type": "training",
      "description": "Training step 4989",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:51",
      "total_flops_so_far": 2.964587967995904e+16,
      "budget_used_percent": 29.64587967995904
    },
    {
      "type": "training",
      "description": "Training step 4990",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:51",
      "total_flops_so_far": 2.9651820738011136e+16,
      "budget_used_percent": 29.651820738011136
    },
    {
      "type": "training",
      "description": "Training step 4991",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:52",
      "total_flops_so_far": 2.965776179606323e+16,
      "budget_used_percent": 29.65776179606323
    },
    {
      "type": "training",
      "description": "Training step 4992",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:52",
      "total_flops_so_far": 2.966370285411533e+16,
      "budget_used_percent": 29.663702854115325
    },
    {
      "type": "training",
      "description": "Training step 4993",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:53",
      "total_flops_so_far": 2.9669643912167424e+16,
      "budget_used_percent": 29.669643912167427
    },
    {
      "type": "training",
      "description": "Training step 4994",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:53",
      "total_flops_so_far": 2.967558497021952e+16,
      "budget_used_percent": 29.67558497021952
    },
    {
      "type": "training",
      "description": "Training step 4995",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:53",
      "total_flops_so_far": 2.9681526028271616e+16,
      "budget_used_percent": 29.681526028271616
    },
    {
      "type": "training",
      "description": "Training step 4996",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:54",
      "total_flops_so_far": 2.968746708632371e+16,
      "budget_used_percent": 29.68746708632371
    },
    {
      "type": "training",
      "description": "Training step 4997",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:54",
      "total_flops_so_far": 2.969340814437581e+16,
      "budget_used_percent": 29.69340814437581
    },
    {
      "type": "training",
      "description": "Training step 4998",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:55",
      "total_flops_so_far": 2.9699349202427904e+16,
      "budget_used_percent": 29.699349202427904
    },
    {
      "type": "training",
      "description": "Training step 4999",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:09:55",
      "total_flops_so_far": 2.970529026048e+16,
      "budget_used_percent": 29.705290260479998
    },
    {
      "type": "training",
      "description": "Training step 5000",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:19",
      "total_flops_so_far": 2.9711231318532096e+16,
      "budget_used_percent": 29.711231318532093
    },
    {
      "type": "training",
      "description": "Training step 5001",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:20",
      "total_flops_so_far": 2.971717237658419e+16,
      "budget_used_percent": 29.717172376584195
    },
    {
      "type": "training",
      "description": "Training step 5002",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:20",
      "total_flops_so_far": 2.972311343463629e+16,
      "budget_used_percent": 29.72311343463629
    },
    {
      "type": "training",
      "description": "Training step 5003",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:20",
      "total_flops_so_far": 2.9729054492688384e+16,
      "budget_used_percent": 29.729054492688384
    },
    {
      "type": "training",
      "description": "Training step 5004",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:21",
      "total_flops_so_far": 2.973499555074048e+16,
      "budget_used_percent": 29.73499555074048
    },
    {
      "type": "training",
      "description": "Training step 5005",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:21",
      "total_flops_so_far": 2.9740936608792576e+16,
      "budget_used_percent": 29.74093660879258
    },
    {
      "type": "training",
      "description": "Training step 5006",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:22",
      "total_flops_so_far": 2.974687766684467e+16,
      "budget_used_percent": 29.746877666844675
    },
    {
      "type": "training",
      "description": "Training step 5007",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:22",
      "total_flops_so_far": 2.975281872489677e+16,
      "budget_used_percent": 29.75281872489677
    },
    {
      "type": "training",
      "description": "Training step 5008",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:23",
      "total_flops_so_far": 2.9758759782948864e+16,
      "budget_used_percent": 29.758759782948864
    },
    {
      "type": "training",
      "description": "Training step 5009",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:23",
      "total_flops_so_far": 2.976470084100096e+16,
      "budget_used_percent": 29.76470084100096
    },
    {
      "type": "training",
      "description": "Training step 5010",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:23",
      "total_flops_so_far": 2.9770641899053056e+16,
      "budget_used_percent": 29.770641899053057
    },
    {
      "type": "training",
      "description": "Training step 5011",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:24",
      "total_flops_so_far": 2.977658295710515e+16,
      "budget_used_percent": 29.776582957105152
    },
    {
      "type": "training",
      "description": "Training step 5012",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:24",
      "total_flops_so_far": 2.978252401515725e+16,
      "budget_used_percent": 29.782524015157247
    },
    {
      "type": "training",
      "description": "Training step 5013",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:25",
      "total_flops_so_far": 2.9788465073209344e+16,
      "budget_used_percent": 29.78846507320934
    },
    {
      "type": "training",
      "description": "Training step 5014",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:25",
      "total_flops_so_far": 2.979440613126144e+16,
      "budget_used_percent": 29.794406131261443
    },
    {
      "type": "training",
      "description": "Training step 5015",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:26",
      "total_flops_so_far": 2.9800347189313536e+16,
      "budget_used_percent": 29.800347189313538
    },
    {
      "type": "training",
      "description": "Training step 5016",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:26",
      "total_flops_so_far": 2.980628824736563e+16,
      "budget_used_percent": 29.806288247365632
    },
    {
      "type": "training",
      "description": "Training step 5017",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:26",
      "total_flops_so_far": 2.981222930541773e+16,
      "budget_used_percent": 29.812229305417727
    },
    {
      "type": "training",
      "description": "Training step 5018",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:27",
      "total_flops_so_far": 2.9818170363469824e+16,
      "budget_used_percent": 29.818170363469825
    },
    {
      "type": "training",
      "description": "Training step 5019",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:28",
      "total_flops_so_far": 2.982411142152192e+16,
      "budget_used_percent": 29.82411142152192
    },
    {
      "type": "training",
      "description": "Training step 5020",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:28",
      "total_flops_so_far": 2.9830052479574016e+16,
      "budget_used_percent": 29.830052479574015
    },
    {
      "type": "training",
      "description": "Training step 5021",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:28",
      "total_flops_so_far": 2.983599353762611e+16,
      "budget_used_percent": 29.83599353762611
    },
    {
      "type": "training",
      "description": "Training step 5022",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:29",
      "total_flops_so_far": 2.984193459567821e+16,
      "budget_used_percent": 29.84193459567821
    },
    {
      "type": "training",
      "description": "Training step 5023",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:29",
      "total_flops_so_far": 2.9847875653730304e+16,
      "budget_used_percent": 29.847875653730306
    },
    {
      "type": "training",
      "description": "Training step 5024",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:30",
      "total_flops_so_far": 2.98538167117824e+16,
      "budget_used_percent": 29.8538167117824
    },
    {
      "type": "training",
      "description": "Training step 5025",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:30",
      "total_flops_so_far": 2.9859757769834496e+16,
      "budget_used_percent": 29.859757769834495
    },
    {
      "type": "training",
      "description": "Training step 5026",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:31",
      "total_flops_so_far": 2.986569882788659e+16,
      "budget_used_percent": 29.865698827886593
    },
    {
      "type": "training",
      "description": "Training step 5027",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:31",
      "total_flops_so_far": 2.987163988593869e+16,
      "budget_used_percent": 29.871639885938688
    },
    {
      "type": "training",
      "description": "Training step 5028",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:31",
      "total_flops_so_far": 2.9877580943990784e+16,
      "budget_used_percent": 29.877580943990782
    },
    {
      "type": "training",
      "description": "Training step 5029",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:32",
      "total_flops_so_far": 2.988352200204288e+16,
      "budget_used_percent": 29.883522002042877
    },
    {
      "type": "training",
      "description": "Training step 5030",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:32",
      "total_flops_so_far": 2.9889463060094976e+16,
      "budget_used_percent": 29.88946306009498
    },
    {
      "type": "training",
      "description": "Training step 5031",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:33",
      "total_flops_so_far": 2.989540411814707e+16,
      "budget_used_percent": 29.895404118147074
    },
    {
      "type": "training",
      "description": "Training step 5032",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:33",
      "total_flops_so_far": 2.990134517619917e+16,
      "budget_used_percent": 29.901345176199168
    },
    {
      "type": "training",
      "description": "Training step 5033",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:33",
      "total_flops_so_far": 2.9907286234251264e+16,
      "budget_used_percent": 29.907286234251263
    },
    {
      "type": "training",
      "description": "Training step 5034",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:34",
      "total_flops_so_far": 2.991322729230336e+16,
      "budget_used_percent": 29.91322729230336
    },
    {
      "type": "training",
      "description": "Training step 5035",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:34",
      "total_flops_so_far": 2.9919168350355456e+16,
      "budget_used_percent": 29.919168350355456
    },
    {
      "type": "training",
      "description": "Training step 5036",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:35",
      "total_flops_so_far": 2.992510940840755e+16,
      "budget_used_percent": 29.92510940840755
    },
    {
      "type": "training",
      "description": "Training step 5037",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:35",
      "total_flops_so_far": 2.993105046645965e+16,
      "budget_used_percent": 29.931050466459645
    },
    {
      "type": "training",
      "description": "Training step 5038",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:36",
      "total_flops_so_far": 2.9936991524511744e+16,
      "budget_used_percent": 29.93699152451174
    },
    {
      "type": "training",
      "description": "Training step 5039",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:36",
      "total_flops_so_far": 2.994293258256384e+16,
      "budget_used_percent": 29.94293258256384
    },
    {
      "type": "training",
      "description": "Training step 5040",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:36",
      "total_flops_so_far": 2.9948873640615936e+16,
      "budget_used_percent": 29.948873640615936
    },
    {
      "type": "training",
      "description": "Training step 5041",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:37",
      "total_flops_so_far": 2.995481469866803e+16,
      "budget_used_percent": 29.95481469866803
    },
    {
      "type": "training",
      "description": "Training step 5042",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:37",
      "total_flops_so_far": 2.996075575672013e+16,
      "budget_used_percent": 29.960755756720125
    },
    {
      "type": "training",
      "description": "Training step 5043",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:38",
      "total_flops_so_far": 2.9966696814772224e+16,
      "budget_used_percent": 29.966696814772227
    },
    {
      "type": "training",
      "description": "Training step 5044",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:38",
      "total_flops_so_far": 2.997263787282432e+16,
      "budget_used_percent": 29.972637872824322
    },
    {
      "type": "training",
      "description": "Training step 5045",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:39",
      "total_flops_so_far": 2.9978578930876416e+16,
      "budget_used_percent": 29.978578930876417
    },
    {
      "type": "training",
      "description": "Training step 5046",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:39",
      "total_flops_so_far": 2.998451998892851e+16,
      "budget_used_percent": 29.98451998892851
    },
    {
      "type": "training",
      "description": "Training step 5047",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:39",
      "total_flops_so_far": 2.999046104698061e+16,
      "budget_used_percent": 29.99046104698061
    },
    {
      "type": "training",
      "description": "Training step 5048",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:40",
      "total_flops_so_far": 2.9996402105032704e+16,
      "budget_used_percent": 29.996402105032704
    },
    {
      "type": "training",
      "description": "Training step 5049",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:40",
      "total_flops_so_far": 3.00023431630848e+16,
      "budget_used_percent": 30.0023431630848
    },
    {
      "type": "training",
      "description": "Training step 5050",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:41",
      "total_flops_so_far": 3.0008284221136896e+16,
      "budget_used_percent": 30.008284221136893
    },
    {
      "type": "training",
      "description": "Training step 5051",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:41",
      "total_flops_so_far": 3.001422527918899e+16,
      "budget_used_percent": 30.014225279188995
    },
    {
      "type": "training",
      "description": "Training step 5052",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:42",
      "total_flops_so_far": 3.002016633724109e+16,
      "budget_used_percent": 30.02016633724109
    },
    {
      "type": "training",
      "description": "Training step 5053",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:42",
      "total_flops_so_far": 3.0026107395293184e+16,
      "budget_used_percent": 30.026107395293185
    },
    {
      "type": "training",
      "description": "Training step 5054",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:43",
      "total_flops_so_far": 3.003204845334528e+16,
      "budget_used_percent": 30.03204845334528
    },
    {
      "type": "training",
      "description": "Training step 5055",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:43",
      "total_flops_so_far": 3.0037989511397376e+16,
      "budget_used_percent": 30.037989511397377
    },
    {
      "type": "training",
      "description": "Training step 5056",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:44",
      "total_flops_so_far": 3.004393056944947e+16,
      "budget_used_percent": 30.043930569449472
    },
    {
      "type": "training",
      "description": "Training step 5057",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:44",
      "total_flops_so_far": 3.004987162750157e+16,
      "budget_used_percent": 30.049871627501567
    },
    {
      "type": "training",
      "description": "Training step 5058",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:44",
      "total_flops_so_far": 3.0055812685553664e+16,
      "budget_used_percent": 30.05581268555366
    },
    {
      "type": "training",
      "description": "Training step 5059",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:45",
      "total_flops_so_far": 3.006175374360576e+16,
      "budget_used_percent": 30.061753743605763
    },
    {
      "type": "training",
      "description": "Training step 5060",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:45",
      "total_flops_so_far": 3.0067694801657856e+16,
      "budget_used_percent": 30.067694801657858
    },
    {
      "type": "training",
      "description": "Training step 5061",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:46",
      "total_flops_so_far": 3.007363585970995e+16,
      "budget_used_percent": 30.073635859709952
    },
    {
      "type": "training",
      "description": "Training step 5062",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:46",
      "total_flops_so_far": 3.007957691776205e+16,
      "budget_used_percent": 30.079576917762047
    },
    {
      "type": "training",
      "description": "Training step 5063",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:47",
      "total_flops_so_far": 3.0085517975814144e+16,
      "budget_used_percent": 30.085517975814145
    },
    {
      "type": "training",
      "description": "Training step 5064",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:47",
      "total_flops_so_far": 3.009145903386624e+16,
      "budget_used_percent": 30.09145903386624
    },
    {
      "type": "training",
      "description": "Training step 5065",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:47",
      "total_flops_so_far": 3.0097400091918336e+16,
      "budget_used_percent": 30.097400091918335
    },
    {
      "type": "training",
      "description": "Training step 5066",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:48",
      "total_flops_so_far": 3.010334114997043e+16,
      "budget_used_percent": 30.10334114997043
    },
    {
      "type": "training",
      "description": "Training step 5067",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:48",
      "total_flops_so_far": 3.010928220802253e+16,
      "budget_used_percent": 30.109282208022524
    },
    {
      "type": "training",
      "description": "Training step 5068",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:49",
      "total_flops_so_far": 3.0115223266074624e+16,
      "budget_used_percent": 30.115223266074626
    },
    {
      "type": "training",
      "description": "Training step 5069",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:49",
      "total_flops_so_far": 3.012116432412672e+16,
      "budget_used_percent": 30.12116432412672
    },
    {
      "type": "training",
      "description": "Training step 5070",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:50",
      "total_flops_so_far": 3.0127105382178816e+16,
      "budget_used_percent": 30.127105382178815
    },
    {
      "type": "training",
      "description": "Training step 5071",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:50",
      "total_flops_so_far": 3.013304644023091e+16,
      "budget_used_percent": 30.13304644023091
    },
    {
      "type": "training",
      "description": "Training step 5072",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:50",
      "total_flops_so_far": 3.013898749828301e+16,
      "budget_used_percent": 30.138987498283008
    },
    {
      "type": "training",
      "description": "Training step 5073",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:51",
      "total_flops_so_far": 3.0144928556335104e+16,
      "budget_used_percent": 30.144928556335103
    },
    {
      "type": "training",
      "description": "Training step 5074",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:51",
      "total_flops_so_far": 3.01508696143872e+16,
      "budget_used_percent": 30.150869614387197
    },
    {
      "type": "training",
      "description": "Training step 5075",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:52",
      "total_flops_so_far": 3.0156810672439296e+16,
      "budget_used_percent": 30.156810672439292
    },
    {
      "type": "training",
      "description": "Training step 5076",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:52",
      "total_flops_so_far": 3.016275173049139e+16,
      "budget_used_percent": 30.162751730491394
    },
    {
      "type": "training",
      "description": "Training step 5077",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:52",
      "total_flops_so_far": 3.016869278854349e+16,
      "budget_used_percent": 30.16869278854349
    },
    {
      "type": "training",
      "description": "Training step 5078",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:53",
      "total_flops_so_far": 3.0174633846595584e+16,
      "budget_used_percent": 30.174633846595583
    },
    {
      "type": "training",
      "description": "Training step 5079",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:53",
      "total_flops_so_far": 3.018057490464768e+16,
      "budget_used_percent": 30.180574904647678
    },
    {
      "type": "training",
      "description": "Training step 5080",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:54",
      "total_flops_so_far": 3.0186515962699776e+16,
      "budget_used_percent": 30.18651596269978
    },
    {
      "type": "training",
      "description": "Training step 5081",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:54",
      "total_flops_so_far": 3.019245702075187e+16,
      "budget_used_percent": 30.192457020751874
    },
    {
      "type": "training",
      "description": "Training step 5082",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:55",
      "total_flops_so_far": 3.019839807880397e+16,
      "budget_used_percent": 30.19839807880397
    },
    {
      "type": "training",
      "description": "Training step 5083",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:55",
      "total_flops_so_far": 3.0204339136856064e+16,
      "budget_used_percent": 30.204339136856063
    },
    {
      "type": "training",
      "description": "Training step 5084",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:56",
      "total_flops_so_far": 3.021028019490816e+16,
      "budget_used_percent": 30.21028019490816
    },
    {
      "type": "training",
      "description": "Training step 5085",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:56",
      "total_flops_so_far": 3.0216221252960256e+16,
      "budget_used_percent": 30.216221252960256
    },
    {
      "type": "training",
      "description": "Training step 5086",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:57",
      "total_flops_so_far": 3.022216231101235e+16,
      "budget_used_percent": 30.22216231101235
    },
    {
      "type": "training",
      "description": "Training step 5087",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:57",
      "total_flops_so_far": 3.022810336906445e+16,
      "budget_used_percent": 30.228103369064446
    },
    {
      "type": "training",
      "description": "Training step 5088",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:57",
      "total_flops_so_far": 3.0234044427116544e+16,
      "budget_used_percent": 30.234044427116547
    },
    {
      "type": "training",
      "description": "Training step 5089",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:58",
      "total_flops_so_far": 3.023998548516864e+16,
      "budget_used_percent": 30.239985485168642
    },
    {
      "type": "training",
      "description": "Training step 5090",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:58",
      "total_flops_so_far": 3.0245926543220736e+16,
      "budget_used_percent": 30.245926543220737
    },
    {
      "type": "training",
      "description": "Training step 5091",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:59",
      "total_flops_so_far": 3.025186760127283e+16,
      "budget_used_percent": 30.25186760127283
    },
    {
      "type": "training",
      "description": "Training step 5092",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:12:59",
      "total_flops_so_far": 3.025780865932493e+16,
      "budget_used_percent": 30.25780865932493
    },
    {
      "type": "training",
      "description": "Training step 5093",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:00",
      "total_flops_so_far": 3.0263749717377024e+16,
      "budget_used_percent": 30.263749717377024
    },
    {
      "type": "training",
      "description": "Training step 5094",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:00",
      "total_flops_so_far": 3.026969077542912e+16,
      "budget_used_percent": 30.26969077542912
    },
    {
      "type": "training",
      "description": "Training step 5095",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:00",
      "total_flops_so_far": 3.0275631833481216e+16,
      "budget_used_percent": 30.275631833481214
    },
    {
      "type": "training",
      "description": "Training step 5096",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:01",
      "total_flops_so_far": 3.028157289153331e+16,
      "budget_used_percent": 30.281572891533308
    },
    {
      "type": "training",
      "description": "Training step 5097",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:01",
      "total_flops_so_far": 3.028751394958541e+16,
      "budget_used_percent": 30.28751394958541
    },
    {
      "type": "training",
      "description": "Training step 5098",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:02",
      "total_flops_so_far": 3.0293455007637504e+16,
      "budget_used_percent": 30.293455007637505
    },
    {
      "type": "training",
      "description": "Training step 5099",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:02",
      "total_flops_so_far": 3.02993960656896e+16,
      "budget_used_percent": 30.2993960656896
    },
    {
      "type": "training",
      "description": "Training step 5100",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:03",
      "total_flops_so_far": 3.0305337123741696e+16,
      "budget_used_percent": 30.305337123741694
    },
    {
      "type": "training",
      "description": "Training step 5101",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:03",
      "total_flops_so_far": 3.031127818179379e+16,
      "budget_used_percent": 30.311278181793792
    },
    {
      "type": "training",
      "description": "Training step 5102",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:03",
      "total_flops_so_far": 3.031721923984589e+16,
      "budget_used_percent": 30.317219239845887
    },
    {
      "type": "training",
      "description": "Training step 5103",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:04",
      "total_flops_so_far": 3.0323160297897984e+16,
      "budget_used_percent": 30.32316029789798
    },
    {
      "type": "training",
      "description": "Training step 5104",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:04",
      "total_flops_so_far": 3.032910135595008e+16,
      "budget_used_percent": 30.329101355950076
    },
    {
      "type": "training",
      "description": "Training step 5105",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:05",
      "total_flops_so_far": 3.0335042414002176e+16,
      "budget_used_percent": 30.335042414002178
    },
    {
      "type": "training",
      "description": "Training step 5106",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:05",
      "total_flops_so_far": 3.034098347205427e+16,
      "budget_used_percent": 30.340983472054273
    },
    {
      "type": "training",
      "description": "Training step 5107",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:06",
      "total_flops_so_far": 3.034692453010637e+16,
      "budget_used_percent": 30.346924530106367
    },
    {
      "type": "training",
      "description": "Training step 5108",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:06",
      "total_flops_so_far": 3.0352865588158464e+16,
      "budget_used_percent": 30.352865588158462
    },
    {
      "type": "training",
      "description": "Training step 5109",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:06",
      "total_flops_so_far": 3.035880664621056e+16,
      "budget_used_percent": 30.358806646210564
    },
    {
      "type": "training",
      "description": "Training step 5110",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:07",
      "total_flops_so_far": 3.0364747704262656e+16,
      "budget_used_percent": 30.36474770426266
    },
    {
      "type": "training",
      "description": "Training step 5111",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:07",
      "total_flops_so_far": 3.037068876231475e+16,
      "budget_used_percent": 30.370688762314753
    },
    {
      "type": "training",
      "description": "Training step 5112",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:08",
      "total_flops_so_far": 3.037662982036685e+16,
      "budget_used_percent": 30.376629820366848
    },
    {
      "type": "training",
      "description": "Training step 5113",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:08",
      "total_flops_so_far": 3.0382570878418944e+16,
      "budget_used_percent": 30.382570878418946
    },
    {
      "type": "training",
      "description": "Training step 5114",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:09",
      "total_flops_so_far": 3.038851193647104e+16,
      "budget_used_percent": 30.38851193647104
    },
    {
      "type": "training",
      "description": "Training step 5115",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:09",
      "total_flops_so_far": 3.0394452994523136e+16,
      "budget_used_percent": 30.394452994523135
    },
    {
      "type": "training",
      "description": "Training step 5116",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:09",
      "total_flops_so_far": 3.040039405257523e+16,
      "budget_used_percent": 30.40039405257523
    },
    {
      "type": "training",
      "description": "Training step 5117",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:10",
      "total_flops_so_far": 3.040633511062733e+16,
      "budget_used_percent": 30.40633511062733
    },
    {
      "type": "training",
      "description": "Training step 5118",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:10",
      "total_flops_so_far": 3.0412276168679424e+16,
      "budget_used_percent": 30.412276168679426
    },
    {
      "type": "training",
      "description": "Training step 5119",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:11",
      "total_flops_so_far": 3.041821722673152e+16,
      "budget_used_percent": 30.41821722673152
    },
    {
      "type": "training",
      "description": "Training step 5120",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:11",
      "total_flops_so_far": 3.0424158284783616e+16,
      "budget_used_percent": 30.424158284783616
    },
    {
      "type": "training",
      "description": "Training step 5121",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:12",
      "total_flops_so_far": 3.043009934283571e+16,
      "budget_used_percent": 30.430099342835714
    },
    {
      "type": "training",
      "description": "Training step 5122",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:12",
      "total_flops_so_far": 3.043604040088781e+16,
      "budget_used_percent": 30.43604040088781
    },
    {
      "type": "training",
      "description": "Training step 5123",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:13",
      "total_flops_so_far": 3.0441981458939904e+16,
      "budget_used_percent": 30.441981458939903
    },
    {
      "type": "training",
      "description": "Training step 5124",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:13",
      "total_flops_so_far": 3.0447922516992e+16,
      "budget_used_percent": 30.447922516991998
    },
    {
      "type": "training",
      "description": "Training step 5125",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:14",
      "total_flops_so_far": 3.0453863575044096e+16,
      "budget_used_percent": 30.453863575044092
    },
    {
      "type": "training",
      "description": "Training step 5126",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:14",
      "total_flops_so_far": 3.045980463309619e+16,
      "budget_used_percent": 30.459804633096194
    },
    {
      "type": "training",
      "description": "Training step 5127",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:14",
      "total_flops_so_far": 3.046574569114829e+16,
      "budget_used_percent": 30.46574569114829
    },
    {
      "type": "training",
      "description": "Training step 5128",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:15",
      "total_flops_so_far": 3.0471686749200384e+16,
      "budget_used_percent": 30.471686749200384
    },
    {
      "type": "training",
      "description": "Training step 5129",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:15",
      "total_flops_so_far": 3.047762780725248e+16,
      "budget_used_percent": 30.477627807252478
    },
    {
      "type": "training",
      "description": "Training step 5130",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:16",
      "total_flops_so_far": 3.0483568865304576e+16,
      "budget_used_percent": 30.483568865304576
    },
    {
      "type": "training",
      "description": "Training step 5131",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:16",
      "total_flops_so_far": 3.048950992335667e+16,
      "budget_used_percent": 30.48950992335667
    },
    {
      "type": "training",
      "description": "Training step 5132",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:17",
      "total_flops_so_far": 3.049545098140877e+16,
      "budget_used_percent": 30.495450981408766
    },
    {
      "type": "training",
      "description": "Training step 5133",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:17",
      "total_flops_so_far": 3.0501392039460864e+16,
      "budget_used_percent": 30.50139203946086
    },
    {
      "type": "training",
      "description": "Training step 5134",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:17",
      "total_flops_so_far": 3.050733309751296e+16,
      "budget_used_percent": 30.507333097512962
    },
    {
      "type": "training",
      "description": "Training step 5135",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:18",
      "total_flops_so_far": 3.0513274155565056e+16,
      "budget_used_percent": 30.513274155565057
    },
    {
      "type": "training",
      "description": "Training step 5136",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:18",
      "total_flops_so_far": 3.051921521361715e+16,
      "budget_used_percent": 30.51921521361715
    },
    {
      "type": "training",
      "description": "Training step 5137",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:19",
      "total_flops_so_far": 3.052515627166925e+16,
      "budget_used_percent": 30.525156271669246
    },
    {
      "type": "training",
      "description": "Training step 5138",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:19",
      "total_flops_so_far": 3.0531097329721344e+16,
      "budget_used_percent": 30.531097329721348
    },
    {
      "type": "training",
      "description": "Training step 5139",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:20",
      "total_flops_so_far": 3.053703838777344e+16,
      "budget_used_percent": 30.537038387773443
    },
    {
      "type": "training",
      "description": "Training step 5140",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:20",
      "total_flops_so_far": 3.0542979445825536e+16,
      "budget_used_percent": 30.542979445825537
    },
    {
      "type": "training",
      "description": "Training step 5141",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:20",
      "total_flops_so_far": 3.054892050387763e+16,
      "budget_used_percent": 30.548920503877632
    },
    {
      "type": "training",
      "description": "Training step 5142",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:21",
      "total_flops_so_far": 3.055486156192973e+16,
      "budget_used_percent": 30.55486156192973
    },
    {
      "type": "training",
      "description": "Training step 5143",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:21",
      "total_flops_so_far": 3.0560802619981824e+16,
      "budget_used_percent": 30.560802619981825
    },
    {
      "type": "training",
      "description": "Training step 5144",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:22",
      "total_flops_so_far": 3.056674367803392e+16,
      "budget_used_percent": 30.56674367803392
    },
    {
      "type": "training",
      "description": "Training step 5145",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:22",
      "total_flops_so_far": 3.0572684736086016e+16,
      "budget_used_percent": 30.572684736086014
    },
    {
      "type": "training",
      "description": "Training step 5146",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:23",
      "total_flops_so_far": 3.057862579413811e+16,
      "budget_used_percent": 30.578625794138116
    },
    {
      "type": "training",
      "description": "Training step 5147",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:23",
      "total_flops_so_far": 3.058456685219021e+16,
      "budget_used_percent": 30.58456685219021
    },
    {
      "type": "training",
      "description": "Training step 5148",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:23",
      "total_flops_so_far": 3.0590507910242304e+16,
      "budget_used_percent": 30.590507910242305
    },
    {
      "type": "training",
      "description": "Training step 5149",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:24",
      "total_flops_so_far": 3.05964489682944e+16,
      "budget_used_percent": 30.5964489682944
    },
    {
      "type": "training",
      "description": "Training step 5150",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:24",
      "total_flops_so_far": 3.0602390026346496e+16,
      "budget_used_percent": 30.602390026346498
    },
    {
      "type": "training",
      "description": "Training step 5151",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:25",
      "total_flops_so_far": 3.060833108439859e+16,
      "budget_used_percent": 30.608331084398593
    },
    {
      "type": "training",
      "description": "Training step 5152",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:25",
      "total_flops_so_far": 3.061427214245069e+16,
      "budget_used_percent": 30.614272142450687
    },
    {
      "type": "training",
      "description": "Training step 5153",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:26",
      "total_flops_so_far": 3.0620213200502784e+16,
      "budget_used_percent": 30.620213200502782
    },
    {
      "type": "training",
      "description": "Training step 5154",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:26",
      "total_flops_so_far": 3.062615425855488e+16,
      "budget_used_percent": 30.626154258554877
    },
    {
      "type": "training",
      "description": "Training step 5155",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:27",
      "total_flops_so_far": 3.0632095316606976e+16,
      "budget_used_percent": 30.63209531660698
    },
    {
      "type": "training",
      "description": "Training step 5156",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:27",
      "total_flops_so_far": 3.063803637465907e+16,
      "budget_used_percent": 30.638036374659073
    },
    {
      "type": "training",
      "description": "Training step 5157",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:28",
      "total_flops_so_far": 3.064397743271117e+16,
      "budget_used_percent": 30.643977432711168
    },
    {
      "type": "training",
      "description": "Training step 5158",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:28",
      "total_flops_so_far": 3.0649918490763264e+16,
      "budget_used_percent": 30.649918490763262
    },
    {
      "type": "training",
      "description": "Training step 5159",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:28",
      "total_flops_so_far": 3.065585954881536e+16,
      "budget_used_percent": 30.65585954881536
    },
    {
      "type": "training",
      "description": "Training step 5160",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:29",
      "total_flops_so_far": 3.0661800606867456e+16,
      "budget_used_percent": 30.661800606867455
    },
    {
      "type": "training",
      "description": "Training step 5161",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:29",
      "total_flops_so_far": 3.066774166491955e+16,
      "budget_used_percent": 30.66774166491955
    },
    {
      "type": "training",
      "description": "Training step 5162",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:30",
      "total_flops_so_far": 3.067368272297165e+16,
      "budget_used_percent": 30.673682722971645
    },
    {
      "type": "training",
      "description": "Training step 5163",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:30",
      "total_flops_so_far": 3.0679623781023744e+16,
      "budget_used_percent": 30.679623781023746
    },
    {
      "type": "training",
      "description": "Training step 5164",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:31",
      "total_flops_so_far": 3.068556483907584e+16,
      "budget_used_percent": 30.68556483907584
    },
    {
      "type": "training",
      "description": "Training step 5165",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:31",
      "total_flops_so_far": 3.0691505897127936e+16,
      "budget_used_percent": 30.691505897127936
    },
    {
      "type": "training",
      "description": "Training step 5166",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:31",
      "total_flops_so_far": 3.069744695518003e+16,
      "budget_used_percent": 30.69744695518003
    },
    {
      "type": "training",
      "description": "Training step 5167",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:32",
      "total_flops_so_far": 3.070338801323213e+16,
      "budget_used_percent": 30.70338801323213
    },
    {
      "type": "training",
      "description": "Training step 5168",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:32",
      "total_flops_so_far": 3.0709329071284224e+16,
      "budget_used_percent": 30.709329071284223
    },
    {
      "type": "training",
      "description": "Training step 5169",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:33",
      "total_flops_so_far": 3.071527012933632e+16,
      "budget_used_percent": 30.715270129336318
    },
    {
      "type": "training",
      "description": "Training step 5170",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:33",
      "total_flops_so_far": 3.0721211187388416e+16,
      "budget_used_percent": 30.721211187388413
    },
    {
      "type": "training",
      "description": "Training step 5171",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:34",
      "total_flops_so_far": 3.072715224544051e+16,
      "budget_used_percent": 30.727152245440514
    },
    {
      "type": "training",
      "description": "Training step 5172",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:34",
      "total_flops_so_far": 3.073309330349261e+16,
      "budget_used_percent": 30.73309330349261
    },
    {
      "type": "training",
      "description": "Training step 5173",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:34",
      "total_flops_so_far": 3.0739034361544704e+16,
      "budget_used_percent": 30.739034361544704
    },
    {
      "type": "training",
      "description": "Training step 5174",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:35",
      "total_flops_so_far": 3.07449754195968e+16,
      "budget_used_percent": 30.7449754195968
    },
    {
      "type": "training",
      "description": "Training step 5175",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:35",
      "total_flops_so_far": 3.0750916477648896e+16,
      "budget_used_percent": 30.7509164776489
    },
    {
      "type": "training",
      "description": "Training step 5176",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:36",
      "total_flops_so_far": 3.075685753570099e+16,
      "budget_used_percent": 30.756857535700995
    },
    {
      "type": "training",
      "description": "Training step 5177",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:36",
      "total_flops_so_far": 3.076279859375309e+16,
      "budget_used_percent": 30.76279859375309
    },
    {
      "type": "training",
      "description": "Training step 5178",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:37",
      "total_flops_so_far": 3.0768739651805184e+16,
      "budget_used_percent": 30.768739651805184
    },
    {
      "type": "training",
      "description": "Training step 5179",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:37",
      "total_flops_so_far": 3.077468070985728e+16,
      "budget_used_percent": 30.774680709857282
    },
    {
      "type": "training",
      "description": "Training step 5180",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:37",
      "total_flops_so_far": 3.0780621767909376e+16,
      "budget_used_percent": 30.780621767909377
    },
    {
      "type": "training",
      "description": "Training step 5181",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:38",
      "total_flops_so_far": 3.078656282596147e+16,
      "budget_used_percent": 30.78656282596147
    },
    {
      "type": "training",
      "description": "Training step 5182",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:38",
      "total_flops_so_far": 3.079250388401357e+16,
      "budget_used_percent": 30.792503884013566
    },
    {
      "type": "training",
      "description": "Training step 5183",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:39",
      "total_flops_so_far": 3.0798444942065664e+16,
      "budget_used_percent": 30.79844494206566
    },
    {
      "type": "training",
      "description": "Training step 5184",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:39",
      "total_flops_so_far": 3.080438600011776e+16,
      "budget_used_percent": 30.804386000117763
    },
    {
      "type": "training",
      "description": "Training step 5185",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:40",
      "total_flops_so_far": 3.0810327058169856e+16,
      "budget_used_percent": 30.810327058169857
    },
    {
      "type": "training",
      "description": "Training step 5186",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:40",
      "total_flops_so_far": 3.081626811622195e+16,
      "budget_used_percent": 30.816268116221952
    },
    {
      "type": "training",
      "description": "Training step 5187",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:41",
      "total_flops_so_far": 3.082220917427405e+16,
      "budget_used_percent": 30.822209174274047
    },
    {
      "type": "training",
      "description": "Training step 5188",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:41",
      "total_flops_so_far": 3.0828150232326144e+16,
      "budget_used_percent": 30.828150232326145
    },
    {
      "type": "training",
      "description": "Training step 5189",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:42",
      "total_flops_so_far": 3.083409129037824e+16,
      "budget_used_percent": 30.83409129037824
    },
    {
      "type": "training",
      "description": "Training step 5190",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:42",
      "total_flops_so_far": 3.0840032348430336e+16,
      "budget_used_percent": 30.840032348430334
    },
    {
      "type": "training",
      "description": "Training step 5191",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:42",
      "total_flops_so_far": 3.084597340648243e+16,
      "budget_used_percent": 30.84597340648243
    },
    {
      "type": "training",
      "description": "Training step 5192",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:43",
      "total_flops_so_far": 3.085191446453453e+16,
      "budget_used_percent": 30.85191446453453
    },
    {
      "type": "training",
      "description": "Training step 5193",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:43",
      "total_flops_so_far": 3.0857855522586624e+16,
      "budget_used_percent": 30.857855522586625
    },
    {
      "type": "training",
      "description": "Training step 5194",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:44",
      "total_flops_so_far": 3.086379658063872e+16,
      "budget_used_percent": 30.86379658063872
    },
    {
      "type": "training",
      "description": "Training step 5195",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:44",
      "total_flops_so_far": 3.0869737638690816e+16,
      "budget_used_percent": 30.869737638690815
    },
    {
      "type": "training",
      "description": "Training step 5196",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:45",
      "total_flops_so_far": 3.087567869674291e+16,
      "budget_used_percent": 30.875678696742913
    },
    {
      "type": "training",
      "description": "Training step 5197",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:45",
      "total_flops_so_far": 3.088161975479501e+16,
      "budget_used_percent": 30.881619754795008
    },
    {
      "type": "training",
      "description": "Training step 5198",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:45",
      "total_flops_so_far": 3.0887560812847104e+16,
      "budget_used_percent": 30.887560812847102
    },
    {
      "type": "training",
      "description": "Training step 5199",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:46",
      "total_flops_so_far": 3.08935018708992e+16,
      "budget_used_percent": 30.893501870899197
    },
    {
      "type": "training",
      "description": "Training step 5200",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:46",
      "total_flops_so_far": 3.0899442928951296e+16,
      "budget_used_percent": 30.8994429289513
    },
    {
      "type": "training",
      "description": "Training step 5201",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:47",
      "total_flops_so_far": 3.090538398700339e+16,
      "budget_used_percent": 30.905383987003393
    },
    {
      "type": "training",
      "description": "Training step 5202",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:47",
      "total_flops_so_far": 3.091132504505549e+16,
      "budget_used_percent": 30.911325045055488
    },
    {
      "type": "training",
      "description": "Training step 5203",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:48",
      "total_flops_so_far": 3.0917266103107584e+16,
      "budget_used_percent": 30.917266103107583
    },
    {
      "type": "training",
      "description": "Training step 5204",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:48",
      "total_flops_so_far": 3.092320716115968e+16,
      "budget_used_percent": 30.92320716115968
    },
    {
      "type": "training",
      "description": "Training step 5205",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:48",
      "total_flops_so_far": 3.0929148219211776e+16,
      "budget_used_percent": 30.929148219211775
    },
    {
      "type": "training",
      "description": "Training step 5206",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:49",
      "total_flops_so_far": 3.093508927726387e+16,
      "budget_used_percent": 30.93508927726387
    },
    {
      "type": "training",
      "description": "Training step 5207",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:49",
      "total_flops_so_far": 3.094103033531597e+16,
      "budget_used_percent": 30.941030335315965
    },
    {
      "type": "training",
      "description": "Training step 5208",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:50",
      "total_flops_so_far": 3.0946971393368064e+16,
      "budget_used_percent": 30.94697139336806
    },
    {
      "type": "training",
      "description": "Training step 5209",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:50",
      "total_flops_so_far": 3.095291245142016e+16,
      "budget_used_percent": 30.95291245142016
    },
    {
      "type": "training",
      "description": "Training step 5210",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:51",
      "total_flops_so_far": 3.0958853509472256e+16,
      "budget_used_percent": 30.958853509472256
    },
    {
      "type": "training",
      "description": "Training step 5211",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:51",
      "total_flops_so_far": 3.096479456752435e+16,
      "budget_used_percent": 30.96479456752435
    },
    {
      "type": "training",
      "description": "Training step 5212",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:51",
      "total_flops_so_far": 3.097073562557645e+16,
      "budget_used_percent": 30.970735625576445
    },
    {
      "type": "training",
      "description": "Training step 5213",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:52",
      "total_flops_so_far": 3.0976676683628544e+16,
      "budget_used_percent": 30.976676683628547
    },
    {
      "type": "training",
      "description": "Training step 5214",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:53",
      "total_flops_so_far": 3.098261774168064e+16,
      "budget_used_percent": 30.98261774168064
    },
    {
      "type": "training",
      "description": "Training step 5215",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:53",
      "total_flops_so_far": 3.0988558799732736e+16,
      "budget_used_percent": 30.988558799732736
    },
    {
      "type": "training",
      "description": "Training step 5216",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:53",
      "total_flops_so_far": 3.099449985778483e+16,
      "budget_used_percent": 30.99449985778483
    },
    {
      "type": "training",
      "description": "Training step 5217",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:54",
      "total_flops_so_far": 3.100044091583693e+16,
      "budget_used_percent": 31.00044091583693
    },
    {
      "type": "training",
      "description": "Training step 5218",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:54",
      "total_flops_so_far": 3.1006381973889024e+16,
      "budget_used_percent": 31.006381973889024
    },
    {
      "type": "training",
      "description": "Training step 5219",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:55",
      "total_flops_so_far": 3.101232303194112e+16,
      "budget_used_percent": 31.01232303194112
    },
    {
      "type": "training",
      "description": "Training step 5220",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:55",
      "total_flops_so_far": 3.1018264089993216e+16,
      "budget_used_percent": 31.018264089993213
    },
    {
      "type": "training",
      "description": "Training step 5221",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:56",
      "total_flops_so_far": 3.102420514804531e+16,
      "budget_used_percent": 31.024205148045315
    },
    {
      "type": "training",
      "description": "Training step 5222",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:56",
      "total_flops_so_far": 3.103014620609741e+16,
      "budget_used_percent": 31.03014620609741
    },
    {
      "type": "training",
      "description": "Training step 5223",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:56",
      "total_flops_so_far": 3.1036087264149504e+16,
      "budget_used_percent": 31.036087264149504
    },
    {
      "type": "training",
      "description": "Training step 5224",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:57",
      "total_flops_so_far": 3.10420283222016e+16,
      "budget_used_percent": 31.0420283222016
    },
    {
      "type": "training",
      "description": "Training step 5225",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:57",
      "total_flops_so_far": 3.1047969380253696e+16,
      "budget_used_percent": 31.047969380253697
    },
    {
      "type": "training",
      "description": "Training step 5226",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:58",
      "total_flops_so_far": 3.105391043830579e+16,
      "budget_used_percent": 31.053910438305792
    },
    {
      "type": "training",
      "description": "Training step 5227",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:58",
      "total_flops_so_far": 3.105985149635789e+16,
      "budget_used_percent": 31.059851496357886
    },
    {
      "type": "training",
      "description": "Training step 5228",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:59",
      "total_flops_so_far": 3.1065792554409984e+16,
      "budget_used_percent": 31.06579255440998
    },
    {
      "type": "training",
      "description": "Training step 5229",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:59",
      "total_flops_so_far": 3.107173361246208e+16,
      "budget_used_percent": 31.071733612462083
    },
    {
      "type": "training",
      "description": "Training step 5230",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:13:59",
      "total_flops_so_far": 3.1077674670514176e+16,
      "budget_used_percent": 31.077674670514178
    },
    {
      "type": "training",
      "description": "Training step 5231",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:00",
      "total_flops_so_far": 3.108361572856627e+16,
      "budget_used_percent": 31.083615728566272
    },
    {
      "type": "training",
      "description": "Training step 5232",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:00",
      "total_flops_so_far": 3.108955678661837e+16,
      "budget_used_percent": 31.089556786618367
    },
    {
      "type": "training",
      "description": "Training step 5233",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:01",
      "total_flops_so_far": 3.1095497844670464e+16,
      "budget_used_percent": 31.095497844670465
    },
    {
      "type": "training",
      "description": "Training step 5234",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:01",
      "total_flops_so_far": 3.110143890272256e+16,
      "budget_used_percent": 31.10143890272256
    },
    {
      "type": "training",
      "description": "Training step 5235",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:02",
      "total_flops_so_far": 3.1107379960774656e+16,
      "budget_used_percent": 31.107379960774654
    },
    {
      "type": "training",
      "description": "Training step 5236",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:02",
      "total_flops_so_far": 3.111332101882675e+16,
      "budget_used_percent": 31.11332101882675
    },
    {
      "type": "training",
      "description": "Training step 5237",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:02",
      "total_flops_so_far": 3.111926207687885e+16,
      "budget_used_percent": 31.119262076878844
    },
    {
      "type": "training",
      "description": "Training step 5238",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:03",
      "total_flops_so_far": 3.1125203134930944e+16,
      "budget_used_percent": 31.125203134930945
    },
    {
      "type": "training",
      "description": "Training step 5239",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:03",
      "total_flops_so_far": 3.113114419298304e+16,
      "budget_used_percent": 31.13114419298304
    },
    {
      "type": "training",
      "description": "Training step 5240",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:04",
      "total_flops_so_far": 3.1137085251035136e+16,
      "budget_used_percent": 31.137085251035135
    },
    {
      "type": "training",
      "description": "Training step 5241",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:04",
      "total_flops_so_far": 3.114302630908723e+16,
      "budget_used_percent": 31.14302630908723
    },
    {
      "type": "training",
      "description": "Training step 5242",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:05",
      "total_flops_so_far": 3.114896736713933e+16,
      "budget_used_percent": 31.14896736713933
    },
    {
      "type": "training",
      "description": "Training step 5243",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:05",
      "total_flops_so_far": 3.1154908425191424e+16,
      "budget_used_percent": 31.154908425191426
    },
    {
      "type": "training",
      "description": "Training step 5244",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:05",
      "total_flops_so_far": 3.116084948324352e+16,
      "budget_used_percent": 31.16084948324352
    },
    {
      "type": "training",
      "description": "Training step 5245",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:06",
      "total_flops_so_far": 3.1166790541295616e+16,
      "budget_used_percent": 31.166790541295615
    },
    {
      "type": "training",
      "description": "Training step 5246",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:06",
      "total_flops_so_far": 3.117273159934771e+16,
      "budget_used_percent": 31.172731599347713
    },
    {
      "type": "training",
      "description": "Training step 5247",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:07",
      "total_flops_so_far": 3.117867265739981e+16,
      "budget_used_percent": 31.178672657399808
    },
    {
      "type": "training",
      "description": "Training step 5248",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:07",
      "total_flops_so_far": 3.1184613715451904e+16,
      "budget_used_percent": 31.184613715451903
    },
    {
      "type": "training",
      "description": "Training step 5249",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:08",
      "total_flops_so_far": 3.1190554773504e+16,
      "budget_used_percent": 31.190554773503997
    },
    {
      "type": "training",
      "description": "Training step 5250",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:08",
      "total_flops_so_far": 3.1196495831556096e+16,
      "budget_used_percent": 31.1964958315561
    },
    {
      "type": "training",
      "description": "Training step 5251",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:09",
      "total_flops_so_far": 3.120243688960819e+16,
      "budget_used_percent": 31.202436889608194
    },
    {
      "type": "training",
      "description": "Training step 5252",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:09",
      "total_flops_so_far": 3.120837794766029e+16,
      "budget_used_percent": 31.20837794766029
    },
    {
      "type": "training",
      "description": "Training step 5253",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:10",
      "total_flops_so_far": 3.1214319005712384e+16,
      "budget_used_percent": 31.214319005712383
    },
    {
      "type": "training",
      "description": "Training step 5254",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:10",
      "total_flops_so_far": 3.122026006376448e+16,
      "budget_used_percent": 31.22026006376448
    },
    {
      "type": "training",
      "description": "Training step 5255",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:10",
      "total_flops_so_far": 3.1226201121816576e+16,
      "budget_used_percent": 31.226201121816576
    },
    {
      "type": "training",
      "description": "Training step 5256",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:11",
      "total_flops_so_far": 3.123214217986867e+16,
      "budget_used_percent": 31.23214217986867
    },
    {
      "type": "training",
      "description": "Training step 5257",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:11",
      "total_flops_so_far": 3.123808323792077e+16,
      "budget_used_percent": 31.238083237920765
    },
    {
      "type": "training",
      "description": "Training step 5258",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:12",
      "total_flops_so_far": 3.1244024295972864e+16,
      "budget_used_percent": 31.244024295972867
    },
    {
      "type": "training",
      "description": "Training step 5259",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:12",
      "total_flops_so_far": 3.124996535402496e+16,
      "budget_used_percent": 31.249965354024962
    },
    {
      "type": "training",
      "description": "Training step 5260",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:13",
      "total_flops_so_far": 3.1255906412077056e+16,
      "budget_used_percent": 31.255906412077056
    },
    {
      "type": "training",
      "description": "Training step 5261",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:13",
      "total_flops_so_far": 3.126184747012915e+16,
      "budget_used_percent": 31.26184747012915
    },
    {
      "type": "training",
      "description": "Training step 5262",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:13",
      "total_flops_so_far": 3.126778852818125e+16,
      "budget_used_percent": 31.26778852818125
    },
    {
      "type": "training",
      "description": "Training step 5263",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:14",
      "total_flops_so_far": 3.1273729586233344e+16,
      "budget_used_percent": 31.273729586233344
    },
    {
      "type": "training",
      "description": "Training step 5264",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:14",
      "total_flops_so_far": 3.127967064428544e+16,
      "budget_used_percent": 31.27967064428544
    },
    {
      "type": "training",
      "description": "Training step 5265",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:15",
      "total_flops_so_far": 3.1285611702337536e+16,
      "budget_used_percent": 31.285611702337533
    },
    {
      "type": "training",
      "description": "Training step 5266",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:15",
      "total_flops_so_far": 3.129155276038963e+16,
      "budget_used_percent": 31.291552760389628
    },
    {
      "type": "training",
      "description": "Training step 5267",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:15",
      "total_flops_so_far": 3.129749381844173e+16,
      "budget_used_percent": 31.29749381844173
    },
    {
      "type": "training",
      "description": "Training step 5268",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:16",
      "total_flops_so_far": 3.1303434876493824e+16,
      "budget_used_percent": 31.303434876493824
    },
    {
      "type": "training",
      "description": "Training step 5269",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:16",
      "total_flops_so_far": 3.130937593454592e+16,
      "budget_used_percent": 31.30937593454592
    },
    {
      "type": "training",
      "description": "Training step 5270",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:17",
      "total_flops_so_far": 3.1315316992598016e+16,
      "budget_used_percent": 31.315316992598014
    },
    {
      "type": "training",
      "description": "Training step 5271",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:17",
      "total_flops_so_far": 3.132125805065011e+16,
      "budget_used_percent": 31.321258050650115
    },
    {
      "type": "training",
      "description": "Training step 5272",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:18",
      "total_flops_so_far": 3.132719910870221e+16,
      "budget_used_percent": 31.32719910870221
    },
    {
      "type": "training",
      "description": "Training step 5273",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:18",
      "total_flops_so_far": 3.1333140166754304e+16,
      "budget_used_percent": 31.333140166754305
    },
    {
      "type": "training",
      "description": "Training step 5274",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:19",
      "total_flops_so_far": 3.13390812248064e+16,
      "budget_used_percent": 31.3390812248064
    },
    {
      "type": "training",
      "description": "Training step 5275",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:19",
      "total_flops_so_far": 3.1345022282858496e+16,
      "budget_used_percent": 31.345022282858498
    },
    {
      "type": "training",
      "description": "Training step 5276",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:19",
      "total_flops_so_far": 3.135096334091059e+16,
      "budget_used_percent": 31.350963340910592
    },
    {
      "type": "training",
      "description": "Training step 5277",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:20",
      "total_flops_so_far": 3.135690439896269e+16,
      "budget_used_percent": 31.356904398962687
    },
    {
      "type": "training",
      "description": "Training step 5278",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:20",
      "total_flops_so_far": 3.1362845457014784e+16,
      "budget_used_percent": 31.36284545701478
    },
    {
      "type": "training",
      "description": "Training step 5279",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:21",
      "total_flops_so_far": 3.136878651506688e+16,
      "budget_used_percent": 31.368786515066883
    },
    {
      "type": "training",
      "description": "Training step 5280",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:21",
      "total_flops_so_far": 3.1374727573118976e+16,
      "budget_used_percent": 31.374727573118978
    },
    {
      "type": "training",
      "description": "Training step 5281",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:22",
      "total_flops_so_far": 3.138066863117107e+16,
      "budget_used_percent": 31.380668631171073
    },
    {
      "type": "training",
      "description": "Training step 5282",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:22",
      "total_flops_so_far": 3.138660968922317e+16,
      "budget_used_percent": 31.386609689223167
    },
    {
      "type": "training",
      "description": "Training step 5283",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:23",
      "total_flops_so_far": 3.1392550747275264e+16,
      "budget_used_percent": 31.392550747275266
    },
    {
      "type": "training",
      "description": "Training step 5284",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:23",
      "total_flops_so_far": 3.139849180532736e+16,
      "budget_used_percent": 31.39849180532736
    },
    {
      "type": "training",
      "description": "Training step 5285",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:24",
      "total_flops_so_far": 3.1404432863379456e+16,
      "budget_used_percent": 31.404432863379455
    },
    {
      "type": "training",
      "description": "Training step 5286",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:24",
      "total_flops_so_far": 3.141037392143155e+16,
      "budget_used_percent": 31.41037392143155
    },
    {
      "type": "training",
      "description": "Training step 5287",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:24",
      "total_flops_so_far": 3.141631497948365e+16,
      "budget_used_percent": 31.41631497948365
    },
    {
      "type": "training",
      "description": "Training step 5288",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:25",
      "total_flops_so_far": 3.1422256037535744e+16,
      "budget_used_percent": 31.422256037535746
    },
    {
      "type": "training",
      "description": "Training step 5289",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:25",
      "total_flops_so_far": 3.142819709558784e+16,
      "budget_used_percent": 31.42819709558784
    },
    {
      "type": "training",
      "description": "Training step 5290",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:26",
      "total_flops_so_far": 3.1434138153639936e+16,
      "budget_used_percent": 31.434138153639935
    },
    {
      "type": "training",
      "description": "Training step 5291",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:26",
      "total_flops_so_far": 3.144007921169203e+16,
      "budget_used_percent": 31.440079211692034
    },
    {
      "type": "training",
      "description": "Training step 5292",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:27",
      "total_flops_so_far": 3.144602026974413e+16,
      "budget_used_percent": 31.44602026974413
    },
    {
      "type": "training",
      "description": "Training step 5293",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:27",
      "total_flops_so_far": 3.1451961327796224e+16,
      "budget_used_percent": 31.451961327796223
    },
    {
      "type": "training",
      "description": "Training step 5294",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:27",
      "total_flops_so_far": 3.145790238584832e+16,
      "budget_used_percent": 31.457902385848318
    },
    {
      "type": "training",
      "description": "Training step 5295",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:28",
      "total_flops_so_far": 3.1463843443900416e+16,
      "budget_used_percent": 31.463843443900412
    },
    {
      "type": "training",
      "description": "Training step 5296",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:28",
      "total_flops_so_far": 3.146978450195251e+16,
      "budget_used_percent": 31.469784501952514
    },
    {
      "type": "training",
      "description": "Training step 5297",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:29",
      "total_flops_so_far": 3.147572556000461e+16,
      "budget_used_percent": 31.47572556000461
    },
    {
      "type": "training",
      "description": "Training step 5298",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:29",
      "total_flops_so_far": 3.1481666618056704e+16,
      "budget_used_percent": 31.481666618056703
    },
    {
      "type": "training",
      "description": "Training step 5299",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:30",
      "total_flops_so_far": 3.14876076761088e+16,
      "budget_used_percent": 31.487607676108798
    },
    {
      "type": "training",
      "description": "Training step 5300",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:30",
      "total_flops_so_far": 3.1493548734160896e+16,
      "budget_used_percent": 31.493548734160896
    },
    {
      "type": "training",
      "description": "Training step 5301",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:30",
      "total_flops_so_far": 3.149948979221299e+16,
      "budget_used_percent": 31.49948979221299
    },
    {
      "type": "training",
      "description": "Training step 5302",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:31",
      "total_flops_so_far": 3.150543085026509e+16,
      "budget_used_percent": 31.505430850265085
    },
    {
      "type": "training",
      "description": "Training step 5303",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:31",
      "total_flops_so_far": 3.1511371908317184e+16,
      "budget_used_percent": 31.51137190831718
    },
    {
      "type": "training",
      "description": "Training step 5304",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:32",
      "total_flops_so_far": 3.151731296636928e+16,
      "budget_used_percent": 31.517312966369282
    },
    {
      "type": "training",
      "description": "Training step 5305",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:32",
      "total_flops_so_far": 3.1523254024421376e+16,
      "budget_used_percent": 31.523254024421377
    },
    {
      "type": "training",
      "description": "Training step 5306",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:33",
      "total_flops_so_far": 3.152919508247347e+16,
      "budget_used_percent": 31.52919508247347
    },
    {
      "type": "training",
      "description": "Training step 5307",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:33",
      "total_flops_so_far": 3.153513614052557e+16,
      "budget_used_percent": 31.535136140525566
    },
    {
      "type": "training",
      "description": "Training step 5308",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:34",
      "total_flops_so_far": 3.1541077198577664e+16,
      "budget_used_percent": 31.541077198577668
    },
    {
      "type": "training",
      "description": "Training step 5309",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:34",
      "total_flops_so_far": 3.154701825662976e+16,
      "budget_used_percent": 31.547018256629762
    },
    {
      "type": "training",
      "description": "Training step 5310",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:34",
      "total_flops_so_far": 3.1552959314681856e+16,
      "budget_used_percent": 31.552959314681857
    },
    {
      "type": "training",
      "description": "Training step 5311",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:35",
      "total_flops_so_far": 3.155890037273395e+16,
      "budget_used_percent": 31.55890037273395
    },
    {
      "type": "training",
      "description": "Training step 5312",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:36",
      "total_flops_so_far": 3.156484143078605e+16,
      "budget_used_percent": 31.56484143078605
    },
    {
      "type": "training",
      "description": "Training step 5313",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:36",
      "total_flops_so_far": 3.1570782488838144e+16,
      "budget_used_percent": 31.570782488838145
    },
    {
      "type": "training",
      "description": "Training step 5314",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:36",
      "total_flops_so_far": 3.157672354689024e+16,
      "budget_used_percent": 31.57672354689024
    },
    {
      "type": "training",
      "description": "Training step 5315",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:37",
      "total_flops_so_far": 3.1582664604942336e+16,
      "budget_used_percent": 31.582664604942334
    },
    {
      "type": "training",
      "description": "Training step 5316",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:37",
      "total_flops_so_far": 3.158860566299443e+16,
      "budget_used_percent": 31.588605662994436
    },
    {
      "type": "training",
      "description": "Training step 5317",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:38",
      "total_flops_so_far": 3.159454672104653e+16,
      "budget_used_percent": 31.59454672104653
    },
    {
      "type": "training",
      "description": "Training step 5318",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:38",
      "total_flops_so_far": 3.1600487779098624e+16,
      "budget_used_percent": 31.600487779098625
    },
    {
      "type": "training",
      "description": "Training step 5319",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:39",
      "total_flops_so_far": 3.160642883715072e+16,
      "budget_used_percent": 31.60642883715072
    },
    {
      "type": "training",
      "description": "Training step 5320",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:39",
      "total_flops_so_far": 3.1612369895202816e+16,
      "budget_used_percent": 31.612369895202818
    },
    {
      "type": "training",
      "description": "Training step 5321",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:39",
      "total_flops_so_far": 3.161831095325491e+16,
      "budget_used_percent": 31.618310953254912
    },
    {
      "type": "training",
      "description": "Training step 5322",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:40",
      "total_flops_so_far": 3.162425201130701e+16,
      "budget_used_percent": 31.624252011307007
    },
    {
      "type": "training",
      "description": "Training step 5323",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:40",
      "total_flops_so_far": 3.1630193069359104e+16,
      "budget_used_percent": 31.630193069359102
    },
    {
      "type": "training",
      "description": "Training step 5324",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:41",
      "total_flops_so_far": 3.16361341274112e+16,
      "budget_used_percent": 31.636134127411196
    },
    {
      "type": "training",
      "description": "Training step 5325",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:41",
      "total_flops_so_far": 3.1642075185463296e+16,
      "budget_used_percent": 31.6420751854633
    },
    {
      "type": "training",
      "description": "Training step 5326",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:42",
      "total_flops_so_far": 3.164801624351539e+16,
      "budget_used_percent": 31.648016243515393
    },
    {
      "type": "training",
      "description": "Training step 5327",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:42",
      "total_flops_so_far": 3.165395730156749e+16,
      "budget_used_percent": 31.653957301567488
    },
    {
      "type": "training",
      "description": "Training step 5328",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:43",
      "total_flops_so_far": 3.1659898359619584e+16,
      "budget_used_percent": 31.659898359619582
    },
    {
      "type": "training",
      "description": "Training step 5329",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:43",
      "total_flops_so_far": 3.166583941767168e+16,
      "budget_used_percent": 31.66583941767168
    },
    {
      "type": "training",
      "description": "Training step 5330",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:43",
      "total_flops_so_far": 3.1671780475723776e+16,
      "budget_used_percent": 31.671780475723775
    },
    {
      "type": "training",
      "description": "Training step 5331",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:44",
      "total_flops_so_far": 3.167772153377587e+16,
      "budget_used_percent": 31.67772153377587
    },
    {
      "type": "training",
      "description": "Training step 5332",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:44",
      "total_flops_so_far": 3.168366259182797e+16,
      "budget_used_percent": 31.683662591827964
    },
    {
      "type": "training",
      "description": "Training step 5333",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:45",
      "total_flops_so_far": 3.1689603649880064e+16,
      "budget_used_percent": 31.689603649880066
    },
    {
      "type": "training",
      "description": "Training step 5334",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:45",
      "total_flops_so_far": 3.169554470793216e+16,
      "budget_used_percent": 31.69554470793216
    },
    {
      "type": "training",
      "description": "Training step 5335",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:46",
      "total_flops_so_far": 3.1701485765984256e+16,
      "budget_used_percent": 31.701485765984255
    },
    {
      "type": "training",
      "description": "Training step 5336",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:46",
      "total_flops_so_far": 3.170742682403635e+16,
      "budget_used_percent": 31.70742682403635
    },
    {
      "type": "training",
      "description": "Training step 5337",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:46",
      "total_flops_so_far": 3.171336788208845e+16,
      "budget_used_percent": 31.71336788208845
    },
    {
      "type": "training",
      "description": "Training step 5338",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:47",
      "total_flops_so_far": 3.1719308940140544e+16,
      "budget_used_percent": 31.719308940140543
    },
    {
      "type": "training",
      "description": "Training step 5339",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:48",
      "total_flops_so_far": 3.172524999819264e+16,
      "budget_used_percent": 31.725249998192638
    },
    {
      "type": "training",
      "description": "Training step 5340",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:48",
      "total_flops_so_far": 3.1731191056244736e+16,
      "budget_used_percent": 31.731191056244732
    },
    {
      "type": "training",
      "description": "Training step 5341",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:48",
      "total_flops_so_far": 3.173713211429683e+16,
      "budget_used_percent": 31.737132114296834
    },
    {
      "type": "training",
      "description": "Training step 5342",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:49",
      "total_flops_so_far": 3.174307317234893e+16,
      "budget_used_percent": 31.74307317234893
    },
    {
      "type": "training",
      "description": "Training step 5343",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:49",
      "total_flops_so_far": 3.1749014230401024e+16,
      "budget_used_percent": 31.749014230401023
    },
    {
      "type": "training",
      "description": "Training step 5344",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:50",
      "total_flops_so_far": 3.175495528845312e+16,
      "budget_used_percent": 31.754955288453118
    },
    {
      "type": "training",
      "description": "Training step 5345",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:50",
      "total_flops_so_far": 3.1760896346505216e+16,
      "budget_used_percent": 31.76089634650522
    },
    {
      "type": "training",
      "description": "Training step 5346",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:51",
      "total_flops_so_far": 3.176683740455731e+16,
      "budget_used_percent": 31.766837404557315
    },
    {
      "type": "training",
      "description": "Training step 5347",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:51",
      "total_flops_so_far": 3.177277846260941e+16,
      "budget_used_percent": 31.77277846260941
    },
    {
      "type": "training",
      "description": "Training step 5348",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:51",
      "total_flops_so_far": 3.1778719520661504e+16,
      "budget_used_percent": 31.778719520661504
    },
    {
      "type": "training",
      "description": "Training step 5349",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:52",
      "total_flops_so_far": 3.17846605787136e+16,
      "budget_used_percent": 31.784660578713602
    },
    {
      "type": "training",
      "description": "Training step 5350",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:52",
      "total_flops_so_far": 3.1790601636765696e+16,
      "budget_used_percent": 31.790601636765697
    },
    {
      "type": "training",
      "description": "Training step 5351",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:53",
      "total_flops_so_far": 3.179654269481779e+16,
      "budget_used_percent": 31.79654269481779
    },
    {
      "type": "training",
      "description": "Training step 5352",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:53",
      "total_flops_so_far": 3.180248375286989e+16,
      "budget_used_percent": 31.802483752869886
    },
    {
      "type": "training",
      "description": "Training step 5353",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:54",
      "total_flops_so_far": 3.1808424810921984e+16,
      "budget_used_percent": 31.80842481092198
    },
    {
      "type": "training",
      "description": "Training step 5354",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:54",
      "total_flops_so_far": 3.181436586897408e+16,
      "budget_used_percent": 31.814365868974082
    },
    {
      "type": "training",
      "description": "Training step 5355",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:55",
      "total_flops_so_far": 3.1820306927026176e+16,
      "budget_used_percent": 31.820306927026177
    },
    {
      "type": "training",
      "description": "Training step 5356",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:55",
      "total_flops_so_far": 3.182624798507827e+16,
      "budget_used_percent": 31.826247985078272
    },
    {
      "type": "training",
      "description": "Training step 5357",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:55",
      "total_flops_so_far": 3.183218904313037e+16,
      "budget_used_percent": 31.832189043130366
    },
    {
      "type": "training",
      "description": "Training step 5358",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:56",
      "total_flops_so_far": 3.1838130101182464e+16,
      "budget_used_percent": 31.838130101182465
    },
    {
      "type": "training",
      "description": "Training step 5359",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:56",
      "total_flops_so_far": 3.184407115923456e+16,
      "budget_used_percent": 31.84407115923456
    },
    {
      "type": "training",
      "description": "Training step 5360",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:57",
      "total_flops_so_far": 3.1850012217286656e+16,
      "budget_used_percent": 31.850012217286654
    },
    {
      "type": "training",
      "description": "Training step 5361",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:57",
      "total_flops_so_far": 3.185595327533875e+16,
      "budget_used_percent": 31.85595327533875
    },
    {
      "type": "training",
      "description": "Training step 5362",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:58",
      "total_flops_so_far": 3.186189433339085e+16,
      "budget_used_percent": 31.86189433339085
    },
    {
      "type": "training",
      "description": "Training step 5363",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:58",
      "total_flops_so_far": 3.1867835391442944e+16,
      "budget_used_percent": 31.867835391442945
    },
    {
      "type": "training",
      "description": "Training step 5364",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:58",
      "total_flops_so_far": 3.187377644949504e+16,
      "budget_used_percent": 31.87377644949504
    },
    {
      "type": "training",
      "description": "Training step 5365",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:59",
      "total_flops_so_far": 3.1879717507547136e+16,
      "budget_used_percent": 31.879717507547134
    },
    {
      "type": "training",
      "description": "Training step 5366",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:14:59",
      "total_flops_so_far": 3.188565856559923e+16,
      "budget_used_percent": 31.885658565599233
    },
    {
      "type": "training",
      "description": "Training step 5367",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:00",
      "total_flops_so_far": 3.189159962365133e+16,
      "budget_used_percent": 31.891599623651327
    },
    {
      "type": "training",
      "description": "Training step 5368",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:00",
      "total_flops_so_far": 3.1897540681703424e+16,
      "budget_used_percent": 31.897540681703422
    },
    {
      "type": "training",
      "description": "Training step 5369",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:01",
      "total_flops_so_far": 3.190348173975552e+16,
      "budget_used_percent": 31.903481739755517
    },
    {
      "type": "training",
      "description": "Training step 5370",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:01",
      "total_flops_so_far": 3.1909422797807616e+16,
      "budget_used_percent": 31.90942279780762
    },
    {
      "type": "training",
      "description": "Training step 5371",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:02",
      "total_flops_so_far": 3.191536385585971e+16,
      "budget_used_percent": 31.915363855859713
    },
    {
      "type": "training",
      "description": "Training step 5372",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:02",
      "total_flops_so_far": 3.192130491391181e+16,
      "budget_used_percent": 31.921304913911808
    },
    {
      "type": "training",
      "description": "Training step 5373",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:03",
      "total_flops_so_far": 3.1927245971963904e+16,
      "budget_used_percent": 31.927245971963902
    },
    {
      "type": "training",
      "description": "Training step 5374",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:03",
      "total_flops_so_far": 3.1933187030016e+16,
      "budget_used_percent": 31.933187030016004
    },
    {
      "type": "training",
      "description": "Training step 5375",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:03",
      "total_flops_so_far": 3.1939128088068096e+16,
      "budget_used_percent": 31.9391280880681
    },
    {
      "type": "training",
      "description": "Training step 5376",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:04",
      "total_flops_so_far": 3.194506914612019e+16,
      "budget_used_percent": 31.945069146120193
    },
    {
      "type": "training",
      "description": "Training step 5377",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:04",
      "total_flops_so_far": 3.195101020417229e+16,
      "budget_used_percent": 31.951010204172288
    },
    {
      "type": "training",
      "description": "Training step 5378",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:05",
      "total_flops_so_far": 3.1956951262224384e+16,
      "budget_used_percent": 31.956951262224386
    },
    {
      "type": "training",
      "description": "Training step 5379",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:05",
      "total_flops_so_far": 3.196289232027648e+16,
      "budget_used_percent": 31.96289232027648
    },
    {
      "type": "training",
      "description": "Training step 5380",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:06",
      "total_flops_so_far": 3.1968833378328576e+16,
      "budget_used_percent": 31.968833378328576
    },
    {
      "type": "training",
      "description": "Training step 5381",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:06",
      "total_flops_so_far": 3.197477443638067e+16,
      "budget_used_percent": 31.97477443638067
    },
    {
      "type": "training",
      "description": "Training step 5382",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:07",
      "total_flops_so_far": 3.198071549443277e+16,
      "budget_used_percent": 31.980715494432765
    },
    {
      "type": "training",
      "description": "Training step 5383",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:07",
      "total_flops_so_far": 3.1986656552484864e+16,
      "budget_used_percent": 31.986656552484867
    },
    {
      "type": "training",
      "description": "Training step 5384",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:07",
      "total_flops_so_far": 3.199259761053696e+16,
      "budget_used_percent": 31.99259761053696
    },
    {
      "type": "training",
      "description": "Training step 5385",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:08",
      "total_flops_so_far": 3.1998538668589056e+16,
      "budget_used_percent": 31.998538668589056
    },
    {
      "type": "training",
      "description": "Training step 5386",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:08",
      "total_flops_so_far": 3.200447972664115e+16,
      "budget_used_percent": 32.00447972664115
    },
    {
      "type": "training",
      "description": "Training step 5387",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:09",
      "total_flops_so_far": 3.201042078469325e+16,
      "budget_used_percent": 32.01042078469325
    },
    {
      "type": "training",
      "description": "Training step 5388",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:09",
      "total_flops_so_far": 3.2016361842745344e+16,
      "budget_used_percent": 32.01636184274535
    },
    {
      "type": "training",
      "description": "Training step 5389",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:10",
      "total_flops_so_far": 3.202230290079744e+16,
      "budget_used_percent": 32.02230290079744
    },
    {
      "type": "training",
      "description": "Training step 5390",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:10",
      "total_flops_so_far": 3.2028243958849536e+16,
      "budget_used_percent": 32.02824395884954
    },
    {
      "type": "training",
      "description": "Training step 5391",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:10",
      "total_flops_so_far": 3.203418501690163e+16,
      "budget_used_percent": 32.03418501690163
    },
    {
      "type": "training",
      "description": "Training step 5392",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:11",
      "total_flops_so_far": 3.204012607495373e+16,
      "budget_used_percent": 32.040126074953726
    },
    {
      "type": "training",
      "description": "Training step 5393",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:11",
      "total_flops_so_far": 3.2046067133005824e+16,
      "budget_used_percent": 32.04606713300582
    },
    {
      "type": "training",
      "description": "Training step 5394",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:12",
      "total_flops_so_far": 3.205200819105792e+16,
      "budget_used_percent": 32.052008191057915
    },
    {
      "type": "training",
      "description": "Training step 5395",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:12",
      "total_flops_so_far": 3.2057949249110016e+16,
      "budget_used_percent": 32.05794924911002
    },
    {
      "type": "training",
      "description": "Training step 5396",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:13",
      "total_flops_so_far": 3.206389030716211e+16,
      "budget_used_percent": 32.06389030716211
    },
    {
      "type": "training",
      "description": "Training step 5397",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:13",
      "total_flops_so_far": 3.206983136521421e+16,
      "budget_used_percent": 32.069831365214206
    },
    {
      "type": "training",
      "description": "Training step 5398",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:13",
      "total_flops_so_far": 3.2075772423266304e+16,
      "budget_used_percent": 32.0757724232663
    },
    {
      "type": "training",
      "description": "Training step 5399",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:14",
      "total_flops_so_far": 3.20817134813184e+16,
      "budget_used_percent": 32.0817134813184
    },
    {
      "type": "training",
      "description": "Training step 5400",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:15",
      "total_flops_so_far": 3.2087654539370496e+16,
      "budget_used_percent": 32.0876545393705
    },
    {
      "type": "training",
      "description": "Training step 5401",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:15",
      "total_flops_so_far": 3.209359559742259e+16,
      "budget_used_percent": 32.09359559742259
    },
    {
      "type": "training",
      "description": "Training step 5402",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:16",
      "total_flops_so_far": 3.209953665547469e+16,
      "budget_used_percent": 32.09953665547469
    },
    {
      "type": "training",
      "description": "Training step 5403",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:16",
      "total_flops_so_far": 3.2105477713526784e+16,
      "budget_used_percent": 32.10547771352679
    },
    {
      "type": "training",
      "description": "Training step 5404",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:16",
      "total_flops_so_far": 3.211141877157888e+16,
      "budget_used_percent": 32.11141877157888
    },
    {
      "type": "training",
      "description": "Training step 5405",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:17",
      "total_flops_so_far": 3.2117359829630976e+16,
      "budget_used_percent": 32.11735982963098
    },
    {
      "type": "training",
      "description": "Training step 5406",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:17",
      "total_flops_so_far": 3.212330088768307e+16,
      "budget_used_percent": 32.12330088768307
    },
    {
      "type": "training",
      "description": "Training step 5407",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:18",
      "total_flops_so_far": 3.212924194573517e+16,
      "budget_used_percent": 32.12924194573517
    },
    {
      "type": "training",
      "description": "Training step 5408",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:18",
      "total_flops_so_far": 3.2135183003787264e+16,
      "budget_used_percent": 32.13518300378727
    },
    {
      "type": "training",
      "description": "Training step 5409",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:19",
      "total_flops_so_far": 3.214112406183936e+16,
      "budget_used_percent": 32.14112406183936
    },
    {
      "type": "training",
      "description": "Training step 5410",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:19",
      "total_flops_so_far": 3.2147065119891456e+16,
      "budget_used_percent": 32.14706511989146
    },
    {
      "type": "training",
      "description": "Training step 5411",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:19",
      "total_flops_so_far": 3.215300617794355e+16,
      "budget_used_percent": 32.15300617794355
    },
    {
      "type": "training",
      "description": "Training step 5412",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:20",
      "total_flops_so_far": 3.215894723599565e+16,
      "budget_used_percent": 32.15894723599565
    },
    {
      "type": "training",
      "description": "Training step 5413",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:20",
      "total_flops_so_far": 3.2164888294047744e+16,
      "budget_used_percent": 32.16488829404774
    },
    {
      "type": "training",
      "description": "Training step 5414",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:21",
      "total_flops_so_far": 3.217082935209984e+16,
      "budget_used_percent": 32.17082935209984
    },
    {
      "type": "training",
      "description": "Training step 5415",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:21",
      "total_flops_so_far": 3.2176770410151936e+16,
      "budget_used_percent": 32.17677041015193
    },
    {
      "type": "training",
      "description": "Training step 5416",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:22",
      "total_flops_so_far": 3.218271146820403e+16,
      "budget_used_percent": 32.18271146820403
    },
    {
      "type": "training",
      "description": "Training step 5417",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:22",
      "total_flops_so_far": 3.218865252625613e+16,
      "budget_used_percent": 32.18865252625613
    },
    {
      "type": "training",
      "description": "Training step 5418",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:22",
      "total_flops_so_far": 3.2194593584308224e+16,
      "budget_used_percent": 32.19459358430822
    },
    {
      "type": "training",
      "description": "Training step 5419",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:23",
      "total_flops_so_far": 3.220053464236032e+16,
      "budget_used_percent": 32.20053464236032
    },
    {
      "type": "training",
      "description": "Training step 5420",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:23",
      "total_flops_so_far": 3.2206475700412416e+16,
      "budget_used_percent": 32.20647570041242
    },
    {
      "type": "training",
      "description": "Training step 5421",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:24",
      "total_flops_so_far": 3.221241675846451e+16,
      "budget_used_percent": 32.212416758464514
    },
    {
      "type": "training",
      "description": "Training step 5422",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:24",
      "total_flops_so_far": 3.221835781651661e+16,
      "budget_used_percent": 32.21835781651661
    },
    {
      "type": "training",
      "description": "Training step 5423",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:25",
      "total_flops_so_far": 3.2224298874568704e+16,
      "budget_used_percent": 32.2242988745687
    },
    {
      "type": "training",
      "description": "Training step 5424",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:25",
      "total_flops_so_far": 3.22302399326208e+16,
      "budget_used_percent": 32.230239932620805
    },
    {
      "type": "training",
      "description": "Training step 5425",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:26",
      "total_flops_so_far": 3.2236180990672896e+16,
      "budget_used_percent": 32.2361809906729
    },
    {
      "type": "training",
      "description": "Training step 5426",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:26",
      "total_flops_so_far": 3.224212204872499e+16,
      "budget_used_percent": 32.242122048724994
    },
    {
      "type": "training",
      "description": "Training step 5427",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:27",
      "total_flops_so_far": 3.224806310677709e+16,
      "budget_used_percent": 32.24806310677709
    },
    {
      "type": "training",
      "description": "Training step 5428",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:27",
      "total_flops_so_far": 3.2254004164829184e+16,
      "budget_used_percent": 32.25400416482918
    },
    {
      "type": "training",
      "description": "Training step 5429",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:28",
      "total_flops_so_far": 3.225994522288128e+16,
      "budget_used_percent": 32.25994522288128
    },
    {
      "type": "training",
      "description": "Training step 5430",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:28",
      "total_flops_so_far": 3.2265886280933376e+16,
      "budget_used_percent": 32.26588628093337
    },
    {
      "type": "training",
      "description": "Training step 5431",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:28",
      "total_flops_so_far": 3.227182733898547e+16,
      "budget_used_percent": 32.27182733898547
    },
    {
      "type": "training",
      "description": "Training step 5432",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:29",
      "total_flops_so_far": 3.227776839703757e+16,
      "budget_used_percent": 32.27776839703757
    },
    {
      "type": "training",
      "description": "Training step 5433",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:29",
      "total_flops_so_far": 3.2283709455089664e+16,
      "budget_used_percent": 32.283709455089664
    },
    {
      "type": "training",
      "description": "Training step 5434",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:30",
      "total_flops_so_far": 3.228965051314176e+16,
      "budget_used_percent": 32.28965051314176
    },
    {
      "type": "training",
      "description": "Training step 5435",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:30",
      "total_flops_so_far": 3.2295591571193856e+16,
      "budget_used_percent": 32.29559157119385
    },
    {
      "type": "training",
      "description": "Training step 5436",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:31",
      "total_flops_so_far": 3.230153262924595e+16,
      "budget_used_percent": 32.30153262924595
    },
    {
      "type": "training",
      "description": "Training step 5437",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:31",
      "total_flops_so_far": 3.230747368729805e+16,
      "budget_used_percent": 32.30747368729805
    },
    {
      "type": "training",
      "description": "Training step 5438",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:31",
      "total_flops_so_far": 3.2313414745350144e+16,
      "budget_used_percent": 32.313414745350144
    },
    {
      "type": "training",
      "description": "Training step 5439",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:32",
      "total_flops_so_far": 3.231935580340224e+16,
      "budget_used_percent": 32.31935580340224
    },
    {
      "type": "training",
      "description": "Training step 5440",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:32",
      "total_flops_so_far": 3.2325296861454336e+16,
      "budget_used_percent": 32.32529686145433
    },
    {
      "type": "training",
      "description": "Training step 5441",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:33",
      "total_flops_so_far": 3.233123791950643e+16,
      "budget_used_percent": 32.331237919506435
    },
    {
      "type": "training",
      "description": "Training step 5442",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:33",
      "total_flops_so_far": 3.233717897755853e+16,
      "budget_used_percent": 32.33717897755853
    },
    {
      "type": "training",
      "description": "Training step 5443",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:34",
      "total_flops_so_far": 3.2343120035610624e+16,
      "budget_used_percent": 32.343120035610625
    },
    {
      "type": "training",
      "description": "Training step 5444",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:34",
      "total_flops_so_far": 3.234906109366272e+16,
      "budget_used_percent": 32.34906109366272
    },
    {
      "type": "training",
      "description": "Training step 5445",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:35",
      "total_flops_so_far": 3.2355002151714816e+16,
      "budget_used_percent": 32.35500215171482
    },
    {
      "type": "training",
      "description": "Training step 5446",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:35",
      "total_flops_so_far": 3.236094320976691e+16,
      "budget_used_percent": 32.360943209766916
    },
    {
      "type": "training",
      "description": "Training step 5447",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:35",
      "total_flops_so_far": 3.236688426781901e+16,
      "budget_used_percent": 32.36688426781901
    },
    {
      "type": "training",
      "description": "Training step 5448",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:36",
      "total_flops_so_far": 3.2372825325871104e+16,
      "budget_used_percent": 32.372825325871105
    },
    {
      "type": "training",
      "description": "Training step 5449",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:36",
      "total_flops_so_far": 3.23787663839232e+16,
      "budget_used_percent": 32.3787663839232
    },
    {
      "type": "training",
      "description": "Training step 5450",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:37",
      "total_flops_so_far": 3.2384707441975296e+16,
      "budget_used_percent": 32.384707441975294
    },
    {
      "type": "training",
      "description": "Training step 5451",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:37",
      "total_flops_so_far": 3.239064850002739e+16,
      "budget_used_percent": 32.39064850002739
    },
    {
      "type": "training",
      "description": "Training step 5452",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:38",
      "total_flops_so_far": 3.239658955807949e+16,
      "budget_used_percent": 32.396589558079484
    },
    {
      "type": "training",
      "description": "Training step 5453",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:38",
      "total_flops_so_far": 3.2402530616131584e+16,
      "budget_used_percent": 32.402530616131585
    },
    {
      "type": "training",
      "description": "Training step 5454",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:38",
      "total_flops_so_far": 3.240847167418368e+16,
      "budget_used_percent": 32.40847167418368
    },
    {
      "type": "training",
      "description": "Training step 5455",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:39",
      "total_flops_so_far": 3.2414412732235776e+16,
      "budget_used_percent": 32.414412732235775
    },
    {
      "type": "training",
      "description": "Training step 5456",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:39",
      "total_flops_so_far": 3.242035379028787e+16,
      "budget_used_percent": 32.42035379028787
    },
    {
      "type": "training",
      "description": "Training step 5457",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:40",
      "total_flops_so_far": 3.242629484833997e+16,
      "budget_used_percent": 32.42629484833997
    },
    {
      "type": "training",
      "description": "Training step 5458",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:40",
      "total_flops_so_far": 3.2432235906392064e+16,
      "budget_used_percent": 32.432235906392066
    },
    {
      "type": "training",
      "description": "Training step 5459",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:41",
      "total_flops_so_far": 3.243817696444416e+16,
      "budget_used_percent": 32.43817696444416
    },
    {
      "type": "training",
      "description": "Training step 5460",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:41",
      "total_flops_so_far": 3.2444118022496256e+16,
      "budget_used_percent": 32.444118022496255
    },
    {
      "type": "training",
      "description": "Training step 5461",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:42",
      "total_flops_so_far": 3.245005908054835e+16,
      "budget_used_percent": 32.45005908054836
    },
    {
      "type": "training",
      "description": "Training step 5462",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:42",
      "total_flops_so_far": 3.245600013860045e+16,
      "budget_used_percent": 32.45600013860045
    },
    {
      "type": "training",
      "description": "Training step 5463",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:43",
      "total_flops_so_far": 3.2461941196652544e+16,
      "budget_used_percent": 32.461941196652546
    },
    {
      "type": "training",
      "description": "Training step 5464",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:43",
      "total_flops_so_far": 3.246788225470464e+16,
      "budget_used_percent": 32.46788225470464
    },
    {
      "type": "training",
      "description": "Training step 5465",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:44",
      "total_flops_so_far": 3.2473823312756736e+16,
      "budget_used_percent": 32.473823312756736
    },
    {
      "type": "training",
      "description": "Training step 5466",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:44",
      "total_flops_so_far": 3.247976437080883e+16,
      "budget_used_percent": 32.47976437080883
    },
    {
      "type": "training",
      "description": "Training step 5467",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:44",
      "total_flops_so_far": 3.248570542886093e+16,
      "budget_used_percent": 32.485705428860925
    },
    {
      "type": "training",
      "description": "Training step 5468",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:45",
      "total_flops_so_far": 3.2491646486913024e+16,
      "budget_used_percent": 32.49164648691302
    },
    {
      "type": "training",
      "description": "Training step 5469",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:45",
      "total_flops_so_far": 3.249758754496512e+16,
      "budget_used_percent": 32.497587544965114
    },
    {
      "type": "training",
      "description": "Training step 5470",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:46",
      "total_flops_so_far": 3.2503528603017216e+16,
      "budget_used_percent": 32.503528603017216
    },
    {
      "type": "training",
      "description": "Training step 5471",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:46",
      "total_flops_so_far": 3.250946966106931e+16,
      "budget_used_percent": 32.50946966106931
    },
    {
      "type": "training",
      "description": "Training step 5472",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:47",
      "total_flops_so_far": 3.251541071912141e+16,
      "budget_used_percent": 32.515410719121405
    },
    {
      "type": "training",
      "description": "Training step 5473",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:47",
      "total_flops_so_far": 3.2521351777173504e+16,
      "budget_used_percent": 32.5213517771735
    },
    {
      "type": "training",
      "description": "Training step 5474",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:47",
      "total_flops_so_far": 3.25272928352256e+16,
      "budget_used_percent": 32.5272928352256
    },
    {
      "type": "training",
      "description": "Training step 5475",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:48",
      "total_flops_so_far": 3.2533233893277696e+16,
      "budget_used_percent": 32.533233893277696
    },
    {
      "type": "training",
      "description": "Training step 5476",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:48",
      "total_flops_so_far": 3.253917495132979e+16,
      "budget_used_percent": 32.53917495132979
    },
    {
      "type": "training",
      "description": "Training step 5477",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:49",
      "total_flops_so_far": 3.254511600938189e+16,
      "budget_used_percent": 32.545116009381886
    },
    {
      "type": "training",
      "description": "Training step 5478",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:49",
      "total_flops_so_far": 3.2551057067433984e+16,
      "budget_used_percent": 32.55105706743399
    },
    {
      "type": "training",
      "description": "Training step 5479",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:50",
      "total_flops_so_far": 3.255699812548608e+16,
      "budget_used_percent": 32.55699812548608
    },
    {
      "type": "training",
      "description": "Training step 5480",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:50",
      "total_flops_so_far": 3.2562939183538176e+16,
      "budget_used_percent": 32.56293918353818
    },
    {
      "type": "training",
      "description": "Training step 5481",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:51",
      "total_flops_so_far": 3.256888024159027e+16,
      "budget_used_percent": 32.56888024159027
    },
    {
      "type": "training",
      "description": "Training step 5482",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:51",
      "total_flops_so_far": 3.257482129964237e+16,
      "budget_used_percent": 32.57482129964237
    },
    {
      "type": "training",
      "description": "Training step 5483",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:51",
      "total_flops_so_far": 3.2580762357694464e+16,
      "budget_used_percent": 32.58076235769447
    },
    {
      "type": "training",
      "description": "Training step 5484",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:52",
      "total_flops_so_far": 3.258670341574656e+16,
      "budget_used_percent": 32.58670341574656
    },
    {
      "type": "training",
      "description": "Training step 5485",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:52",
      "total_flops_so_far": 3.2592644473798656e+16,
      "budget_used_percent": 32.59264447379866
    },
    {
      "type": "training",
      "description": "Training step 5486",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:53",
      "total_flops_so_far": 3.259858553185075e+16,
      "budget_used_percent": 32.59858553185075
    },
    {
      "type": "training",
      "description": "Training step 5487",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:53",
      "total_flops_so_far": 3.260452658990285e+16,
      "budget_used_percent": 32.60452658990285
    },
    {
      "type": "training",
      "description": "Training step 5488",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:54",
      "total_flops_so_far": 3.2610467647954944e+16,
      "budget_used_percent": 32.61046764795494
    },
    {
      "type": "training",
      "description": "Training step 5489",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:54",
      "total_flops_so_far": 3.261640870600704e+16,
      "budget_used_percent": 32.616408706007036
    },
    {
      "type": "training",
      "description": "Training step 5490",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:55",
      "total_flops_so_far": 3.2622349764059136e+16,
      "budget_used_percent": 32.62234976405914
    },
    {
      "type": "training",
      "description": "Training step 5491",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:55",
      "total_flops_so_far": 3.262829082211123e+16,
      "budget_used_percent": 32.62829082211123
    },
    {
      "type": "training",
      "description": "Training step 5492",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:56",
      "total_flops_so_far": 3.263423188016333e+16,
      "budget_used_percent": 32.63423188016333
    },
    {
      "type": "training",
      "description": "Training step 5493",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:56",
      "total_flops_so_far": 3.2640172938215424e+16,
      "budget_used_percent": 32.64017293821542
    },
    {
      "type": "training",
      "description": "Training step 5494",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:56",
      "total_flops_so_far": 3.264611399626752e+16,
      "budget_used_percent": 32.646113996267516
    },
    {
      "type": "training",
      "description": "Training step 5495",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:57",
      "total_flops_so_far": 3.2652055054319616e+16,
      "budget_used_percent": 32.65205505431962
    },
    {
      "type": "training",
      "description": "Training step 5496",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:57",
      "total_flops_so_far": 3.265799611237171e+16,
      "budget_used_percent": 32.65799611237171
    },
    {
      "type": "training",
      "description": "Training step 5497",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:58",
      "total_flops_so_far": 3.266393717042381e+16,
      "budget_used_percent": 32.66393717042381
    },
    {
      "type": "training",
      "description": "Training step 5498",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:58",
      "total_flops_so_far": 3.2669878228475904e+16,
      "budget_used_percent": 32.6698782284759
    },
    {
      "type": "training",
      "description": "Training step 5499",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:15:59",
      "total_flops_so_far": 3.2675819286528e+16,
      "budget_used_percent": 32.675819286528004
    },
    {
      "type": "training",
      "description": "Training step 5500",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:24",
      "total_flops_so_far": 3.2681760344580096e+16,
      "budget_used_percent": 32.6817603445801
    },
    {
      "type": "training",
      "description": "Training step 5501",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:25",
      "total_flops_so_far": 3.268770140263219e+16,
      "budget_used_percent": 32.68770140263219
    },
    {
      "type": "training",
      "description": "Training step 5502",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:25",
      "total_flops_so_far": 3.269364246068429e+16,
      "budget_used_percent": 32.69364246068429
    },
    {
      "type": "training",
      "description": "Training step 5503",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:25",
      "total_flops_so_far": 3.2699583518736384e+16,
      "budget_used_percent": 32.69958351873639
    },
    {
      "type": "training",
      "description": "Training step 5504",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:26",
      "total_flops_so_far": 3.270552457678848e+16,
      "budget_used_percent": 32.70552457678848
    },
    {
      "type": "training",
      "description": "Training step 5505",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:26",
      "total_flops_so_far": 3.2711465634840576e+16,
      "budget_used_percent": 32.71146563484058
    },
    {
      "type": "training",
      "description": "Training step 5506",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:27",
      "total_flops_so_far": 3.271740669289267e+16,
      "budget_used_percent": 32.717406692892666
    },
    {
      "type": "training",
      "description": "Training step 5507",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:27",
      "total_flops_so_far": 3.272334775094477e+16,
      "budget_used_percent": 32.72334775094477
    },
    {
      "type": "training",
      "description": "Training step 5508",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:28",
      "total_flops_so_far": 3.2729288808996864e+16,
      "budget_used_percent": 32.72928880899686
    },
    {
      "type": "training",
      "description": "Training step 5509",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:28",
      "total_flops_so_far": 3.273522986704896e+16,
      "budget_used_percent": 32.73522986704896
    },
    {
      "type": "training",
      "description": "Training step 5510",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:28",
      "total_flops_so_far": 3.2741170925101056e+16,
      "budget_used_percent": 32.74117092510105
    },
    {
      "type": "training",
      "description": "Training step 5511",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:29",
      "total_flops_so_far": 3.274711198315315e+16,
      "budget_used_percent": 32.747111983153154
    },
    {
      "type": "training",
      "description": "Training step 5512",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:29",
      "total_flops_so_far": 3.275305304120525e+16,
      "budget_used_percent": 32.75305304120525
    },
    {
      "type": "training",
      "description": "Training step 5513",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:30",
      "total_flops_so_far": 3.2758994099257344e+16,
      "budget_used_percent": 32.75899409925734
    },
    {
      "type": "training",
      "description": "Training step 5514",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:30",
      "total_flops_so_far": 3.276493515730944e+16,
      "budget_used_percent": 32.76493515730944
    },
    {
      "type": "training",
      "description": "Training step 5515",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:31",
      "total_flops_so_far": 3.2770876215361536e+16,
      "budget_used_percent": 32.77087621536154
    },
    {
      "type": "training",
      "description": "Training step 5516",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:31",
      "total_flops_so_far": 3.277681727341363e+16,
      "budget_used_percent": 32.776817273413634
    },
    {
      "type": "training",
      "description": "Training step 5517",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:32",
      "total_flops_so_far": 3.278275833146573e+16,
      "budget_used_percent": 32.78275833146573
    },
    {
      "type": "training",
      "description": "Training step 5518",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:32",
      "total_flops_so_far": 3.2788699389517824e+16,
      "budget_used_percent": 32.788699389517824
    },
    {
      "type": "training",
      "description": "Training step 5519",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:33",
      "total_flops_so_far": 3.279464044756992e+16,
      "budget_used_percent": 32.794640447569925
    },
    {
      "type": "training",
      "description": "Training step 5520",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:33",
      "total_flops_so_far": 3.2800581505622016e+16,
      "budget_used_percent": 32.80058150562202
    },
    {
      "type": "training",
      "description": "Training step 5521",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:34",
      "total_flops_so_far": 3.280652256367411e+16,
      "budget_used_percent": 32.806522563674115
    },
    {
      "type": "training",
      "description": "Training step 5522",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:34",
      "total_flops_so_far": 3.281246362172621e+16,
      "budget_used_percent": 32.81246362172621
    },
    {
      "type": "training",
      "description": "Training step 5523",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:34",
      "total_flops_so_far": 3.2818404679778304e+16,
      "budget_used_percent": 32.818404679778304
    },
    {
      "type": "training",
      "description": "Training step 5524",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:35",
      "total_flops_so_far": 3.28243457378304e+16,
      "budget_used_percent": 32.8243457378304
    },
    {
      "type": "training",
      "description": "Training step 5525",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:35",
      "total_flops_so_far": 3.2830286795882496e+16,
      "budget_used_percent": 32.83028679588249
    },
    {
      "type": "training",
      "description": "Training step 5526",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:36",
      "total_flops_so_far": 3.283622785393459e+16,
      "budget_used_percent": 32.83622785393459
    },
    {
      "type": "training",
      "description": "Training step 5527",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:36",
      "total_flops_so_far": 3.284216891198669e+16,
      "budget_used_percent": 32.84216891198668
    },
    {
      "type": "training",
      "description": "Training step 5528",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:37",
      "total_flops_so_far": 3.2848109970038784e+16,
      "budget_used_percent": 32.848109970038784
    },
    {
      "type": "training",
      "description": "Training step 5529",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:37",
      "total_flops_so_far": 3.285405102809088e+16,
      "budget_used_percent": 32.85405102809088
    },
    {
      "type": "training",
      "description": "Training step 5530",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:37",
      "total_flops_so_far": 3.2859992086142976e+16,
      "budget_used_percent": 32.859992086142974
    },
    {
      "type": "training",
      "description": "Training step 5531",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:38",
      "total_flops_so_far": 3.286593314419507e+16,
      "budget_used_percent": 32.86593314419507
    },
    {
      "type": "training",
      "description": "Training step 5532",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:38",
      "total_flops_so_far": 3.287187420224717e+16,
      "budget_used_percent": 32.87187420224717
    },
    {
      "type": "training",
      "description": "Training step 5533",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:39",
      "total_flops_so_far": 3.2877815260299264e+16,
      "budget_used_percent": 32.877815260299265
    },
    {
      "type": "training",
      "description": "Training step 5534",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:39",
      "total_flops_so_far": 3.288375631835136e+16,
      "budget_used_percent": 32.88375631835136
    },
    {
      "type": "training",
      "description": "Training step 5535",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:40",
      "total_flops_so_far": 3.2889697376403456e+16,
      "budget_used_percent": 32.889697376403454
    },
    {
      "type": "training",
      "description": "Training step 5536",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:40",
      "total_flops_so_far": 3.289563843445555e+16,
      "budget_used_percent": 32.895638434455556
    },
    {
      "type": "training",
      "description": "Training step 5537",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:41",
      "total_flops_so_far": 3.290157949250765e+16,
      "budget_used_percent": 32.90157949250765
    },
    {
      "type": "training",
      "description": "Training step 5538",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:41",
      "total_flops_so_far": 3.2907520550559744e+16,
      "budget_used_percent": 32.907520550559745
    },
    {
      "type": "training",
      "description": "Training step 5539",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:41",
      "total_flops_so_far": 3.291346160861184e+16,
      "budget_used_percent": 32.91346160861184
    },
    {
      "type": "training",
      "description": "Training step 5540",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:42",
      "total_flops_so_far": 3.2919402666663936e+16,
      "budget_used_percent": 32.91940266666394
    },
    {
      "type": "training",
      "description": "Training step 5541",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:42",
      "total_flops_so_far": 3.292534372471603e+16,
      "budget_used_percent": 32.925343724716036
    },
    {
      "type": "training",
      "description": "Training step 5542",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:43",
      "total_flops_so_far": 3.293128478276813e+16,
      "budget_used_percent": 32.93128478276813
    },
    {
      "type": "training",
      "description": "Training step 5543",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:43",
      "total_flops_so_far": 3.2937225840820224e+16,
      "budget_used_percent": 32.937225840820226
    },
    {
      "type": "training",
      "description": "Training step 5544",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:44",
      "total_flops_so_far": 3.294316689887232e+16,
      "budget_used_percent": 32.94316689887232
    },
    {
      "type": "training",
      "description": "Training step 5545",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:44",
      "total_flops_so_far": 3.2949107956924416e+16,
      "budget_used_percent": 32.949107956924415
    },
    {
      "type": "training",
      "description": "Training step 5546",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:45",
      "total_flops_so_far": 3.295504901497651e+16,
      "budget_used_percent": 32.95504901497651
    },
    {
      "type": "training",
      "description": "Training step 5547",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:45",
      "total_flops_so_far": 3.296099007302861e+16,
      "budget_used_percent": 32.960990073028604
    },
    {
      "type": "training",
      "description": "Training step 5548",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:46",
      "total_flops_so_far": 3.2966931131080704e+16,
      "budget_used_percent": 32.966931131080706
    },
    {
      "type": "training",
      "description": "Training step 5549",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:46",
      "total_flops_so_far": 3.29728721891328e+16,
      "budget_used_percent": 32.9728721891328
    },
    {
      "type": "training",
      "description": "Training step 5550",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:47",
      "total_flops_so_far": 3.2978813247184896e+16,
      "budget_used_percent": 32.978813247184895
    },
    {
      "type": "training",
      "description": "Training step 5551",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:47",
      "total_flops_so_far": 3.298475430523699e+16,
      "budget_used_percent": 32.98475430523699
    },
    {
      "type": "training",
      "description": "Training step 5552",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:47",
      "total_flops_so_far": 3.299069536328909e+16,
      "budget_used_percent": 32.990695363289085
    },
    {
      "type": "training",
      "description": "Training step 5553",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:48",
      "total_flops_so_far": 3.2996636421341184e+16,
      "budget_used_percent": 32.99663642134119
    },
    {
      "type": "training",
      "description": "Training step 5554",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:48",
      "total_flops_so_far": 3.300257747939328e+16,
      "budget_used_percent": 33.00257747939328
    },
    {
      "type": "training",
      "description": "Training step 5555",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:49",
      "total_flops_so_far": 3.3008518537445376e+16,
      "budget_used_percent": 33.008518537445376
    },
    {
      "type": "training",
      "description": "Training step 5556",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:49",
      "total_flops_so_far": 3.301445959549747e+16,
      "budget_used_percent": 33.01445959549747
    },
    {
      "type": "training",
      "description": "Training step 5557",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:50",
      "total_flops_so_far": 3.302040065354957e+16,
      "budget_used_percent": 33.02040065354957
    },
    {
      "type": "training",
      "description": "Training step 5558",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:50",
      "total_flops_so_far": 3.3026341711601664e+16,
      "budget_used_percent": 33.02634171160167
    },
    {
      "type": "training",
      "description": "Training step 5559",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:50",
      "total_flops_so_far": 3.303228276965376e+16,
      "budget_used_percent": 33.03228276965376
    },
    {
      "type": "training",
      "description": "Training step 5560",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:51",
      "total_flops_so_far": 3.3038223827705856e+16,
      "budget_used_percent": 33.038223827705856
    },
    {
      "type": "training",
      "description": "Training step 5561",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:51",
      "total_flops_so_far": 3.304416488575795e+16,
      "budget_used_percent": 33.04416488575795
    },
    {
      "type": "training",
      "description": "Training step 5562",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:52",
      "total_flops_so_far": 3.305010594381005e+16,
      "budget_used_percent": 33.050105943810046
    },
    {
      "type": "training",
      "description": "Training step 5563",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:52",
      "total_flops_so_far": 3.3056047001862144e+16,
      "budget_used_percent": 33.05604700186214
    },
    {
      "type": "training",
      "description": "Training step 5564",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:53",
      "total_flops_so_far": 3.306198805991424e+16,
      "budget_used_percent": 33.061988059914235
    },
    {
      "type": "training",
      "description": "Training step 5565",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:53",
      "total_flops_so_far": 3.3067929117966336e+16,
      "budget_used_percent": 33.06792911796634
    },
    {
      "type": "training",
      "description": "Training step 5566",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:54",
      "total_flops_so_far": 3.307387017601843e+16,
      "budget_used_percent": 33.07387017601843
    },
    {
      "type": "training",
      "description": "Training step 5567",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:54",
      "total_flops_so_far": 3.307981123407053e+16,
      "budget_used_percent": 33.079811234070526
    },
    {
      "type": "training",
      "description": "Training step 5568",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:54",
      "total_flops_so_far": 3.3085752292122624e+16,
      "budget_used_percent": 33.08575229212262
    },
    {
      "type": "training",
      "description": "Training step 5569",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:55",
      "total_flops_so_far": 3.309169335017472e+16,
      "budget_used_percent": 33.09169335017472
    },
    {
      "type": "training",
      "description": "Training step 5570",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:55",
      "total_flops_so_far": 3.3097634408226816e+16,
      "budget_used_percent": 33.09763440822682
    },
    {
      "type": "training",
      "description": "Training step 5571",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:56",
      "total_flops_so_far": 3.310357546627891e+16,
      "budget_used_percent": 33.10357546627891
    },
    {
      "type": "training",
      "description": "Training step 5572",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:56",
      "total_flops_so_far": 3.310951652433101e+16,
      "budget_used_percent": 33.109516524331006
    },
    {
      "type": "training",
      "description": "Training step 5573",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:57",
      "total_flops_so_far": 3.3115457582383104e+16,
      "budget_used_percent": 33.11545758238311
    },
    {
      "type": "training",
      "description": "Training step 5574",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:57",
      "total_flops_so_far": 3.31213986404352e+16,
      "budget_used_percent": 33.1213986404352
    },
    {
      "type": "training",
      "description": "Training step 5575",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:57",
      "total_flops_so_far": 3.3127339698487296e+16,
      "budget_used_percent": 33.1273396984873
    },
    {
      "type": "training",
      "description": "Training step 5576",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:58",
      "total_flops_so_far": 3.313328075653939e+16,
      "budget_used_percent": 33.13328075653939
    },
    {
      "type": "training",
      "description": "Training step 5577",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:59",
      "total_flops_so_far": 3.313922181459149e+16,
      "budget_used_percent": 33.139221814591494
    },
    {
      "type": "training",
      "description": "Training step 5578",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:18:59",
      "total_flops_so_far": 3.3145162872643584e+16,
      "budget_used_percent": 33.14516287264359
    },
    {
      "type": "training",
      "description": "Training step 5579",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:00",
      "total_flops_so_far": 3.315110393069568e+16,
      "budget_used_percent": 33.15110393069568
    },
    {
      "type": "training",
      "description": "Training step 5580",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:00",
      "total_flops_so_far": 3.3157044988747776e+16,
      "budget_used_percent": 33.15704498874778
    },
    {
      "type": "training",
      "description": "Training step 5581",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:00",
      "total_flops_so_far": 3.316298604679987e+16,
      "budget_used_percent": 33.16298604679987
    },
    {
      "type": "training",
      "description": "Training step 5582",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:01",
      "total_flops_so_far": 3.316892710485197e+16,
      "budget_used_percent": 33.16892710485197
    },
    {
      "type": "training",
      "description": "Training step 5583",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:01",
      "total_flops_so_far": 3.3174868162904064e+16,
      "budget_used_percent": 33.17486816290406
    },
    {
      "type": "training",
      "description": "Training step 5584",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:02",
      "total_flops_so_far": 3.318080922095616e+16,
      "budget_used_percent": 33.18080922095616
    },
    {
      "type": "training",
      "description": "Training step 5585",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:02",
      "total_flops_so_far": 3.3186750279008256e+16,
      "budget_used_percent": 33.18675027900825
    },
    {
      "type": "training",
      "description": "Training step 5586",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:03",
      "total_flops_so_far": 3.319269133706035e+16,
      "budget_used_percent": 33.19269133706035
    },
    {
      "type": "training",
      "description": "Training step 5587",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:03",
      "total_flops_so_far": 3.319863239511245e+16,
      "budget_used_percent": 33.19863239511245
    },
    {
      "type": "training",
      "description": "Training step 5588",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:03",
      "total_flops_so_far": 3.3204573453164544e+16,
      "budget_used_percent": 33.20457345316454
    },
    {
      "type": "training",
      "description": "Training step 5589",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:04",
      "total_flops_so_far": 3.321051451121664e+16,
      "budget_used_percent": 33.21051451121664
    },
    {
      "type": "training",
      "description": "Training step 5590",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:04",
      "total_flops_so_far": 3.3216455569268736e+16,
      "budget_used_percent": 33.21645556926874
    },
    {
      "type": "training",
      "description": "Training step 5591",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:05",
      "total_flops_so_far": 3.322239662732083e+16,
      "budget_used_percent": 33.22239662732083
    },
    {
      "type": "training",
      "description": "Training step 5592",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:05",
      "total_flops_so_far": 3.322833768537293e+16,
      "budget_used_percent": 33.22833768537293
    },
    {
      "type": "training",
      "description": "Training step 5593",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:06",
      "total_flops_so_far": 3.3234278743425024e+16,
      "budget_used_percent": 33.23427874342502
    },
    {
      "type": "training",
      "description": "Training step 5594",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:06",
      "total_flops_so_far": 3.324021980147712e+16,
      "budget_used_percent": 33.240219801477124
    },
    {
      "type": "training",
      "description": "Training step 5595",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:07",
      "total_flops_so_far": 3.3246160859529216e+16,
      "budget_used_percent": 33.24616085952922
    },
    {
      "type": "training",
      "description": "Training step 5596",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:07",
      "total_flops_so_far": 3.325210191758131e+16,
      "budget_used_percent": 33.252101917581314
    },
    {
      "type": "training",
      "description": "Training step 5597",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:07",
      "total_flops_so_far": 3.325804297563341e+16,
      "budget_used_percent": 33.25804297563341
    },
    {
      "type": "training",
      "description": "Training step 5598",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:08",
      "total_flops_so_far": 3.3263984033685504e+16,
      "budget_used_percent": 33.2639840336855
    },
    {
      "type": "training",
      "description": "Training step 5599",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:08",
      "total_flops_so_far": 3.32699250917376e+16,
      "budget_used_percent": 33.2699250917376
    },
    {
      "type": "training",
      "description": "Training step 5600",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:09",
      "total_flops_so_far": 3.3275866149789696e+16,
      "budget_used_percent": 33.27586614978969
    },
    {
      "type": "training",
      "description": "Training step 5601",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:09",
      "total_flops_so_far": 3.328180720784179e+16,
      "budget_used_percent": 33.28180720784179
    },
    {
      "type": "training",
      "description": "Training step 5602",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:10",
      "total_flops_so_far": 3.328774826589389e+16,
      "budget_used_percent": 33.28774826589389
    },
    {
      "type": "training",
      "description": "Training step 5603",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:10",
      "total_flops_so_far": 3.3293689323945984e+16,
      "budget_used_percent": 33.29368932394598
    },
    {
      "type": "training",
      "description": "Training step 5604",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:11",
      "total_flops_so_far": 3.329963038199808e+16,
      "budget_used_percent": 33.29963038199808
    },
    {
      "type": "training",
      "description": "Training step 5605",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:11",
      "total_flops_so_far": 3.3305571440050176e+16,
      "budget_used_percent": 33.30557144005017
    },
    {
      "type": "training",
      "description": "Training step 5606",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:12",
      "total_flops_so_far": 3.331151249810227e+16,
      "budget_used_percent": 33.311512498102275
    },
    {
      "type": "training",
      "description": "Training step 5607",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:12",
      "total_flops_so_far": 3.331745355615437e+16,
      "budget_used_percent": 33.31745355615437
    },
    {
      "type": "training",
      "description": "Training step 5608",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:13",
      "total_flops_so_far": 3.3323394614206464e+16,
      "budget_used_percent": 33.323394614206464
    },
    {
      "type": "training",
      "description": "Training step 5609",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:13",
      "total_flops_so_far": 3.332933567225856e+16,
      "budget_used_percent": 33.32933567225856
    },
    {
      "type": "training",
      "description": "Training step 5610",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:13",
      "total_flops_so_far": 3.3335276730310656e+16,
      "budget_used_percent": 33.33527673031065
    },
    {
      "type": "training",
      "description": "Training step 5611",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:14",
      "total_flops_so_far": 3.334121778836275e+16,
      "budget_used_percent": 33.341217788362755
    },
    {
      "type": "training",
      "description": "Training step 5612",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:14",
      "total_flops_so_far": 3.334715884641485e+16,
      "budget_used_percent": 33.34715884641485
    },
    {
      "type": "training",
      "description": "Training step 5613",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:15",
      "total_flops_so_far": 3.3353099904466944e+16,
      "budget_used_percent": 33.353099904466944
    },
    {
      "type": "training",
      "description": "Training step 5614",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:15",
      "total_flops_so_far": 3.335904096251904e+16,
      "budget_used_percent": 33.35904096251904
    },
    {
      "type": "training",
      "description": "Training step 5615",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:16",
      "total_flops_so_far": 3.3364982020571136e+16,
      "budget_used_percent": 33.36498202057114
    },
    {
      "type": "training",
      "description": "Training step 5616",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:16",
      "total_flops_so_far": 3.337092307862323e+16,
      "budget_used_percent": 33.370923078623235
    },
    {
      "type": "training",
      "description": "Training step 5617",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:16",
      "total_flops_so_far": 3.337686413667533e+16,
      "budget_used_percent": 33.37686413667533
    },
    {
      "type": "training",
      "description": "Training step 5618",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:17",
      "total_flops_so_far": 3.3382805194727424e+16,
      "budget_used_percent": 33.382805194727425
    },
    {
      "type": "training",
      "description": "Training step 5619",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:17",
      "total_flops_so_far": 3.338874625277952e+16,
      "budget_used_percent": 33.38874625277952
    },
    {
      "type": "training",
      "description": "Training step 5620",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:18",
      "total_flops_so_far": 3.3394687310831616e+16,
      "budget_used_percent": 33.394687310831614
    },
    {
      "type": "training",
      "description": "Training step 5621",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:18",
      "total_flops_so_far": 3.340062836888371e+16,
      "budget_used_percent": 33.40062836888371
    },
    {
      "type": "training",
      "description": "Training step 5622",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:19",
      "total_flops_so_far": 3.340656942693581e+16,
      "budget_used_percent": 33.4065694269358
    },
    {
      "type": "training",
      "description": "Training step 5623",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:19",
      "total_flops_so_far": 3.3412510484987904e+16,
      "budget_used_percent": 33.412510484987905
    },
    {
      "type": "training",
      "description": "Training step 5624",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:19",
      "total_flops_so_far": 3.341845154304e+16,
      "budget_used_percent": 33.41845154304
    },
    {
      "type": "training",
      "description": "Training step 5625",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:20",
      "total_flops_so_far": 3.3424392601092096e+16,
      "budget_used_percent": 33.424392601092094
    },
    {
      "type": "training",
      "description": "Training step 5626",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:20",
      "total_flops_so_far": 3.343033365914419e+16,
      "budget_used_percent": 33.43033365914419
    },
    {
      "type": "training",
      "description": "Training step 5627",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:21",
      "total_flops_so_far": 3.343627471719629e+16,
      "budget_used_percent": 33.43627471719629
    },
    {
      "type": "training",
      "description": "Training step 5628",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:21",
      "total_flops_so_far": 3.3442215775248384e+16,
      "budget_used_percent": 33.442215775248386
    },
    {
      "type": "training",
      "description": "Training step 5629",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:22",
      "total_flops_so_far": 3.344815683330048e+16,
      "budget_used_percent": 33.44815683330048
    },
    {
      "type": "training",
      "description": "Training step 5630",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:22",
      "total_flops_so_far": 3.3454097891352576e+16,
      "budget_used_percent": 33.454097891352575
    },
    {
      "type": "training",
      "description": "Training step 5631",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:23",
      "total_flops_so_far": 3.346003894940467e+16,
      "budget_used_percent": 33.46003894940468
    },
    {
      "type": "training",
      "description": "Training step 5632",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:23",
      "total_flops_so_far": 3.346598000745677e+16,
      "budget_used_percent": 33.46598000745677
    },
    {
      "type": "training",
      "description": "Training step 5633",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:24",
      "total_flops_so_far": 3.3471921065508864e+16,
      "budget_used_percent": 33.471921065508866
    },
    {
      "type": "training",
      "description": "Training step 5634",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:24",
      "total_flops_so_far": 3.347786212356096e+16,
      "budget_used_percent": 33.47786212356096
    },
    {
      "type": "training",
      "description": "Training step 5635",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:25",
      "total_flops_so_far": 3.3483803181613056e+16,
      "budget_used_percent": 33.483803181613055
    },
    {
      "type": "training",
      "description": "Training step 5636",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:25",
      "total_flops_so_far": 3.348974423966515e+16,
      "budget_used_percent": 33.48974423966516
    },
    {
      "type": "training",
      "description": "Training step 5637",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:26",
      "total_flops_so_far": 3.349568529771725e+16,
      "budget_used_percent": 33.495685297717245
    },
    {
      "type": "training",
      "description": "Training step 5638",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:26",
      "total_flops_so_far": 3.3501626355769344e+16,
      "budget_used_percent": 33.501626355769346
    },
    {
      "type": "training",
      "description": "Training step 5639",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:26",
      "total_flops_so_far": 3.350756741382144e+16,
      "budget_used_percent": 33.507567413821434
    },
    {
      "type": "training",
      "description": "Training step 5640",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:27",
      "total_flops_so_far": 3.3513508471873536e+16,
      "budget_used_percent": 33.513508471873536
    },
    {
      "type": "training",
      "description": "Training step 5641",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:27",
      "total_flops_so_far": 3.351944952992563e+16,
      "budget_used_percent": 33.51944952992563
    },
    {
      "type": "training",
      "description": "Training step 5642",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:28",
      "total_flops_so_far": 3.352539058797773e+16,
      "budget_used_percent": 33.525390587977725
    },
    {
      "type": "training",
      "description": "Training step 5643",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:28",
      "total_flops_so_far": 3.3531331646029824e+16,
      "budget_used_percent": 33.53133164602982
    },
    {
      "type": "training",
      "description": "Training step 5644",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:29",
      "total_flops_so_far": 3.353727270408192e+16,
      "budget_used_percent": 33.53727270408192
    },
    {
      "type": "training",
      "description": "Training step 5645",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:29",
      "total_flops_so_far": 3.3543213762134016e+16,
      "budget_used_percent": 33.543213762134016
    },
    {
      "type": "training",
      "description": "Training step 5646",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:29",
      "total_flops_so_far": 3.354915482018611e+16,
      "budget_used_percent": 33.54915482018611
    },
    {
      "type": "training",
      "description": "Training step 5647",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:30",
      "total_flops_so_far": 3.355509587823821e+16,
      "budget_used_percent": 33.555095878238205
    },
    {
      "type": "training",
      "description": "Training step 5648",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:30",
      "total_flops_so_far": 3.3561036936290304e+16,
      "budget_used_percent": 33.56103693629031
    },
    {
      "type": "training",
      "description": "Training step 5649",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:31",
      "total_flops_so_far": 3.35669779943424e+16,
      "budget_used_percent": 33.5669779943424
    },
    {
      "type": "training",
      "description": "Training step 5650",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:31",
      "total_flops_so_far": 3.3572919052394496e+16,
      "budget_used_percent": 33.5729190523945
    },
    {
      "type": "training",
      "description": "Training step 5651",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:32",
      "total_flops_so_far": 3.357886011044659e+16,
      "budget_used_percent": 33.57886011044659
    },
    {
      "type": "training",
      "description": "Training step 5652",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:32",
      "total_flops_so_far": 3.358480116849869e+16,
      "budget_used_percent": 33.58480116849869
    },
    {
      "type": "training",
      "description": "Training step 5653",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:33",
      "total_flops_so_far": 3.3590742226550784e+16,
      "budget_used_percent": 33.59074222655079
    },
    {
      "type": "training",
      "description": "Training step 5654",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:33",
      "total_flops_so_far": 3.359668328460288e+16,
      "budget_used_percent": 33.59668328460288
    },
    {
      "type": "training",
      "description": "Training step 5655",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:33",
      "total_flops_so_far": 3.3602624342654976e+16,
      "budget_used_percent": 33.60262434265498
    },
    {
      "type": "training",
      "description": "Training step 5656",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:34",
      "total_flops_so_far": 3.360856540070707e+16,
      "budget_used_percent": 33.60856540070707
    },
    {
      "type": "training",
      "description": "Training step 5657",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:35",
      "total_flops_so_far": 3.361450645875917e+16,
      "budget_used_percent": 33.614506458759166
    },
    {
      "type": "training",
      "description": "Training step 5658",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:35",
      "total_flops_so_far": 3.3620447516811264e+16,
      "budget_used_percent": 33.62044751681126
    },
    {
      "type": "training",
      "description": "Training step 5659",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:35",
      "total_flops_so_far": 3.362638857486336e+16,
      "budget_used_percent": 33.626388574863356
    },
    {
      "type": "training",
      "description": "Training step 5660",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:36",
      "total_flops_so_far": 3.3632329632915456e+16,
      "budget_used_percent": 33.63232963291546
    },
    {
      "type": "training",
      "description": "Training step 5661",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:36",
      "total_flops_so_far": 3.363827069096755e+16,
      "budget_used_percent": 33.63827069096755
    },
    {
      "type": "training",
      "description": "Training step 5662",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:37",
      "total_flops_so_far": 3.364421174901965e+16,
      "budget_used_percent": 33.64421174901965
    },
    {
      "type": "training",
      "description": "Training step 5663",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:37",
      "total_flops_so_far": 3.3650152807071744e+16,
      "budget_used_percent": 33.65015280707174
    },
    {
      "type": "training",
      "description": "Training step 5664",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:38",
      "total_flops_so_far": 3.365609386512384e+16,
      "budget_used_percent": 33.656093865123836
    },
    {
      "type": "training",
      "description": "Training step 5665",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:38",
      "total_flops_so_far": 3.3662034923175936e+16,
      "budget_used_percent": 33.66203492317594
    },
    {
      "type": "training",
      "description": "Training step 5666",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:39",
      "total_flops_so_far": 3.366797598122803e+16,
      "budget_used_percent": 33.66797598122803
    },
    {
      "type": "training",
      "description": "Training step 5667",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:39",
      "total_flops_so_far": 3.367391703928013e+16,
      "budget_used_percent": 33.67391703928013
    },
    {
      "type": "training",
      "description": "Training step 5668",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:39",
      "total_flops_so_far": 3.3679858097332224e+16,
      "budget_used_percent": 33.67985809733222
    },
    {
      "type": "training",
      "description": "Training step 5669",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:40",
      "total_flops_so_far": 3.368579915538432e+16,
      "budget_used_percent": 33.68579915538432
    },
    {
      "type": "training",
      "description": "Training step 5670",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:40",
      "total_flops_so_far": 3.3691740213436416e+16,
      "budget_used_percent": 33.69174021343642
    },
    {
      "type": "training",
      "description": "Training step 5671",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:41",
      "total_flops_so_far": 3.369768127148851e+16,
      "budget_used_percent": 33.69768127148851
    },
    {
      "type": "training",
      "description": "Training step 5672",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:41",
      "total_flops_so_far": 3.370362232954061e+16,
      "budget_used_percent": 33.70362232954061
    },
    {
      "type": "training",
      "description": "Training step 5673",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:42",
      "total_flops_so_far": 3.3709563387592704e+16,
      "budget_used_percent": 33.70956338759271
    },
    {
      "type": "training",
      "description": "Training step 5674",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:42",
      "total_flops_so_far": 3.37155044456448e+16,
      "budget_used_percent": 33.715504445644804
    },
    {
      "type": "training",
      "description": "Training step 5675",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:43",
      "total_flops_so_far": 3.3721445503696896e+16,
      "budget_used_percent": 33.7214455036969
    },
    {
      "type": "training",
      "description": "Training step 5676",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:43",
      "total_flops_so_far": 3.372738656174899e+16,
      "budget_used_percent": 33.72738656174899
    },
    {
      "type": "training",
      "description": "Training step 5677",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:43",
      "total_flops_so_far": 3.373332761980109e+16,
      "budget_used_percent": 33.73332761980109
    },
    {
      "type": "training",
      "description": "Training step 5678",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:44",
      "total_flops_so_far": 3.3739268677853184e+16,
      "budget_used_percent": 33.73926867785318
    },
    {
      "type": "training",
      "description": "Training step 5679",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:44",
      "total_flops_so_far": 3.374520973590528e+16,
      "budget_used_percent": 33.74520973590528
    },
    {
      "type": "training",
      "description": "Training step 5680",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:45",
      "total_flops_so_far": 3.3751150793957376e+16,
      "budget_used_percent": 33.75115079395737
    },
    {
      "type": "training",
      "description": "Training step 5681",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:45",
      "total_flops_so_far": 3.375709185200947e+16,
      "budget_used_percent": 33.757091852009474
    },
    {
      "type": "training",
      "description": "Training step 5682",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:46",
      "total_flops_so_far": 3.376303291006157e+16,
      "budget_used_percent": 33.76303291006157
    },
    {
      "type": "training",
      "description": "Training step 5683",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:46",
      "total_flops_so_far": 3.3768973968113664e+16,
      "budget_used_percent": 33.76897396811366
    },
    {
      "type": "training",
      "description": "Training step 5684",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:47",
      "total_flops_so_far": 3.377491502616576e+16,
      "budget_used_percent": 33.77491502616576
    },
    {
      "type": "training",
      "description": "Training step 5685",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:47",
      "total_flops_so_far": 3.3780856084217856e+16,
      "budget_used_percent": 33.78085608421786
    },
    {
      "type": "training",
      "description": "Training step 5686",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:47",
      "total_flops_so_far": 3.378679714226995e+16,
      "budget_used_percent": 33.786797142269954
    },
    {
      "type": "training",
      "description": "Training step 5687",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:48",
      "total_flops_so_far": 3.379273820032205e+16,
      "budget_used_percent": 33.79273820032205
    },
    {
      "type": "training",
      "description": "Training step 5688",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:49",
      "total_flops_so_far": 3.3798679258374144e+16,
      "budget_used_percent": 33.79867925837414
    },
    {
      "type": "training",
      "description": "Training step 5689",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:49",
      "total_flops_so_far": 3.380462031642624e+16,
      "budget_used_percent": 33.804620316426245
    },
    {
      "type": "training",
      "description": "Training step 5690",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:49",
      "total_flops_so_far": 3.3810561374478336e+16,
      "budget_used_percent": 33.81056137447834
    },
    {
      "type": "training",
      "description": "Training step 5691",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:50",
      "total_flops_so_far": 3.381650243253043e+16,
      "budget_used_percent": 33.816502432530434
    },
    {
      "type": "training",
      "description": "Training step 5692",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:50",
      "total_flops_so_far": 3.382244349058253e+16,
      "budget_used_percent": 33.82244349058253
    },
    {
      "type": "training",
      "description": "Training step 5693",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:51",
      "total_flops_so_far": 3.3828384548634624e+16,
      "budget_used_percent": 33.828384548634624
    },
    {
      "type": "training",
      "description": "Training step 5694",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:51",
      "total_flops_so_far": 3.383432560668672e+16,
      "budget_used_percent": 33.83432560668672
    },
    {
      "type": "training",
      "description": "Training step 5695",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:52",
      "total_flops_so_far": 3.3840266664738816e+16,
      "budget_used_percent": 33.84026666473881
    },
    {
      "type": "training",
      "description": "Training step 5696",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:52",
      "total_flops_so_far": 3.384620772279091e+16,
      "budget_used_percent": 33.84620772279091
    },
    {
      "type": "training",
      "description": "Training step 5697",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:53",
      "total_flops_so_far": 3.385214878084301e+16,
      "budget_used_percent": 33.852148780843
    },
    {
      "type": "training",
      "description": "Training step 5698",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:53",
      "total_flops_so_far": 3.3858089838895104e+16,
      "budget_used_percent": 33.858089838895104
    },
    {
      "type": "training",
      "description": "Training step 5699",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:53",
      "total_flops_so_far": 3.38640308969472e+16,
      "budget_used_percent": 33.8640308969472
    },
    {
      "type": "training",
      "description": "Training step 5700",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:54",
      "total_flops_so_far": 3.3869971954999296e+16,
      "budget_used_percent": 33.86997195499929
    },
    {
      "type": "training",
      "description": "Training step 5701",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:54",
      "total_flops_so_far": 3.387591301305139e+16,
      "budget_used_percent": 33.87591301305139
    },
    {
      "type": "training",
      "description": "Training step 5702",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:55",
      "total_flops_so_far": 3.388185407110349e+16,
      "budget_used_percent": 33.88185407110349
    },
    {
      "type": "training",
      "description": "Training step 5703",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:55",
      "total_flops_so_far": 3.3887795129155584e+16,
      "budget_used_percent": 33.887795129155585
    },
    {
      "type": "training",
      "description": "Training step 5704",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:56",
      "total_flops_so_far": 3.389373618720768e+16,
      "budget_used_percent": 33.89373618720768
    },
    {
      "type": "training",
      "description": "Training step 5705",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:56",
      "total_flops_so_far": 3.3899677245259776e+16,
      "budget_used_percent": 33.899677245259774
    },
    {
      "type": "training",
      "description": "Training step 5706",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:57",
      "total_flops_so_far": 3.390561830331187e+16,
      "budget_used_percent": 33.905618303311876
    },
    {
      "type": "training",
      "description": "Training step 5707",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:57",
      "total_flops_so_far": 3.391155936136397e+16,
      "budget_used_percent": 33.91155936136397
    },
    {
      "type": "training",
      "description": "Training step 5708",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:57",
      "total_flops_so_far": 3.3917500419416064e+16,
      "budget_used_percent": 33.917500419416065
    },
    {
      "type": "training",
      "description": "Training step 5709",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:58",
      "total_flops_so_far": 3.392344147746816e+16,
      "budget_used_percent": 33.92344147746816
    },
    {
      "type": "training",
      "description": "Training step 5710",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:58",
      "total_flops_so_far": 3.3929382535520256e+16,
      "budget_used_percent": 33.92938253552026
    },
    {
      "type": "training",
      "description": "Training step 5711",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:59",
      "total_flops_so_far": 3.393532359357235e+16,
      "budget_used_percent": 33.935323593572356
    },
    {
      "type": "training",
      "description": "Training step 5712",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:19:59",
      "total_flops_so_far": 3.394126465162445e+16,
      "budget_used_percent": 33.94126465162445
    },
    {
      "type": "training",
      "description": "Training step 5713",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:00",
      "total_flops_so_far": 3.3947205709676544e+16,
      "budget_used_percent": 33.947205709676545
    },
    {
      "type": "training",
      "description": "Training step 5714",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:00",
      "total_flops_so_far": 3.395314676772864e+16,
      "budget_used_percent": 33.95314676772864
    },
    {
      "type": "training",
      "description": "Training step 5715",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:01",
      "total_flops_so_far": 3.3959087825780736e+16,
      "budget_used_percent": 33.959087825780735
    },
    {
      "type": "training",
      "description": "Training step 5716",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:01",
      "total_flops_so_far": 3.396502888383283e+16,
      "budget_used_percent": 33.96502888383283
    },
    {
      "type": "training",
      "description": "Training step 5717",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:02",
      "total_flops_so_far": 3.397096994188493e+16,
      "budget_used_percent": 33.970969941884924
    },
    {
      "type": "training",
      "description": "Training step 5718",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:02",
      "total_flops_so_far": 3.3976910999937024e+16,
      "budget_used_percent": 33.976910999937026
    },
    {
      "type": "training",
      "description": "Training step 5719",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:03",
      "total_flops_so_far": 3.398285205798912e+16,
      "budget_used_percent": 33.98285205798912
    },
    {
      "type": "training",
      "description": "Training step 5720",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:03",
      "total_flops_so_far": 3.3988793116041216e+16,
      "budget_used_percent": 33.988793116041215
    },
    {
      "type": "training",
      "description": "Training step 5721",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:03",
      "total_flops_so_far": 3.399473417409331e+16,
      "budget_used_percent": 33.99473417409331
    },
    {
      "type": "training",
      "description": "Training step 5722",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:04",
      "total_flops_so_far": 3.400067523214541e+16,
      "budget_used_percent": 34.000675232145404
    },
    {
      "type": "training",
      "description": "Training step 5723",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:04",
      "total_flops_so_far": 3.4006616290197504e+16,
      "budget_used_percent": 34.006616290197506
    },
    {
      "type": "training",
      "description": "Training step 5724",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:05",
      "total_flops_so_far": 3.40125573482496e+16,
      "budget_used_percent": 34.0125573482496
    },
    {
      "type": "training",
      "description": "Training step 5725",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:05",
      "total_flops_so_far": 3.4018498406301696e+16,
      "budget_used_percent": 34.018498406301696
    },
    {
      "type": "training",
      "description": "Training step 5726",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:06",
      "total_flops_so_far": 3.402443946435379e+16,
      "budget_used_percent": 34.02443946435379
    },
    {
      "type": "training",
      "description": "Training step 5727",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:06",
      "total_flops_so_far": 3.403038052240589e+16,
      "budget_used_percent": 34.03038052240589
    },
    {
      "type": "training",
      "description": "Training step 5728",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:07",
      "total_flops_so_far": 3.4036321580457984e+16,
      "budget_used_percent": 34.03632158045799
    },
    {
      "type": "training",
      "description": "Training step 5729",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:07",
      "total_flops_so_far": 3.404226263851008e+16,
      "budget_used_percent": 34.04226263851008
    },
    {
      "type": "training",
      "description": "Training step 5730",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:07",
      "total_flops_so_far": 3.4048203696562176e+16,
      "budget_used_percent": 34.048203696562176
    },
    {
      "type": "training",
      "description": "Training step 5731",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:08",
      "total_flops_so_far": 3.405414475461427e+16,
      "budget_used_percent": 34.05414475461427
    },
    {
      "type": "training",
      "description": "Training step 5732",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:08",
      "total_flops_so_far": 3.406008581266637e+16,
      "budget_used_percent": 34.060085812666365
    },
    {
      "type": "training",
      "description": "Training step 5733",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:09",
      "total_flops_so_far": 3.4066026870718464e+16,
      "budget_used_percent": 34.06602687071846
    },
    {
      "type": "training",
      "description": "Training step 5734",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:09",
      "total_flops_so_far": 3.407196792877056e+16,
      "budget_used_percent": 34.071967928770555
    },
    {
      "type": "training",
      "description": "Training step 5735",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:10",
      "total_flops_so_far": 3.4077908986822656e+16,
      "budget_used_percent": 34.077908986822656
    },
    {
      "type": "training",
      "description": "Training step 5736",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:10",
      "total_flops_so_far": 3.408385004487475e+16,
      "budget_used_percent": 34.08385004487475
    },
    {
      "type": "training",
      "description": "Training step 5737",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:11",
      "total_flops_so_far": 3.408979110292685e+16,
      "budget_used_percent": 34.089791102926846
    },
    {
      "type": "training",
      "description": "Training step 5738",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:11",
      "total_flops_so_far": 3.4095732160978944e+16,
      "budget_used_percent": 34.09573216097894
    },
    {
      "type": "training",
      "description": "Training step 5739",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:11",
      "total_flops_so_far": 3.410167321903104e+16,
      "budget_used_percent": 34.10167321903104
    },
    {
      "type": "training",
      "description": "Training step 5740",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:12",
      "total_flops_so_far": 3.4107614277083136e+16,
      "budget_used_percent": 34.10761427708314
    },
    {
      "type": "training",
      "description": "Training step 5741",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:13",
      "total_flops_so_far": 3.411355533513523e+16,
      "budget_used_percent": 34.11355533513523
    },
    {
      "type": "training",
      "description": "Training step 5742",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:13",
      "total_flops_so_far": 3.411949639318733e+16,
      "budget_used_percent": 34.119496393187326
    },
    {
      "type": "training",
      "description": "Training step 5743",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:14",
      "total_flops_so_far": 3.4125437451239424e+16,
      "budget_used_percent": 34.12543745123943
    },
    {
      "type": "training",
      "description": "Training step 5744",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:14",
      "total_flops_so_far": 3.413137850929152e+16,
      "budget_used_percent": 34.13137850929152
    },
    {
      "type": "training",
      "description": "Training step 5745",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:14",
      "total_flops_so_far": 3.4137319567343616e+16,
      "budget_used_percent": 34.13731956734362
    },
    {
      "type": "training",
      "description": "Training step 5746",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:15",
      "total_flops_so_far": 3.414326062539571e+16,
      "budget_used_percent": 34.14326062539571
    },
    {
      "type": "training",
      "description": "Training step 5747",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:15",
      "total_flops_so_far": 3.414920168344781e+16,
      "budget_used_percent": 34.149201683447814
    },
    {
      "type": "training",
      "description": "Training step 5748",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:16",
      "total_flops_so_far": 3.4155142741499904e+16,
      "budget_used_percent": 34.15514274149991
    },
    {
      "type": "training",
      "description": "Training step 5749",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:16",
      "total_flops_so_far": 3.4161083799552e+16,
      "budget_used_percent": 34.161083799552
    },
    {
      "type": "training",
      "description": "Training step 5750",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:17",
      "total_flops_so_far": 3.4167024857604096e+16,
      "budget_used_percent": 34.1670248576041
    },
    {
      "type": "training",
      "description": "Training step 5751",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:17",
      "total_flops_so_far": 3.417296591565619e+16,
      "budget_used_percent": 34.17296591565619
    },
    {
      "type": "training",
      "description": "Training step 5752",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:17",
      "total_flops_so_far": 3.417890697370829e+16,
      "budget_used_percent": 34.17890697370829
    },
    {
      "type": "training",
      "description": "Training step 5753",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:18",
      "total_flops_so_far": 3.4184848031760384e+16,
      "budget_used_percent": 34.18484803176038
    },
    {
      "type": "training",
      "description": "Training step 5754",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:18",
      "total_flops_so_far": 3.419078908981248e+16,
      "budget_used_percent": 34.190789089812476
    },
    {
      "type": "training",
      "description": "Training step 5755",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:19",
      "total_flops_so_far": 3.4196730147864576e+16,
      "budget_used_percent": 34.19673014786457
    },
    {
      "type": "training",
      "description": "Training step 5756",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:19",
      "total_flops_so_far": 3.420267120591667e+16,
      "budget_used_percent": 34.20267120591667
    },
    {
      "type": "training",
      "description": "Training step 5757",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:20",
      "total_flops_so_far": 3.420861226396877e+16,
      "budget_used_percent": 34.20861226396877
    },
    {
      "type": "training",
      "description": "Training step 5758",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:20",
      "total_flops_so_far": 3.4214553322020864e+16,
      "budget_used_percent": 34.21455332202086
    },
    {
      "type": "training",
      "description": "Training step 5759",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:21",
      "total_flops_so_far": 3.422049438007296e+16,
      "budget_used_percent": 34.22049438007296
    },
    {
      "type": "training",
      "description": "Training step 5760",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:21",
      "total_flops_so_far": 3.4226435438125056e+16,
      "budget_used_percent": 34.22643543812506
    },
    {
      "type": "training",
      "description": "Training step 5761",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:21",
      "total_flops_so_far": 3.423237649617715e+16,
      "budget_used_percent": 34.23237649617715
    },
    {
      "type": "training",
      "description": "Training step 5762",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:22",
      "total_flops_so_far": 3.423831755422925e+16,
      "budget_used_percent": 34.23831755422925
    },
    {
      "type": "training",
      "description": "Training step 5763",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:22",
      "total_flops_so_far": 3.4244258612281344e+16,
      "budget_used_percent": 34.24425861228134
    },
    {
      "type": "training",
      "description": "Training step 5764",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:23",
      "total_flops_so_far": 3.425019967033344e+16,
      "budget_used_percent": 34.250199670333444
    },
    {
      "type": "training",
      "description": "Training step 5765",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:24",
      "total_flops_so_far": 3.4256140728385536e+16,
      "budget_used_percent": 34.25614072838554
    },
    {
      "type": "training",
      "description": "Training step 5766",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:24",
      "total_flops_so_far": 3.426208178643763e+16,
      "budget_used_percent": 34.26208178643763
    },
    {
      "type": "training",
      "description": "Training step 5767",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:24",
      "total_flops_so_far": 3.426802284448973e+16,
      "budget_used_percent": 34.26802284448973
    },
    {
      "type": "training",
      "description": "Training step 5768",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:25",
      "total_flops_so_far": 3.4273963902541824e+16,
      "budget_used_percent": 34.27396390254182
    },
    {
      "type": "training",
      "description": "Training step 5769",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:25",
      "total_flops_so_far": 3.427990496059392e+16,
      "budget_used_percent": 34.279904960593925
    },
    {
      "type": "training",
      "description": "Training step 5770",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:26",
      "total_flops_so_far": 3.4285846018646016e+16,
      "budget_used_percent": 34.28584601864601
    },
    {
      "type": "training",
      "description": "Training step 5771",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:26",
      "total_flops_so_far": 3.429178707669811e+16,
      "budget_used_percent": 34.291787076698114
    },
    {
      "type": "training",
      "description": "Training step 5772",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:27",
      "total_flops_so_far": 3.429772813475021e+16,
      "budget_used_percent": 34.29772813475021
    },
    {
      "type": "training",
      "description": "Training step 5773",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:27",
      "total_flops_so_far": 3.4303669192802304e+16,
      "budget_used_percent": 34.3036691928023
    },
    {
      "type": "training",
      "description": "Training step 5774",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:28",
      "total_flops_so_far": 3.43096102508544e+16,
      "budget_used_percent": 34.3096102508544
    },
    {
      "type": "training",
      "description": "Training step 5775",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:28",
      "total_flops_so_far": 3.4315551308906496e+16,
      "budget_used_percent": 34.31555130890649
    },
    {
      "type": "training",
      "description": "Training step 5776",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:28",
      "total_flops_so_far": 3.432149236695859e+16,
      "budget_used_percent": 34.321492366958594
    },
    {
      "type": "training",
      "description": "Training step 5777",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:29",
      "total_flops_so_far": 3.432743342501069e+16,
      "budget_used_percent": 34.32743342501069
    },
    {
      "type": "training",
      "description": "Training step 5778",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:29",
      "total_flops_so_far": 3.4333374483062784e+16,
      "budget_used_percent": 34.333374483062784
    },
    {
      "type": "training",
      "description": "Training step 5779",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:30",
      "total_flops_so_far": 3.433931554111488e+16,
      "budget_used_percent": 34.33931554111488
    },
    {
      "type": "training",
      "description": "Training step 5780",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:30",
      "total_flops_so_far": 3.4345256599166976e+16,
      "budget_used_percent": 34.34525659916697
    },
    {
      "type": "training",
      "description": "Training step 5781",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:31",
      "total_flops_so_far": 3.435119765721907e+16,
      "budget_used_percent": 34.351197657219075
    },
    {
      "type": "training",
      "description": "Training step 5782",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:31",
      "total_flops_so_far": 3.435713871527117e+16,
      "budget_used_percent": 34.35713871527117
    },
    {
      "type": "training",
      "description": "Training step 5783",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:32",
      "total_flops_so_far": 3.4363079773323264e+16,
      "budget_used_percent": 34.363079773323264
    },
    {
      "type": "training",
      "description": "Training step 5784",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:32",
      "total_flops_so_far": 3.436902083137536e+16,
      "budget_used_percent": 34.36902083137536
    },
    {
      "type": "training",
      "description": "Training step 5785",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:32",
      "total_flops_so_far": 3.4374961889427456e+16,
      "budget_used_percent": 34.37496188942746
    },
    {
      "type": "training",
      "description": "Training step 5786",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:33",
      "total_flops_so_far": 3.438090294747955e+16,
      "budget_used_percent": 34.380902947479555
    },
    {
      "type": "training",
      "description": "Training step 5787",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:33",
      "total_flops_so_far": 3.438684400553165e+16,
      "budget_used_percent": 34.38684400553165
    },
    {
      "type": "training",
      "description": "Training step 5788",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:34",
      "total_flops_so_far": 3.4392785063583744e+16,
      "budget_used_percent": 34.392785063583744
    },
    {
      "type": "training",
      "description": "Training step 5789",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:34",
      "total_flops_so_far": 3.439872612163584e+16,
      "budget_used_percent": 34.39872612163584
    },
    {
      "type": "training",
      "description": "Training step 5790",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:35",
      "total_flops_so_far": 3.4404667179687936e+16,
      "budget_used_percent": 34.404667179687934
    },
    {
      "type": "training",
      "description": "Training step 5791",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:35",
      "total_flops_so_far": 3.441060823774003e+16,
      "budget_used_percent": 34.41060823774003
    },
    {
      "type": "training",
      "description": "Training step 5792",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:36",
      "total_flops_so_far": 3.441654929579213e+16,
      "budget_used_percent": 34.41654929579212
    },
    {
      "type": "training",
      "description": "Training step 5793",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:36",
      "total_flops_so_far": 3.4422490353844224e+16,
      "budget_used_percent": 34.422490353844225
    },
    {
      "type": "training",
      "description": "Training step 5794",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:36",
      "total_flops_so_far": 3.442843141189632e+16,
      "budget_used_percent": 34.42843141189632
    },
    {
      "type": "training",
      "description": "Training step 5795",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:37",
      "total_flops_so_far": 3.4434372469948416e+16,
      "budget_used_percent": 34.434372469948414
    },
    {
      "type": "training",
      "description": "Training step 5796",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:38",
      "total_flops_so_far": 3.444031352800051e+16,
      "budget_used_percent": 34.44031352800051
    },
    {
      "type": "training",
      "description": "Training step 5797",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:38",
      "total_flops_so_far": 3.444625458605261e+16,
      "budget_used_percent": 34.44625458605261
    },
    {
      "type": "training",
      "description": "Training step 5798",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:38",
      "total_flops_so_far": 3.4452195644104704e+16,
      "budget_used_percent": 34.452195644104705
    },
    {
      "type": "training",
      "description": "Training step 5799",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:39",
      "total_flops_so_far": 3.44581367021568e+16,
      "budget_used_percent": 34.4581367021568
    },
    {
      "type": "training",
      "description": "Training step 5800",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:39",
      "total_flops_so_far": 3.4464077760208896e+16,
      "budget_used_percent": 34.464077760208895
    },
    {
      "type": "training",
      "description": "Training step 5801",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:40",
      "total_flops_so_far": 3.447001881826099e+16,
      "budget_used_percent": 34.470018818260996
    },
    {
      "type": "training",
      "description": "Training step 5802",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:40",
      "total_flops_so_far": 3.447595987631309e+16,
      "budget_used_percent": 34.47595987631309
    },
    {
      "type": "training",
      "description": "Training step 5803",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:41",
      "total_flops_so_far": 3.4481900934365184e+16,
      "budget_used_percent": 34.481900934365186
    },
    {
      "type": "training",
      "description": "Training step 5804",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:41",
      "total_flops_so_far": 3.448784199241728e+16,
      "budget_used_percent": 34.48784199241728
    },
    {
      "type": "training",
      "description": "Training step 5805",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:42",
      "total_flops_so_far": 3.4493783050469376e+16,
      "budget_used_percent": 34.49378305046938
    },
    {
      "type": "training",
      "description": "Training step 5806",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:42",
      "total_flops_so_far": 3.449972410852147e+16,
      "budget_used_percent": 34.49972410852148
    },
    {
      "type": "training",
      "description": "Training step 5807",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:42",
      "total_flops_so_far": 3.450566516657357e+16,
      "budget_used_percent": 34.50566516657357
    },
    {
      "type": "training",
      "description": "Training step 5808",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:43",
      "total_flops_so_far": 3.4511606224625664e+16,
      "budget_used_percent": 34.511606224625666
    },
    {
      "type": "training",
      "description": "Training step 5809",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:43",
      "total_flops_so_far": 3.451754728267776e+16,
      "budget_used_percent": 34.51754728267776
    },
    {
      "type": "training",
      "description": "Training step 5810",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:44",
      "total_flops_so_far": 3.4523488340729856e+16,
      "budget_used_percent": 34.523488340729855
    },
    {
      "type": "training",
      "description": "Training step 5811",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:44",
      "total_flops_so_far": 3.452942939878195e+16,
      "budget_used_percent": 34.52942939878195
    },
    {
      "type": "training",
      "description": "Training step 5812",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:45",
      "total_flops_so_far": 3.453537045683405e+16,
      "budget_used_percent": 34.535370456834045
    },
    {
      "type": "training",
      "description": "Training step 5813",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:45",
      "total_flops_so_far": 3.4541311514886144e+16,
      "budget_used_percent": 34.54131151488614
    },
    {
      "type": "training",
      "description": "Training step 5814",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:46",
      "total_flops_so_far": 3.454725257293824e+16,
      "budget_used_percent": 34.54725257293824
    },
    {
      "type": "training",
      "description": "Training step 5815",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:46",
      "total_flops_so_far": 3.4553193630990336e+16,
      "budget_used_percent": 34.553193630990336
    },
    {
      "type": "training",
      "description": "Training step 5816",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:46",
      "total_flops_so_far": 3.455913468904243e+16,
      "budget_used_percent": 34.55913468904243
    },
    {
      "type": "training",
      "description": "Training step 5817",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:47",
      "total_flops_so_far": 3.456507574709453e+16,
      "budget_used_percent": 34.565075747094525
    },
    {
      "type": "training",
      "description": "Training step 5818",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:47",
      "total_flops_so_far": 3.4571016805146624e+16,
      "budget_used_percent": 34.57101680514663
    },
    {
      "type": "training",
      "description": "Training step 5819",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:48",
      "total_flops_so_far": 3.457695786319872e+16,
      "budget_used_percent": 34.57695786319872
    },
    {
      "type": "training",
      "description": "Training step 5820",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:48",
      "total_flops_so_far": 3.4582898921250816e+16,
      "budget_used_percent": 34.582898921250816
    },
    {
      "type": "training",
      "description": "Training step 5821",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:49",
      "total_flops_so_far": 3.458883997930291e+16,
      "budget_used_percent": 34.58883997930291
    },
    {
      "type": "training",
      "description": "Training step 5822",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:49",
      "total_flops_so_far": 3.459478103735501e+16,
      "budget_used_percent": 34.59478103735501
    },
    {
      "type": "training",
      "description": "Training step 5823",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:50",
      "total_flops_so_far": 3.4600722095407104e+16,
      "budget_used_percent": 34.60072209540711
    },
    {
      "type": "training",
      "description": "Training step 5824",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:50",
      "total_flops_so_far": 3.46066631534592e+16,
      "budget_used_percent": 34.6066631534592
    },
    {
      "type": "training",
      "description": "Training step 5825",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:51",
      "total_flops_so_far": 3.4612604211511296e+16,
      "budget_used_percent": 34.6126042115113
    },
    {
      "type": "training",
      "description": "Training step 5826",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:51",
      "total_flops_so_far": 3.461854526956339e+16,
      "budget_used_percent": 34.61854526956339
    },
    {
      "type": "training",
      "description": "Training step 5827",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:52",
      "total_flops_so_far": 3.462448632761549e+16,
      "budget_used_percent": 34.624486327615486
    },
    {
      "type": "training",
      "description": "Training step 5828",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:52",
      "total_flops_so_far": 3.4630427385667584e+16,
      "budget_used_percent": 34.63042738566758
    },
    {
      "type": "training",
      "description": "Training step 5829",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:53",
      "total_flops_so_far": 3.463636844371968e+16,
      "budget_used_percent": 34.636368443719675
    },
    {
      "type": "training",
      "description": "Training step 5830",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:53",
      "total_flops_so_far": 3.4642309501771776e+16,
      "budget_used_percent": 34.64230950177178
    },
    {
      "type": "training",
      "description": "Training step 5831",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:53",
      "total_flops_so_far": 3.464825055982387e+16,
      "budget_used_percent": 34.64825055982387
    },
    {
      "type": "training",
      "description": "Training step 5832",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:54",
      "total_flops_so_far": 3.465419161787597e+16,
      "budget_used_percent": 34.654191617875966
    },
    {
      "type": "training",
      "description": "Training step 5833",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:54",
      "total_flops_so_far": 3.4660132675928064e+16,
      "budget_used_percent": 34.66013267592806
    },
    {
      "type": "training",
      "description": "Training step 5834",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:55",
      "total_flops_so_far": 3.466607373398016e+16,
      "budget_used_percent": 34.666073733980156
    },
    {
      "type": "training",
      "description": "Training step 5835",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:55",
      "total_flops_so_far": 3.4672014792032256e+16,
      "budget_used_percent": 34.67201479203226
    },
    {
      "type": "training",
      "description": "Training step 5836",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:56",
      "total_flops_so_far": 3.467795585008435e+16,
      "budget_used_percent": 34.67795585008435
    },
    {
      "type": "training",
      "description": "Training step 5837",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:56",
      "total_flops_so_far": 3.468389690813645e+16,
      "budget_used_percent": 34.68389690813645
    },
    {
      "type": "training",
      "description": "Training step 5838",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:57",
      "total_flops_so_far": 3.4689837966188544e+16,
      "budget_used_percent": 34.68983796618854
    },
    {
      "type": "training",
      "description": "Training step 5839",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:57",
      "total_flops_so_far": 3.469577902424064e+16,
      "budget_used_percent": 34.69577902424064
    },
    {
      "type": "training",
      "description": "Training step 5840",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:57",
      "total_flops_so_far": 3.4701720082292736e+16,
      "budget_used_percent": 34.70172008229274
    },
    {
      "type": "training",
      "description": "Training step 5841",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:58",
      "total_flops_so_far": 3.470766114034483e+16,
      "budget_used_percent": 34.70766114034483
    },
    {
      "type": "training",
      "description": "Training step 5842",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:58",
      "total_flops_so_far": 3.471360219839693e+16,
      "budget_used_percent": 34.71360219839693
    },
    {
      "type": "training",
      "description": "Training step 5843",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:59",
      "total_flops_so_far": 3.4719543256449024e+16,
      "budget_used_percent": 34.71954325644903
    },
    {
      "type": "training",
      "description": "Training step 5844",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:20:59",
      "total_flops_so_far": 3.472548431450112e+16,
      "budget_used_percent": 34.725484314501124
    },
    {
      "type": "training",
      "description": "Training step 5845",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:00",
      "total_flops_so_far": 3.4731425372553216e+16,
      "budget_used_percent": 34.73142537255322
    },
    {
      "type": "training",
      "description": "Training step 5846",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:00",
      "total_flops_so_far": 3.473736643060531e+16,
      "budget_used_percent": 34.73736643060531
    },
    {
      "type": "training",
      "description": "Training step 5847",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:01",
      "total_flops_so_far": 3.474330748865741e+16,
      "budget_used_percent": 34.74330748865741
    },
    {
      "type": "training",
      "description": "Training step 5848",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:01",
      "total_flops_so_far": 3.4749248546709504e+16,
      "budget_used_percent": 34.7492485467095
    },
    {
      "type": "training",
      "description": "Training step 5849",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:02",
      "total_flops_so_far": 3.47551896047616e+16,
      "budget_used_percent": 34.7551896047616
    },
    {
      "type": "training",
      "description": "Training step 5850",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:02",
      "total_flops_so_far": 3.4761130662813696e+16,
      "budget_used_percent": 34.76113066281369
    },
    {
      "type": "training",
      "description": "Training step 5851",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:03",
      "total_flops_so_far": 3.476707172086579e+16,
      "budget_used_percent": 34.76707172086579
    },
    {
      "type": "training",
      "description": "Training step 5852",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:03",
      "total_flops_so_far": 3.477301277891789e+16,
      "budget_used_percent": 34.77301277891789
    },
    {
      "type": "training",
      "description": "Training step 5853",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:04",
      "total_flops_so_far": 3.4778953836969984e+16,
      "budget_used_percent": 34.77895383696998
    },
    {
      "type": "training",
      "description": "Training step 5854",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:04",
      "total_flops_so_far": 3.478489489502208e+16,
      "budget_used_percent": 34.78489489502208
    },
    {
      "type": "training",
      "description": "Training step 5855",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:04",
      "total_flops_so_far": 3.4790835953074176e+16,
      "budget_used_percent": 34.79083595307418
    },
    {
      "type": "training",
      "description": "Training step 5856",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:05",
      "total_flops_so_far": 3.479677701112627e+16,
      "budget_used_percent": 34.796777011126274
    },
    {
      "type": "training",
      "description": "Training step 5857",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:05",
      "total_flops_so_far": 3.480271806917837e+16,
      "budget_used_percent": 34.80271806917837
    },
    {
      "type": "training",
      "description": "Training step 5858",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:06",
      "total_flops_so_far": 3.4808659127230464e+16,
      "budget_used_percent": 34.80865912723046
    },
    {
      "type": "training",
      "description": "Training step 5859",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:06",
      "total_flops_so_far": 3.481460018528256e+16,
      "budget_used_percent": 34.814600185282565
    },
    {
      "type": "training",
      "description": "Training step 5860",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:07",
      "total_flops_so_far": 3.4820541243334656e+16,
      "budget_used_percent": 34.82054124333466
    },
    {
      "type": "training",
      "description": "Training step 5861",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:07",
      "total_flops_so_far": 3.482648230138675e+16,
      "budget_used_percent": 34.826482301386754
    },
    {
      "type": "training",
      "description": "Training step 5862",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:08",
      "total_flops_so_far": 3.483242335943885e+16,
      "budget_used_percent": 34.83242335943885
    },
    {
      "type": "training",
      "description": "Training step 5863",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:08",
      "total_flops_so_far": 3.4838364417490944e+16,
      "budget_used_percent": 34.83836441749094
    },
    {
      "type": "training",
      "description": "Training step 5864",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:08",
      "total_flops_so_far": 3.484430547554304e+16,
      "budget_used_percent": 34.84430547554304
    },
    {
      "type": "training",
      "description": "Training step 5865",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:09",
      "total_flops_so_far": 3.4850246533595136e+16,
      "budget_used_percent": 34.85024653359513
    },
    {
      "type": "training",
      "description": "Training step 5866",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:09",
      "total_flops_so_far": 3.485618759164723e+16,
      "budget_used_percent": 34.85618759164723
    },
    {
      "type": "training",
      "description": "Training step 5867",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:10",
      "total_flops_so_far": 3.486212864969933e+16,
      "budget_used_percent": 34.86212864969932
    },
    {
      "type": "training",
      "description": "Training step 5868",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:10",
      "total_flops_so_far": 3.4868069707751424e+16,
      "budget_used_percent": 34.868069707751424
    },
    {
      "type": "training",
      "description": "Training step 5869",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:11",
      "total_flops_so_far": 3.487401076580352e+16,
      "budget_used_percent": 34.87401076580352
    },
    {
      "type": "training",
      "description": "Training step 5870",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:11",
      "total_flops_so_far": 3.4879951823855616e+16,
      "budget_used_percent": 34.87995182385561
    },
    {
      "type": "training",
      "description": "Training step 5871",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:12",
      "total_flops_so_far": 3.488589288190771e+16,
      "budget_used_percent": 34.88589288190771
    },
    {
      "type": "training",
      "description": "Training step 5872",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:12",
      "total_flops_so_far": 3.489183393995981e+16,
      "budget_used_percent": 34.89183393995981
    },
    {
      "type": "training",
      "description": "Training step 5873",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:13",
      "total_flops_so_far": 3.4897774998011904e+16,
      "budget_used_percent": 34.897774998011904
    },
    {
      "type": "training",
      "description": "Training step 5874",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:13",
      "total_flops_so_far": 3.4903716056064e+16,
      "budget_used_percent": 34.903716056064
    },
    {
      "type": "training",
      "description": "Training step 5875",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:14",
      "total_flops_so_far": 3.4909657114116096e+16,
      "budget_used_percent": 34.909657114116094
    },
    {
      "type": "training",
      "description": "Training step 5876",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:14",
      "total_flops_so_far": 3.491559817216819e+16,
      "budget_used_percent": 34.915598172168195
    },
    {
      "type": "training",
      "description": "Training step 5877",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:15",
      "total_flops_so_far": 3.492153923022029e+16,
      "budget_used_percent": 34.92153923022029
    },
    {
      "type": "training",
      "description": "Training step 5878",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:15",
      "total_flops_so_far": 3.4927480288272384e+16,
      "budget_used_percent": 34.927480288272385
    },
    {
      "type": "training",
      "description": "Training step 5879",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:15",
      "total_flops_so_far": 3.493342134632448e+16,
      "budget_used_percent": 34.93342134632448
    },
    {
      "type": "training",
      "description": "Training step 5880",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:16",
      "total_flops_so_far": 3.4939362404376576e+16,
      "budget_used_percent": 34.93936240437658
    },
    {
      "type": "training",
      "description": "Training step 5881",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:16",
      "total_flops_so_far": 3.494530346242867e+16,
      "budget_used_percent": 34.945303462428676
    },
    {
      "type": "training",
      "description": "Training step 5882",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:17",
      "total_flops_so_far": 3.495124452048077e+16,
      "budget_used_percent": 34.95124452048077
    },
    {
      "type": "training",
      "description": "Training step 5883",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:17",
      "total_flops_so_far": 3.4957185578532864e+16,
      "budget_used_percent": 34.957185578532865
    },
    {
      "type": "training",
      "description": "Training step 5884",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:18",
      "total_flops_so_far": 3.496312663658496e+16,
      "budget_used_percent": 34.96312663658496
    },
    {
      "type": "training",
      "description": "Training step 5885",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:18",
      "total_flops_so_far": 3.4969067694637056e+16,
      "budget_used_percent": 34.969067694637054
    },
    {
      "type": "training",
      "description": "Training step 5886",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:19",
      "total_flops_so_far": 3.497500875268915e+16,
      "budget_used_percent": 34.97500875268915
    },
    {
      "type": "training",
      "description": "Training step 5887",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:19",
      "total_flops_so_far": 3.498094981074125e+16,
      "budget_used_percent": 34.980949810741244
    },
    {
      "type": "training",
      "description": "Training step 5888",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:19",
      "total_flops_so_far": 3.4986890868793344e+16,
      "budget_used_percent": 34.986890868793346
    },
    {
      "type": "training",
      "description": "Training step 5889",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:20",
      "total_flops_so_far": 3.499283192684544e+16,
      "budget_used_percent": 34.99283192684544
    },
    {
      "type": "training",
      "description": "Training step 5890",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:20",
      "total_flops_so_far": 3.4998772984897536e+16,
      "budget_used_percent": 34.998772984897535
    },
    {
      "type": "training",
      "description": "Training step 5891",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-04-02 19:21:21",
      "total_flops_so_far": 3.500471404294963e+16,
      "budget_used_percent": 35.00471404294963
    }
  ],
  "total_flops": 3.500471404294963e+16,
  "budget_used_percent": 35.00471404294963
}