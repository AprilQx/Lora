{
  "experiment_name": "lora_r8_a16_lr1e-04",
  "model_config": {
    "hidden_size": 896,
    "num_attention_heads": 14,
    "num_hidden_layers": 24,
    "intermediate_size": 4864,
    "head_dim": 64,
    "vocab_size": 151936,
    "lora_r": 8,
    "lora_target_modules": [
      "q_proj",
      "v_proj"
    ]
  },
  "max_budget": 1e+17,
  "start_time": "2025-03-16 12:25:25",
  "operations": [
    {
      "type": "training",
      "description": "Training step 0",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:25:32",
      "total_flops_so_far": 5941058052096.0,
      "budget_used_percent": 0.005941058052096
    },
    {
      "type": "training",
      "description": "Training step 1",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:25:57",
      "total_flops_so_far": 11882116104192.0,
      "budget_used_percent": 0.011882116104192
    },
    {
      "type": "training",
      "description": "Training step 2",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:26:46",
      "total_flops_so_far": 17823174156288.0,
      "budget_used_percent": 0.017823174156288
    },
    {
      "type": "training",
      "description": "Training step 3",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:27:19",
      "total_flops_so_far": 23764232208384.0,
      "budget_used_percent": 0.023764232208384
    },
    {
      "type": "training",
      "description": "Training step 4",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:27:58",
      "total_flops_so_far": 29705290260480.0,
      "budget_used_percent": 0.02970529026048
    },
    {
      "type": "training",
      "description": "Training step 5",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:28:31",
      "total_flops_so_far": 35646348312576.0,
      "budget_used_percent": 0.035646348312576
    },
    {
      "type": "training",
      "description": "Training step 6",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:29:06",
      "total_flops_so_far": 41587406364672.0,
      "budget_used_percent": 0.041587406364672
    },
    {
      "type": "training",
      "description": "Training step 7",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:29:40",
      "total_flops_so_far": 47528464416768.0,
      "budget_used_percent": 0.047528464416768
    },
    {
      "type": "training",
      "description": "Training step 8",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:30:23",
      "total_flops_so_far": 53469522468864.0,
      "budget_used_percent": 0.05346952246886401
    },
    {
      "type": "training",
      "description": "Training step 9",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:31:01",
      "total_flops_so_far": 59410580520960.0,
      "budget_used_percent": 0.05941058052096
    },
    {
      "type": "training",
      "description": "Training step 10",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:31:32",
      "total_flops_so_far": 65351638573056.0,
      "budget_used_percent": 0.06535163857305601
    },
    {
      "type": "training",
      "description": "Training step 11",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:32:09",
      "total_flops_so_far": 71292696625152.0,
      "budget_used_percent": 0.071292696625152
    },
    {
      "type": "training",
      "description": "Training step 12",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:32:40",
      "total_flops_so_far": 77233754677248.0,
      "budget_used_percent": 0.07723375467724801
    },
    {
      "type": "training",
      "description": "Training step 13",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:33:19",
      "total_flops_so_far": 83174812729344.0,
      "budget_used_percent": 0.083174812729344
    },
    {
      "type": "training",
      "description": "Training step 14",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:33:53",
      "total_flops_so_far": 89115870781440.0,
      "budget_used_percent": 0.08911587078144001
    },
    {
      "type": "training",
      "description": "Training step 15",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:34:36",
      "total_flops_so_far": 95056928833536.0,
      "budget_used_percent": 0.095056928833536
    },
    {
      "type": "training",
      "description": "Training step 16",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:35:22",
      "total_flops_so_far": 100997986885632.0,
      "budget_used_percent": 0.10099798688563201
    },
    {
      "type": "training",
      "description": "Training step 17",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:36:06",
      "total_flops_so_far": 106939044937728.0,
      "budget_used_percent": 0.10693904493772802
    },
    {
      "type": "training",
      "description": "Training step 18",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:36:53",
      "total_flops_so_far": 112880102989824.0,
      "budget_used_percent": 0.112880102989824
    },
    {
      "type": "training",
      "description": "Training step 19",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:37:35",
      "total_flops_so_far": 118821161041920.0,
      "budget_used_percent": 0.11882116104192
    },
    {
      "type": "training",
      "description": "Training step 20",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:38:22",
      "total_flops_so_far": 124762219094016.0,
      "budget_used_percent": 0.12476221909401601
    },
    {
      "type": "training",
      "description": "Training step 21",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:39:08",
      "total_flops_so_far": 130703277146112.0,
      "budget_used_percent": 0.13070327714611202
    },
    {
      "type": "training",
      "description": "Training step 22",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:39:45",
      "total_flops_so_far": 136644335198208.0,
      "budget_used_percent": 0.13664433519820798
    },
    {
      "type": "training",
      "description": "Training step 23",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:40:36",
      "total_flops_so_far": 142585393250304.0,
      "budget_used_percent": 0.142585393250304
    },
    {
      "type": "training",
      "description": "Training step 24",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:41:24",
      "total_flops_so_far": 148526451302400.0,
      "budget_used_percent": 0.1485264513024
    },
    {
      "type": "training",
      "description": "Training step 25",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:42:05",
      "total_flops_so_far": 154467509354496.0,
      "budget_used_percent": 0.15446750935449602
    },
    {
      "type": "training",
      "description": "Training step 26",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:42:44",
      "total_flops_so_far": 160408567406592.0,
      "budget_used_percent": 0.16040856740659198
    },
    {
      "type": "training",
      "description": "Training step 27",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:43:15",
      "total_flops_so_far": 166349625458688.0,
      "budget_used_percent": 0.166349625458688
    },
    {
      "type": "training",
      "description": "Training step 28",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:43:56",
      "total_flops_so_far": 172290683510784.0,
      "budget_used_percent": 0.172290683510784
    },
    {
      "type": "training",
      "description": "Training step 29",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:44:32",
      "total_flops_so_far": 178231741562880.0,
      "budget_used_percent": 0.17823174156288002
    },
    {
      "type": "training",
      "description": "Training step 30",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:45:17",
      "total_flops_so_far": 184172799614976.0,
      "budget_used_percent": 0.18417279961497598
    },
    {
      "type": "training",
      "description": "Training step 31",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:46:06",
      "total_flops_so_far": 190113857667072.0,
      "budget_used_percent": 0.190113857667072
    },
    {
      "type": "training",
      "description": "Training step 32",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:46:55",
      "total_flops_so_far": 196054915719168.0,
      "budget_used_percent": 0.196054915719168
    },
    {
      "type": "training",
      "description": "Training step 33",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:47:36",
      "total_flops_so_far": 201995973771264.0,
      "budget_used_percent": 0.20199597377126402
    },
    {
      "type": "training",
      "description": "Training step 34",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:48:17",
      "total_flops_so_far": 207937031823360.0,
      "budget_used_percent": 0.20793703182336
    },
    {
      "type": "training",
      "description": "Training step 35",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:48:52",
      "total_flops_so_far": 213878089875456.0,
      "budget_used_percent": 0.21387808987545603
    },
    {
      "type": "training",
      "description": "Training step 36",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:49:27",
      "total_flops_so_far": 219819147927552.0,
      "budget_used_percent": 0.21981914792755197
    },
    {
      "type": "training",
      "description": "Training step 37",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:50:02",
      "total_flops_so_far": 225760205979648.0,
      "budget_used_percent": 0.225760205979648
    },
    {
      "type": "training",
      "description": "Training step 38",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:50:46",
      "total_flops_so_far": 231701264031744.0,
      "budget_used_percent": 0.231701264031744
    },
    {
      "type": "training",
      "description": "Training step 39",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:51:28",
      "total_flops_so_far": 237642322083840.0,
      "budget_used_percent": 0.23764232208384
    },
    {
      "type": "training",
      "description": "Training step 40",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:52:03",
      "total_flops_so_far": 243583380135936.0,
      "budget_used_percent": 0.243583380135936
    },
    {
      "type": "training",
      "description": "Training step 41",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:52:54",
      "total_flops_so_far": 249524438188032.0,
      "budget_used_percent": 0.24952443818803202
    },
    {
      "type": "training",
      "description": "Training step 42",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:53:49",
      "total_flops_so_far": 255465496240128.0,
      "budget_used_percent": 0.25546549624012804
    },
    {
      "type": "training",
      "description": "Training step 43",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:54:36",
      "total_flops_so_far": 261406554292224.0,
      "budget_used_percent": 0.26140655429222404
    },
    {
      "type": "training",
      "description": "Training step 44",
      "seq_len": 512,
      "batch_size": 4,
      "forward_flops": 1980352684032.0,
      "backward_flops": 3960705368064.0,
      "total_flops": 5941058052096.0,
      "lora_r": 8,
      "lora_target_modules": [
        "q_proj",
        "v_proj"
      ],
      "timestamp": "2025-03-16 12:55:12",
      "total_flops_so_far": 267347612344320.0,
      "budget_used_percent": 0.26734761234432
    }
  ],
  "total_flops": 267347612344320.0,
  "budget_used_percent": 0.26734761234432
}